{"posts":[{"title":"「基因组」001｜基因组组装之前要做的：Genome Survey","content":"基因组组装之前，有一些问题还是需要注意的， genome size是多少？ 评估得到的genome heterozygosity是多少？ 重复序列的占比是多少？ 可以系统性地称为genome survey，这是一个非常简单的分析，但是其实有一些问题是值得注意的 Genome Survey一般基于Illumina short reads进行分析，因为二代测序便宜，先测出来试试水， 再判断三代的数据量，这应该算是一个非常经济实惠的做法。 分析流程 1）fastp、Trimmomatic等软件挑一个过滤低质量序列 2）Jellyfish 2.3.0、KMC3 我个人其实比较喜欢KMC，因为可以直接读取.gz文件（绝对不是因为之前KMC作者帮助我愉快地解决了一个Bug），但是解决jellyfish脚本的过程中也让我对Shell Kernel有了一个更深刻的理解。 3）Genome Scope 2.0、GCE等软件，挑一个进行genome size、heterozygosity等指标的估计 我个人比较熟悉的还是Genome Scope 2.0，因为这个软件可以用于判断auto-tetraploid和allo-tetraploid， 同时作者Michael C. Schatz的实验室还开发了FALCON~ fastp vim fastp.sh sh fastp.sh 2&gt;fastp.err.log &amp; Shell script: #!/bin/bash # Preset dir=&lt;specicy_path_of_your_rawdata&gt; echo &quot;The raw dataset is placed at $dir&quot; echo &quot;Now running Quality control&quot; thread=24 # set 24 threads quality=20 # set quality cutoff to 20 based on Phred33 # fastp -w $thread -q $quality -i $dir/sample1.fq.gz -I $dir/sample2.fq.gz -o ./sample_clean_1.fq.gz -O ./sample_clean_2.fq.gz Jellyfish: count k-mer vim jellyfish.sh chmod 777 jellyfish.sh # key step, otherwise the script below will report syntax error ./jellyfish.sh 2&gt;jellyfish.err.log &amp; “哼男人，嘴上说着喜欢KMC，但是却用Jellyfish”， Shell script: # Preset dir=&lt;specicy_path_of_your_cleandata&gt; echo &quot;The clean dataset is placed at $dir&quot; echo &quot;Now running Jellyfish Kmercount&quot; # 17mer echo &quot;Now running 17mer counting&quot; content1=&quot;jellyfish count -C -m 17 -o ./sample.17mer.jf -s 10G -t 24 &lt;(pigz -dc $dir/sample_clean_1.fq.gz) &lt;(pigz -dc $dir/sample_clean_2.fq.gz)&quot; echo &quot;The command is $content1&quot; jellyfish count \\ -C \\ -m 17 \\ &lt;( pigz -dc $dir/sample_clean_1.fq.gz ) &lt;( pigz -dc $dir/sample_clean_2.fq.gz ) \\ -o ./sample.17mer.jf -s 10G -t 24 # 21mer: recommanded by author echo &quot;Now running 21mer counting&quot; content2=&quot;jellyfish count -C -m 21 -o ./sample.21mer.jf -s 10G -t 24 &lt;(pigz -dc $dir/sample_clean_1.fq.gz) &lt;(pigz -dc $dir/sample_clean_2.fq.gz)&quot; echo &quot;The command is $content2&quot; jellyfish count \\ -C \\ -m 21 \\ &lt;( pigz -dc $dir/sample_clean_1.fq.gz ) &lt;( pigz -dc $dir/sample_clean_2.fq.gz ) \\ -o ./sample.21mer.jf -s 10G -t 24 注意点：为什么要用chmod 777？ 答：未经777赋予可执行权限的脚本，仍为shell脚本，需要指定bash或者sh来运行程序，即不可从jellyfish直接开始运行程序， 就好比原本的运行方式为bash &lt;script_name&gt;.sh，现在要修改成为./&lt;script_name&gt;.sh的运行方式，不然就会出现syntax errror Jellyfish: k-mer spectrum generation jellyfish histo -t 24 -l 1 -h 500000 sample.17mer.jf &gt; sample.17mer.histo &amp; jellyfish histo -t 24 -l 1 -h 500000 sample.21mer.jf &gt; sample.21mer.histo &amp; 注意点：upper limitation的修改。 Genome Scope 2.0的分析需要将k-mer spectrum的upper limit设置得高一些，不然后续genome size估计塌缩比例会特别大。 Genome Scope 2.0 + Smudeplot 1）The estimation of genome size, heterozygosity, etc. vim genomescope.sh chmod 777 genomescope.sh ./genomescope.sh 2&gt;genomescope.err.log &amp; Shell script: script=&lt;path_to_your_genomescope_repo&gt;/genomescope.R dir=&lt;where_kmerspectrum_deposited&gt; Rscript $script -i &lt;input_histo&gt; -o ./ -n &lt;outputname&gt; -p &lt;ploidy_level&gt; -k &lt;kmer_used&gt; 2）Kmer-pair plot 这部分官网其实给出了比较好的流程，我就只是简单概括走一下， dir=&lt;specify_your_kmercount_database&gt; L=$(smudgeplot.py cutoff &lt;histo_from_kmc&gt; L) U=$(smudgeplot.py cutoff &lt;histo_from_kmc&gt; U) echo $L $U # these need to be sane values # conda activate genomesurvey kmc_tools transform $dir/&lt;kmc_db&gt; -ci&quot;$L&quot; -cx&quot;$U&quot; dump -s smudgeplot_kmc_db/&lt;kmc_db&gt;.kmc_L&quot;$L&quot;_U&quot;$U&quot;.dump # conda activate smudgeplot smudgeplot.py hetkmers -o smudgeplot_kmercounts/&lt;kmc_db&gt;.kmc_L&quot;$L&quot;_U&quot;$U&quot; &lt; smudgeplot_kmc_db/&lt;kmc_db&gt;.kmc_L&quot;$L&quot;_U&quot;$U&quot;.dump # Plot smudgeplot.py plot &lt;kmc_db&gt;._L&quot;$L&quot;_U&quot;$U&quot;_coverages.tsv 结果示意图如下， 基因组大小估计需要注意的点， 0）三代数据不适合用于Kmer分析，因为测序错误率高了很多，会对分析结果产生非常大的影响， 但是HiFi reads以及canu和falcon产生的corrected reads可以很好的适用于Genome Scope分析 1）jellyfish histo输出时指定的maximum kmer-freq，会极大地影响到genome size的估计，因此需要根据自己的数据进行调整，一般100000再往上也可 2）genonomescope.R的-p以及--kcov的设置，都会影响到genome size的估计 比如在genomescope2.0 model下，如果在输入数据和模型都已经定下来的基础上，将--kcov设置为原本的一倍，则genome size的大小估计会减半（此处感兴趣的，我建议还是自行搜索下基于kmer计算genome size的公式） 3）结果中的transformed_linear_plot和linear_plot有什么区别？ 前者的observed曲线经过了一个转换，越往后的peak其峰值越大，即在原始的kmer freq上乘了一个n（n代表第n个peak）， 后者的observed曲线为实际观测到的一个数值，没有经过上述转换 transformed linear： linear plot： 4）Genome Scope 2.0分析时，如果将过多的kmer判定为了error，最终的genome size就会小了特别多（基于genome size的计算公式） 背后的原理 首先需要明确的一个点是：Genome Scope 是基于diploid进行编写的。 关于二倍体物种的基因组大小估计，如何理解。我想要举一个非常简单的例子来理解： 给个用kmer将genome给“划分”开的示意， kmer: ---A-- --A--- -A---- A----- genome: ---A----------------------------------------------- 假设当前的基因组非常纯合（-&gt; homozygous, not -&gt; heterozygous），kmer会在某一个频数上呈现一个峰值， 但是如果当前的基因组杂合度上升了，也就是我们一般在文献中看到的heterozygosity，kmer在另一个频数较小的区域，也会呈现一个峰值，也就是Genome Survey中提到的杂合峰 就比如下图中的T，是A对应的allele。该种情况的存在，会导致kmer出现另一种情况，从而降低了纯合峰的高度，比如下图的例子就表示了对应位置kmer的频数，从4降低到了2， 即可以将diploid的kmer topology理解为：aa，ab kmer: ---A-- --A--- genome: ---A----------------------------------------------- T ---T-- --T--- 所以，为了满足polyploid的产生，Genome Scope 2.0被开发 —— 基于负二项分布的Kmer模型，用于估计genome size、heterozygosity等。 三倍体的kmer topolygy：aaa（3种haplotype均一致），aab（有1种haplotype和另外2个haplotype存在区别），abc（3种haplotype各不相同） 四倍体的kmer topology：aaaa，aaab，aabb，abcd 参考资料 [1] https://github.com/schatzlab/genomescope/issues/32 [2] https://github.com/schatzlab/genomescope/issues/43 [3] https://github.com/schatzlab/genomescope/issues/48 [4] https://github.com/schatzlab/genomescope [5] https://github.com/KamilSJaron/smudgeplot ","link":"https://grideaBlog/post/genomesurvey/"},{"title":"「生物信息学中的Math」001｜浅谈统计检验效能和FDR","content":"什么是统计检验的效能（power）？ 我们首先来回顾一下，在假设检验中，我们需要什么。 （1）原假设（H0H_{0}H0​）：可以认为是辩论赛中，保守一方的观点（e.g. 超级英雄和普通人没啥区别） （2）备择假设（HA/H1H_{A}/H_{1}HA​/H1​）：可以类比为辩论赛中，激进一方的观点（e.g. 超级英雄就是爷，就是比普通人牛） 在做计算的过程中，会涉及到一个非常重要的数值 —— ααα（一般取0.05 或 0.01） ααα的含义是：显著性水平、一类错误发生的概率（Type I Error）、弃真错误发生的概率。 Note：α —— 一类错误、弃真错误、假阳性错误 从这里有需要再引入几个概念，如下表： H0H_{0}H0​是正确的 H0H_{0}H0​是错误的 接受H0H_{0}H0​ 接受H0H_{0}H0​（1−α1-α1−α） βββ错误 拒绝H0H_{0}H0​ ααα错误 接受H1H_{1}H1​（1−β1-β1−β） β从α衍生出来，是二类错误发生的概率（Type II Error）、取伪错误发生的概率。 在假设检验过程中，我们把拒绝原假设后，接受正确的备择假设的概率称为统计检验的效能/功效（statistical power），因此其在数值上等于1−β1-β1−β。 【个人理解】1−β1-β1−β，实际上就是确定两个样本的总体有差异之后，假设检验能够顺利根据样本推断出真实的总体信息的概率。 什么是假阳性（false positive rate）？ 结合时事，我就拿新冠检测作为例子。 假设对100个人进行核酸检测，检测结果分别如下： 被测对象真实情况：阳性 被测对象真实情况：阴性 新冠检测结果：阳性 5 2 新冠检测结果：阴性 3 90 可以得到2个指标的计算结果，如下： （1）true positve rate（sensitivity，即灵敏度）：检测出的真实阳性样本数 除以 所有真实阳性样本数 55+3=0.625 \\frac{5}{5+3} = 0.625 5+35​=0.625 （2）false positive rate：检测出的假阳性样本数 除以 所有真实阴性样本数 22+90=0.0217 \\frac{2}{2+90} = 0.0217 2+902​=0.0217 还有一个非常重要的计算指标，是特异度（specificity），计算公式如下： 9090+2=0.9783 \\frac{90}{90+2}= 0.9783 90+290​=0.9783 一张找来的总结表： 参考资料 [1] https://www.jianshu.com/p/d5ea74ca61f8 [2] https://blog.csdn.net/fish2009122/article/details/110040002 [3] 统计功效, 百度百科 [4] 真阳性率, 百度百度 [5] 假阳性率, 百度百科 [6] https://zhwhong.cn/2017/04/14/ROC-AUC-Precision-Recall-analysis/ ","link":"https://grideaBlog/post/statistic-powerandfalse-positive/"},{"title":"「一文搞定序列比对算法」Global以及Local Alignment序列比对算法的实现","content":"序列比对是什么以及序列比对主要的作用是什么，本篇博客就一笔带过，因为不是主要分享内容。 序列比对，此处引申为pairwise alignment会更加恰当一些，用于比较2条序列之间的相似程度，推断它们之间的相似程度，进而探索对应功能以及系统发育关系。 接下来大体分为2个部分，1）全局比对，2）局部比对 首先要明确一个概念：序列比对想要达到的目的是什么？ 引一张图来说明序列比对的目的以及全局比对、局部比对之间的区别， 总的来说，也就是全局比对和局部比对想要达到的目的是不一样的， 全局比对想要得到的是2条序列最佳的匹配结果（e.g. 最多的match数量、最高的比对得分、最高的identity），局部比对想要得到的是2条序列中最佳匹配片段（注意：最佳的匹配结果需要建立在相对较少的序列修改上） 全局比对更适用于evolution关系上更加靠近的（e.g. 粳稻和籼稻），而局部比对更加适用于evolution关系上关系比较远的（e.g. 水稻和葡萄） 「步骤拆解」Global Alignment 利用动态规划来解决问题，最关键的一步就是列出动态规划公式，只要能列出公式，后面的编程也都只是时间问题。 但是，我并不想一上来就列出数学公式，我认为以一个简单的例子入手更有利于序列比对问题中的动态规划应用。 接下来，先理一理基于动态规划的序列比对的过程。 （1）Intialization + Matrix Filling 假设现在有2条长度分别为n、m的序列， 那则需要构建行数为n+1，列数为m+1的矩阵， 而“Filling”这个过程，即将第一列和第一行进行填充，从数学公式的角度来理解的话， 第一列的填充：$$g*length(matrix[1:i, 1])$$，i~[1, n] 第一行的填充：$$g*length(matrix[1, 1:j])$$，j~[1, m] （2）Tracing Back 每一个单元的填充模式如下， 横向和竖向的移动代表了gap open（horizontal，vertical） 但更加复杂的情况应该考虑到gap在哪一条序列打开 对角线的移动则可以分为1）match（从大的数值回溯到小的数值），2）mismatch（从小的数值回溯到大的数值） Note：数值增大代表替换矩阵中，该碱基对应关系为match，而数值减小，代表替换矩阵中碱基对应关系为mismatch 「公式」Global Alignment 引入gap extension 出现4个单位长度的一个完整gap将两条序列给比对上，或者4个单位长度的单独gap将两条序列给比对上是更符合生物学原理的？ 上述的文字情况如下所示， # 1. ATCGATCGATCGATCG---- AGCTAGCTCAGTACGT---- # 2. ATCG-ATCG-ATCG-ATCG-ATCG ATCG-ATCG-ATCG-ATCG-ATCG 答案是前者。这就需要在序列比对中引入另一个非常重要的细节 —— affine gap penalty。 Note：此处引入的affine gap penalty为“not penalize open with extension”，即在打开一个gap的时候，不会在该gap上同时引入open和extension的罚分 affine gap penalty，即在打开第一个gap的时候引入gap open罚分，而在该gap的基础上进行延续则采用gap extension罚分。 该种做法与原来的常量gap有一定区别，因此就需要改变动态规划公式，同时引入CS中的状态机可以帮助我们更好地理解这个问题。 上图中存在3个状态， 1）M：当前的比对情况下为match或mismatch 2）Ix：当前的比对情况为在seq2上打开一个gap，而seq1上为一个base 3）Iy：当前的比对情况为在seq1上打开一个gap，而seq2上为一个base 三者之间是可以相互转换的，通过d、e、s(x, y)来调整。 因此动态规划公式变为如下的形式， gap extension情况下的动态规划矩阵初始化 M(0,0)=0 Ix(i,0)=d+e∗(i−1)I_{x}(i,0)=d + e*(i-1) Ix​(i,0)=d+e∗(i−1) Iy(j,0)=d+e∗(j−1)I_{y}(j,0)=d+e*(j-1) Iy​(j,0)=d+e∗(j−1) 但由于$$I_{x}$$在第一行以及$$I_{y}$$在第一列的取值都是不存在的，因此定义为-inf。 同时，由于每一个cell都存在一种情况，我们需要建立3个矩阵来存储对应的信息，分别用M、X、Y来表示。 经初始化之后，就可以得到如下3个矩阵： 1）M 2）X 代表在列方向打开一个gap，即seq2上插入一个gap 3）Y 代表在行方向上打开一个gap，即seq1上插入一个gap gap extension情况下的动态规划矩阵回溯 3个矩阵，可以使用1个矩阵来记录当前的cell的数值来源，3种情况如下 来自M，即当前为一个match/mismatch，记录为0 来自X，即当前为一个gap open/gap extension，记录为1 来自Y，即当前为一个gap open/gap extension，记录为2 给个示例的回溯矩阵， 「步骤拆解」Local Alignment 步骤与Global Alignment近似，只是引入了一个0，就可以得到局部的最佳匹配。 公式如下， 代码实现 import numpy as np # Preset variables seq1 = &quot;TCGTAGACGA&quot; seq2 = &quot;ATAGAATGCGG&quot; print(f'The seq1 has length of {len(seq1)}') print(f'The seq2 has length of {len(seq2)}') match = 1 mismatch = -1 gap_open = -2 gap_extension = -1 # global MIN MIN = -float(&quot;inf&quot;) def identity_match(base1, base2): '''Note: this function is used to compare the bases and return match point or mismatch point''' if base1 == base2: return match else: return mismatch def createscorematrix(n, m): '''Note: this function is used to generate the original score function''' # Create match matrix, x matrix and y matrix m_mat = [np.zeros(m+1) for i in range(0, n+1)] x_mat = [np.zeros(m+1) for i in range(0, n+1)] y_mat = [np.zeros(m+1) for i in range(0, n+1)] return m_mat, x_mat, y_mat m_mat, x_mat, y_mat = createscorematrix(len(seq1), len(seq2)) # print(m_mat) # print(x_mat) # print(y_mat) def scorematrix_init(m_mat, x_mat, y_mat, d, e, local=False): '''Note: this function conduct the score matrix initialization''' '''Global Alignment''' if local == False: '''match matrix filling''' for i in range(0, len(m_mat)): for j in range(0, len(m_mat[0])): if i == 0 and j == 0: m_mat[i][j] = 0 elif i == 0 and j &gt; 0: m_mat[i][j] = MIN elif i &gt; 0 and j == 0: m_mat[i][j] = MIN # print(m_mat) for line in m_mat: r_list = [str(i) for i in line] print(' '.join(r_list)) '''x_matrix filling''' for i in range(0, len(x_mat)): for j in range(0, len(x_mat[0])): if i == 0 and j == 0: x_mat[i][j]=0 if i &gt; 0 and j == 0: x_mat[i][j] = d+e*(i-1) x_first_row = [0] x_first_row.extend([MIN]*(len(x_mat[0])-1)) x_mat[0] = x_first_row # print(x_mat) for line in x_mat: r_list = [str(i) for i in line] print(' '.join(r_list)) '''y_matrix filling''' for i in range(0, len(y_mat)): for j in range(0, len(y_mat[0])): if i == 0 and j == 0: y_mat[i][j]=0 elif i &gt; 0 and j == 0: y_mat[i][j] = MIN y_first_row = [0] y_first_row.extend([d+e*(i-1) for i in range(1, len(y_mat[0]-1))]) y_mat[0] = y_first_row # print(y_mat) for line in y_mat: r_list = [str(i) for i in line] print(' '.join(r_list)) return m_mat, x_mat, y_mat '''Local Alignment: Initialization step for Smith-Watermen is useless''' if local == True: return m_mat, x_mat, y_mat m_mat, x_mat, y_mat = scorematrix_init(m_mat, x_mat, y_mat, -2, -1, local=False) # m_mat, x_mat, y_mat = scorematrix_init(m_mat, x_mat, y_mat, -2, -1, local=True) def matrix_filling(m_mat, x_mat, y_mat, d, e, local=False): '''this function is used to create the scoring matrix using three dynamic programming, and building a tracing matrix to restore the paths for the retrieve of aliignments''' '''Global Alignment Activation''' if local == False: # Filling score matrix and record the trace trace_matrix = [np.zeros(len(m_mat[0])) for i in range(0, len(m_mat))] for i in range(1, len(m_mat)): # print(m_mat[0]) for j in range(1, len(m_mat[0])): # print(i, j) m_mat[i][j] = max( m_mat[i-1][j-1] + identity_match(seq1[i-1], seq2[j-1]), x_mat[i-1][j-1] + identity_match(seq1[i-1], seq2[j-1]), y_mat[i-1][j-1] + identity_match(seq1[i-1], seq2[j-1]) ) x_mat[i][j] = max(m_mat[i-1][j] + d, x_mat[i-1][j] + e) y_mat[i][j] = max(m_mat[i][j-1] + d, y_mat[i][j-1] + e) # for line in m_mat: # print(line) # Take the greatest values in these three matrix, # merge into one matrix, # and record the path new_mat = [np.zeros(len(m_mat[0])) for i in range(0, len(m_mat))] for i in range(0, len(m_mat)): for j in range(0, len(m_mat[0])): new_mat[i][j] = max(m_mat[i][j], x_mat[i][j], y_mat[i][j]) # Fill the trace matrix # Note: from match/mismatch is 0, from x_mat (open a gap in seq2) is 1, from y_mat (open a gap in seq1) if m_mat[i][j] == max(m_mat[i][j], x_mat[i][j], y_mat[i][j]): trace_matrix[i][j] = 0 elif x_mat[i][j] == max(m_mat[i][j], x_mat[i][j], y_mat[i][j]): trace_matrix[i][j] = 1 elif y_mat[i][j] == max(m_mat[i][j], x_mat[i][j], y_mat[i][j]): trace_matrix[i][j] = 2 # # Print out the scoring matrix # for line in new_mat: # r_list = [str(i) for i in line] # print('\\t'.join(r_list)) # # Print out the tracing matrix for line in trace_matrix: r_list = [str(i) for i in line] print('\\t'.join(r_list)) return new_mat, trace_matrix '''Local Alignment Activation''' if local == True: # Filling score matrix and record the trace trace_matrix = [np.zeros(len(m_mat[0])) for i in range(0, len(m_mat))] for i in range(1, len(m_mat)): # print(m_mat[0]) for j in range(1, len(m_mat[0])): # print(i, j) m_mat[i][j] = max( m_mat[i-1][j-1] + identity_match(seq1[i-1], seq2[j-1]), x_mat[i-1][j-1] + identity_match(seq1[i-1], seq2[j-1]), y_mat[i-1][j-1] + identity_match(seq1[i-1], seq2[j-1]), 0 ) x_mat[i][j] = max(m_mat[i-1][j] + d, x_mat[i-1][j] + e) y_mat[i][j] = max(m_mat[i][j-1] + d, y_mat[i][j-1] + e) # for line in m_mat: # print(line) # Take the Greatest values in these three matrix new_mat = [np.zeros(len(m_mat[0])) for i in range(0, len(m_mat))] for i in range(0, len(m_mat)): for j in range(0, len(m_mat[0])): new_mat[i][j] = max(m_mat[i][j], x_mat[i][j], y_mat[i][j]) if m_mat[i][j] == max(m_mat[i][j], x_mat[i][j], y_mat[i][j]): trace_matrix[i][j] = 0 elif x_mat[i][j] == max(m_mat[i][j], x_mat[i][j], y_mat[i][j]): trace_matrix[i][j] = 1 elif y_mat[i][j] == max(m_mat[i][j], x_mat[i][j], y_mat[i][j]): trace_matrix[i][j] = 2 # # Print out the scoring matrix # for line in new_mat: # r_list = [str(i) for i in line] # print(' '.join(r_list)) # # Print out the tracing matrix # for line in trace_matrix: # r_list = [str(i) for i in line] # print('\\t'.join(r_list)) return new_mat, trace_matrix score_matrix, trace_matrix = matrix_filling(m_mat, x_mat, y_mat, -2, -1, local=False) # score_matrix, trace_matrix = matrix_filling(m_mat, x_mat, y_mat, -2, -1, local=True) # seq1 = &quot;-TCGTAGACGA&quot; # seq2 = &quot;ATAGAATGCGG&quot; def global_backtracking(matrix, score_matrix): '''this function is used to trace back the input matrix and output the final alignment Note: the input matrix is trace matrix''' ti = len(seq1) tj = len(seq2) alignment1 = '' alignment2 = '' while (ti &gt; 0 or tj &gt; 0): # Choose to go left, up or diagonal cell = matrix[ti][tj] if cell == 0: alignment1 = seq1[ti-1] + alignment1 alignment2 = seq2[tj-1] + alignment2 ti -= 1 tj -= 1 elif cell == 1: alignment1 = seq1[ti-1] + alignment1 alignment2 = '-' + alignment2 ti -= 1 elif cell == 2: alignment1 = '-' + alignment1 alignment2 = seq2[tj-1] + alignment2 tj -= 1 # fmt_alignment = f'{alignment1}\\n{alignment2}' # print(fmt_alignment) # Formt the info info = f&quot;======The Global======\\n {alignment1}\\n {alignment2}\\nSCORE: {score_matrix[len(score_matrix)-1][len(score_matrix[0])-1]}&quot; print(info) global_backtracking(trace_matrix, score_matrix) def local_backtracking(trace_matrix, score_matrix): '''this function does backtracking like FUNCTION global_backtracking, but in the way of local aligment''' # Convert score matrix into Numpy array to find maximum value new_score_matrix = np.array(score_matrix) pos = np.unravel_index(np.argmax(new_score_matrix, axis=None), new_score_matrix.shape) # retrieve the maximum value ti = pos[0] tj = pos[1] # print(f'{ti}\\t{tj}') alignment1 = '' alignment2 = '' while (ti &gt; 0 or tj &gt; 0): if new_score_matrix[ti][tj] == 0: # stop local alignment back tracking when 0 values met break cell = trace_matrix[ti][tj] if cell == 0: alignment1 = seq1[ti-1] + alignment1 alignment2 = seq2[tj-1] + alignment2 ti -= 1 tj -= 1 elif cell == 1: alignment1 = seq1[ti-1] + alignment1 alignment2 = '-' + alignment2 ti -= 1 elif cell == 2: alignment1 = '-' + alignment1 alignment2 = seq2[tj-1] + alignment2 tj -= 1 info = f&quot;======The Local======\\n {alignment1}\\n {alignment2}\\nSCORE: {np.ndarray.max(new_score_matrix)}&quot; print(info) # local_backtracking(trace_matrix, score_matrix) 参考文献 [1] 《组学数据中的统计与分析》，田卫东 [2] https://users.soe.ucsc.edu/~karplus/bme205/f12/Alignment.html [3] https://www.youtube.com/watch?v=ZBD9he4Zp1E [4] Biological sequence analysis: Probabilistic models of proteins and nucleic acids ","link":"https://grideaBlog/post/globalandlocal_alignment/"},{"title":"「R语言刷题」数据类型篇 - 从tidyr和dplyr入手","content":"R语言到底可以用来做什么？ 在生物信息学领域，大数据、机器学习等方向，和Python一比，我感觉已经相形见绌， 所以为什么还要学习R语言？ 1）ggplot2的绘图体系非常完美 2）R base能够完成我日常小数据的分析 有很多人说，tidyverse给了R二次生命，但是在我看来它的编程思维，稍显奇怪，不太能够适应， 但是我属于一个钻牛角尖的人，既然学了，而且R我也没有完全丢掉，也是有一定必要学习一些重获新生的R语言。 因此这篇文章就是针对我在最近的R语言刷题中的总结。 数据读入 / 写出 从R base的角度看问题 read.table() read.csv() write.table() write.csv() 从readr的角度看问题 read_csv() read_delim() read_table() write_delim(df, 'filename.csv') write_csv(df, 'filename.csv') write_excel_csv(df, 'filename.csv') write_tsv(df, 'filename.csv') data.frame 从R base的角度看问题 如何随心所欲的操作data.frame？ 使用data.frame()创建数据库， 1）给定colname，再给定该变量对应的element（一般是vector格式），即 colname=c(elements) 2）依此类推，增添每一列 # 1. 数据框的创建 df &lt;- data.frame( &quot;grammer&quot; = c(&quot;Python&quot;,&quot;C&quot;,&quot;Java&quot;,&quot;GO&quot;,NA,&quot;SQL&quot;,&quot;PHP&quot;,&quot;Python&quot;), &quot;score&quot; = c(1,2,NA,4,5,6,7,10) ) 从tidyr的角度看问题 library(tidyr) df &lt;- tibble( &quot;grammer&quot; = c(&quot;Python&quot;,&quot;C&quot;,&quot;Java&quot;,&quot;GO&quot;,NA,&quot;SQL&quot;,&quot;PHP&quot;,&quot;Python&quot;), &quot;score&quot; = c(1,2,NA,4,5,6,7,10) ) # # A tibble: 8 x 2 # grammer score # &lt;chr&gt; &lt;dbl&gt; # 1 Python 1 # 2 C 2 # 3 Java NA # 4 GO 4 # 5 NA 5 # 6 SQL 6 # 7 PHP 7 # 8 Python 10 dplyr的衔接 dplyr提供了一系列用C++重新编写过的函数，能够加快我们处理数据的速度。 主要包含的函数如下， select() filter() mutate() group_by() arrange() 1）数据框列名的重新命名：names(), colnames(), rename() R base对列名进行修改的话，使用如下格式， # R base names(df)[2] &lt;- c('popularity') # dplyr df &lt;- df %&gt;% rename(popularity = score) 2）数据过滤：which(), filter() # R base df[which(df$popularity &gt; 3), ] # dplyr df %&gt;% filter(popularity &gt; 3) # 按照范围也是一样的， df %&gt;% filter(popularity &gt; 3 &amp; popularity &lt;7) 3）选择某一列数据：$, select() # R base df$popularity # dplyr df &lt;- df %&gt;% select(popularity, everything()) Note：tidyverse::everything()的作用，为选择数据框中的所有变量。 与select()函数一起使用，可以改变数据框中变量的排列顺序（e.g. 将变量A从变量B后移动到变量B前），以iris数据集为例， select(iris, everything()) # A tibble: 150 x 5 # Sepal.Length Sepal.Width Petal.Length Petal.Width Species # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; # 1 5.1 3.5 1.4 0.2 setosa select(iris, Species, everything()) # # A tibble: 150 x 5 # Species Sepal.Length Sepal.Width Petal.Length Petal.Width # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 setosa 5.1 3.5 1.4 0.2 可以看到Species被提前了。 4）将一行/多行数据添加到数据框中：rbind() row &lt;- data.frame( &quot;grammer&quot; = c(&quot;Perl&quot;), &quot;popularity&quot; = c(6.6) ) df &lt;- rbind(df, row) 5）分组：aggregate(), group_by R base：aggregate() dplyr：group_by + summarise # R base # 使用aggregate对df进行分组计算 library(flipAPI) data = DownloadXLSX(&quot;https://wiki.q-researchsoftware.com/images/1/1b/Aggregation_data.xlsx&quot;, want.row.names = FALSE, want.data.frame = TRUE) data.agg = aggregate(df, by = list(data$Role), FUN = mean) # dplyr # 使用group_by()分组，并进行相关计算 library(readr) df &lt;- read_csv('pandas120.csv') df %&gt;% group_by(education) %&gt;% # 设置用于分组的变量，此处为education summarise(mean = mean(salary)) 6）构建新的一列变量：mutate() # R base # 使用cbind()函数 # dplyr df &lt;- df %&gt;% mutate(test = paste0(df$education, df$createTime)) Note：paste0，先将变量转换为字符类型 7）将某一列的设置为索引：rownames(), column_to_rownames() # R base rownames(df) &lt;- df$createTime # dplyr # 使用column_to_rownames，该函数将某一列设置为行名后得到 df %&gt;% tibble::column_to_rownames('createTime') 8）数据排序 # R base # sort(): 返回的是排序后的结果 # order() df &lt;- df[order(df$popularity), ] # dplyr # 使用arrange() df &lt;- df %&gt;% arrange(popularity) df &lt;- df %&gt;% arrange(desc(popularity)) 9）数据的总结：summary(), summarize() 略 与data.frame有关的实例 1）填补NA 1）使用R包Hmisc中的impute()函数 2）使用impute函数的一般模式，一为输入的向量，二为如何填充NA值所定义的FUN Note：下列代码中的unlist，只是为了安全作用 library(Hmisc) index &lt;- which(is.na(df$popularity)) df$popularity &lt;- impute(df$popularity, (unlist(df[index-1, 2] + df[index+1, 2]))/2) 2）将宽数据转换为长数据 使用tidyr中的pivot_longer()函数， 一般的使用模式如下， pivot_longer(data, cols, names_to = &quot;name&quot;, # cols，定义一个vector用于选择df中的列并进行合并 values_to = &quot;value&quot;, # 每一行对应的元素 values_drop_na = FALSE) 举个例子， library(tidyr) df %&gt;% select(日期,`开盘价(元)`,`收盘价(元)`) %&gt;% pivot_longer(c(`开盘价(元)`,`收盘价(元)`), names_to='type',values_to='price') %&gt;% ggplot(aes(日期,price,color=type)) + geom_line(size=1.2) + scale_color_manual(values=c('steelblue','orange')) + theme_bw() + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.title = element_blank(), legend.position = c(0.86, 0.9) ) 3）读入数据的测试 假设现在有一个非常大的数据集，但是可以用read.csv等函数进行读入，然后后续用到的数据实际上只占了df的2列， 1）如何找到这两列？ 2）如何在后续的分析中只读取到这两列？ res &lt;- read.csv('数据1.csv',encoding = 'GBK',nrows = 3) # 测试读入3行数据 # 获取每一列的数据类型（e.g. integer、character） classes &lt;- sapply(res, class) classes[-match(c('positionName','salary'), names(classes))] &lt;- rep('NULL', length(classes) - 2) # 保留只想要选择的列 df &lt;- read.csv('数据1.csv', encoding = 'GBK', nrows = 10, colClasses = classes) # 使用&quot;colClasses=&quot;指定列 代码的解读， 1）测试读入数据，即读取3行 2）使用sapply获取每一列的数据类型 3）去除不想要的列的数值类型，即 -&gt;None 4）重新使用read.csv(, colClasses=)，对目的数据进行读取 4）使用sapply()创建一个数据框 df1 &lt;- sapply(20, function(n){ replicate(n, sample(1:100, 1)) }) %&gt;% as.data.frame(.) %&gt;% dplyr::rename(`0` = V1) R tips str()：查看对象每一个子对象的类型 R中有非常多的对象，比如 data.frame中的每一个变量都是一个子对象， 列表中的每一个$，均为一个子对象 str(df) # 'data.frame': 8 obs. of 2 variables: # $ grammer: Factor w/ 6 levels &quot;C&quot;,&quot;GO&quot;,&quot;Java&quot;,..: 5 1 3 2 NA 6 4 5 # $ score : num 1 2 NA 4 5 6 7 10 .：既是占位符，也是通配符 .在一些情况下，可以充当占位符，比如下列代码的含义为过滤掉salary小于10000的行，且查看过滤后的数据的行数， Note：可以类比为Shell中的- df &lt;- read_csv('pandas120.csv') df %&gt;% filter(salary &gt; 10000) %&gt;% dim(.) %&gt;% .[1] .的另一种含义为通配符，但是在此处不做过多赘述。 &quot;[&quot;：索引的另一种方式 直接举例说明， &quot;[&quot;(c(123, 12, 3), 1) # 123 &quot;[&quot;(c(123, 12, 3), 2) # 12 基于glue的格式化输出 1）使用R包glue（集合于tidyverse中），即可以直接理解为C、Pytohn中的格式化输出（e.g. sprintf） 2）paste(, collapse=) -&gt; .join() # library(readr) # df &lt;- read_csv('600000.SH.csv') library(glue) for (i in names(df)){ if(sum(is.na(df[,'日期'])) != 0){ res1 &lt;- which(is.na(df[,i])) res2 &lt;- paste(res1, collapse = ',') print(glue('列名：&quot;{i}&quot;, 第[{res2}]行有缺失值')) } } 参考资料 [1] R语言数据处理120题 ","link":"https://grideaBlog/post/R-exercises-1/"},{"title":"To change or not to change？","content":"Part I 专属于我的弯路 其实我一直都保留着，我想要研究做人类方面的研究的想法，一直都有。 但是本科阶段做了很多的傻事，也没有遇到一个好的老师，对我的方向有着怎么样的指引，我现在就正走在属于我的弯路上。 Part II 规划重要吗？ 规划自己的生活重要吗？ 虽然开学才短短不到20天，但是我却已经产生了要转硕士/转课题组的想法， 不为什么， 但是也为了什么。 为了想要完成自己曾经不敢去想的事情，为了将自己生命中宝贵的时间，都投入到自己感兴趣的事情上去。 在上一堂课的时候，林老师说，“规划人生不重要，我当年想读生物信息学，但是没有名额，最终现在在做mRNA……”， 引发了我的疑惑，我下课就去问他， 最终具体说了什么我忘了，大概说了一些，“你永远不知道明天会发生什么，明天也有可能直接被车了”、“路上碰到一个女孩，我想让她当我女朋友，最后没有成功，有什么关系呢？？我尽力去做了”， 所以，并不是规划不重要，而是能否接受现实与理想的不同，并继续努力好好生活。 “有时候，就是头脑一热，就干了” —— 林金钟 Part III 我的反抗、我的自由、我的激情 上面这句话来自加缪。 而我想写一句对应着我自己的话，“我的兴趣、我的坚持、我的义无反顾”。 作为一个小镇错题库，我不知道我是如何成长到现在这样的， 我看了很多的心理学书籍，我看了很多的哲学书籍，我也看了很多我兴趣方面的书籍。 我的父母不理解我想要做什么，大概也已经觉得我走到现在这一步就已经可以了， 但是，我不这样认为。 ","link":"https://grideaBlog/post/to-change-or-no-to-change/"},{"title":"「转录组」001｜WGCNA专题：实战原理两不误","content":"Hello，这里是即将开学的陈有朴。 表达矩阵的处理 后续分析所用到的数据，均为FPKM标准化后的表达矩阵。 从流程上对WGCNA进行解读 1）当对芯片数据或者RNA-Seq等数据完成分析之后，我们可以得到一张表达矩阵 一行为一个样本，一列为一个gene。 2）读取表达矩阵之后，对其进行adjacency矩阵的计算 adjacency矩阵，基于gene之间相关性的矩阵。每一个单元代表了2个gene之间的关联性（similarity）。 adjacency矩阵的构建，涉及到软阈值的选择，即构建一个幂函数，选择一个指数（power），强化强相关性，弱化弱相关性。 「adjacency矩阵的拓展」 adjacency矩阵有2种计算方式， 1）unsigned：aij=∣cor(xx,xj)∣βa_{ij} = |cor(x_{x}, x_{j})|^{β}aij​=∣cor(xx​,xj​)∣β 2）signed：aij=(0.5∗(1+cor))βa_{ij}=(0.5*(1+cor))^{β}aij​=(0.5∗(1+cor))β 第一种情况，认为具有高负相关的gene之间，是有联系的。 第二种情况，认为具有高负相关的gene之间，是没有联系的。 举个例子， 假设cor = -1，β=2，unsigned情况下，计算得到的adjacency为1，即gene之间高度关联。signed情况下，计算得到的adjacency为0，即gene之间无关联。 3）TOM矩阵的构建 引入一个指标，即topological overlaps的计算，用于定义该gene是否有多个高关联性的gene（用于gene module的构建） Cor(xi,xj)−&gt;TOM(xi,xj)Cor(x_{i},x_{j}) -&gt; TOM(x_{i}, x_{j}) Cor(xi​,xj​)−&gt;TOM(xi​,xj​) 「TOM矩阵的拓展」 但是针对unsigned类型的adjacency矩阵，可能会出现无法判断几个基因之间的关联关系。 比如现有i，j，k 3个gene，计算得到它们之间的关联关系为(+,+,−)(+,+,-)(+,+,−)， 1）i和j之间为正相关；2）i和k之间为正相关；3）j和k之间为负相关。 这就矛盾了，说明上面的关联关系可能受到了一些噪音的影响等等。 此时，引入signed TOM，其相较于unsigned TOM能够更好的解决gene之间的冲突情况。 Note：需要注意的是，当使用signed adjacency进行分析时，不会出现上述矛盾。 同时，这些矛盾作者都已经考虑到了，但还是有一定区别， TOMsimilarity()中，默认设置为TOMType = &quot;unsigned&quot; TOMsimilarityFromExpr()中，默认设置为TOMType = &quot;signed&quot; 4）TOM dissimilarity矩阵的构建 TOM dissimilarity矩阵代表了某个gene与其他gene之间的距离。 用上述矩阵进行聚类分析，得到gene module。 5）初步构建gene module 以TOM dissimilarity矩阵作为输入，进行聚类分析。 代码如下， # Turn data expression into topological overlap matrix power=sft$powerEstimate # 使用sft$powerEstimate调用预估出的软阈值 TOM = TOMsimilarityFromExpr(datExpr, power = power) dissTOM = 1-TOM # Plot gene tree geneTree = hclust(as.dist(dissTOM), method = &quot;average&quot;); # 用于后续cutreeDynamic()，对gene tree进行裁剪 # pdf(file = &quot;3-gene_cluster.pdf&quot;, width = 12, height = 9); plot(geneTree, xlab=&quot;&quot;, sub=&quot;&quot;, main = &quot;Gene clustering on TOM-based dissimilarity&quot;, labels = FALSE, hang = 0.04); # dev.off() 6）使用cutreeDynamic()进行聚类分析的优化 使用pairwise eigengene，进一步计算得到eigengene dissimilarity，用于聚类分析，筛选指标，最终合并gene module。 Note：eigengene为gene表达模式的指标（PCA降维得到的第一个主成分） cutreeDynamic()代码如下， cutreeDynamic labels2colors plotDendroAndColors # Module identification using dynamic tree cut dynamicMods = cutreeDynamic(dendro = geneTree, distM = dissTOM, deepSplit = 2, pamRespectsDendro = FALSE, minClusterSize = 30); table(dynamicMods) length(table(dynamicMods)) # Convert numeric labels into colors dynamicColors = labels2colors(dynamicMods) table(dynamicColors) # Plot the dendrogram and colors underneath # pdf(file = &quot;4-module_tree.pdf&quot;, width = 8, height = 6); plotDendroAndColors(geneTree, dynamicColors, &quot;Dynamic Tree Cut&quot;,dendroLabels = FALSE, hang = 0.03,addGuide = TRUE, guideHang = 0.05,main = &quot;Gene dendrogram and module colors&quot;) # dev.off() 绘制图如下， gene module合并代码如下 -&gt; 基于cutreeDynamic()的分析结果进行聚类分析的gene module的合并 mergeCloseModules：合并gene module plotDendroAndColors # Merge close modules MEDissThres=0.25 abline(h=MEDissThres, col = &quot;red&quot;) merge = mergeCloseModules(datExpr, dynamicColors, cutHeight = MEDissThres, verbose = 3) mergedColors = merge$colors mergedMEs = merge$newMEs # Plot merged module tree # pdf(file = &quot;5-merged_Module_Tree.pdf&quot;, width = 12, height = 9) plotDendroAndColors(geneTree, cbind(dynamicColors, mergedColors), c(&quot;Dynamic Tree Cut&quot;, &quot;Merged dynamic&quot;), dendroLabels = FALSE, hang = 0.03, addGuide = TRUE, guideHang = 0.05) # dev.off() # merge$oldMEs，为数据框，行为样本，列为对应的gene module，其中的数值代表了它们之间的关联度 write.table(merge$oldMEs,file=&quot;oldMEs.txt&quot;); write.table(merge$newMEs,file=&quot;newMEs.txt&quot;); 绘制图如下，可以看到有一些gene module被合并了， 7）将gene module与表型特征相联系 使用标准化的eigengene进行计算。 Cor(MEs,Traits) Cor(MEs, Traits) Cor(MEs,Traits) 代码如下， moduleEigengenes，计算每个gene module的特征值 moduleTraitCor = cor(MEs, datTraits, use = &quot;p&quot;) moduleTraitPvalue = corPvalueStudent(moduleTraitCor, nSamples) # Define numbers of genes and samples nGenes = ncol(datExpr); nSamples = nrow(datExpr); # Recalculate MEs with color labels MEs0 = moduleEigengenes(datExpr, mergedColors)$eigengenes MEs = orderMEs(MEs0) # r$&gt; MEs[1:5, 1:5] # MElightcyan1 MEdarkolivegreen MEgreenyellow MEcyan MEwhite # LH38_Z1_1 0.3093871 -0.01665288 0.1583588 -0.017809267 -0.01305706 # LH38_Z1_2 0.3318247 -0.01679628 0.1629585 -0.010450719 0.04613664 # LH38_Z1_3 0.3087032 -0.01762908 0.1527085 -0.018456470 -0.03661408 # LH38_Z2_1 0.3050361 -0.01431782 0.1589569 -0.020199995 0.06161998 # LH38_Z2_2 0.3422982 -0.01466196 0.1722949 -0.006663298 0.08192540 # 一些数据处理部分 # Read microbial data as traits bac_traits = read.table(&quot;traits_file/b_order_234.txt&quot;, header = T, sep = &quot;\\t&quot;) rownames(bac_traits) = bac_traits[, 1] bac_traits = bac_traits[, -1] # r$&gt; bac_traits[1:5, 1:5] # Pseudomonadales Enterobacteriales Xanthomonadales Burkholderiales Verrucomicrobiales # LH38_Z1_1 0.02120943 0.006338742 0.07261663 0.05920385 0.02674949 # LH38_Z1_2 0.04192444 0.009089757 0.06880071 0.05583164 0.02156440 # LH38_Z1_3 0.01393256 0.004525862 0.06961207 0.05100152 0.02189402 # LH39_Z1_1 0.11288033 0.013045132 0.07138692 0.04186106 0.01743154 # LH39_Z1_2 0.01214503 0.003410243 0.08236562 0.03733519 0.01953600 rownames(MEs) = paste(substr(rownames(MEs), 1, nchar(rownames(MEs))-1), rep(c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;), 60), sep = &quot;&quot;) # sample names should be consistent in eigen genes and traits !!!! bac_traits = bac_traits[match(rownames(MEs), rownames(bac_traits)), ] table(rownames(MEs) == rownames(bac_traits)) # Calculate pearson correlation coefficients between module eigen-genes and traits moduleTraitCor = cor(MEs, bac_traits, use = &quot;p&quot;); moduleTraitPvalue = corPvalueStudent(moduleTraitCor, nSamples); write.table(moduleTraitCor,file=&quot;moduleTrait_correlation.txt&quot;); write.table(moduleTraitPvalue,file=&quot;moduleTrait_pValue.txt&quot;); 结果图可视化代码如下， sizeGrWindow(10,6) # Will display correlations and their p-values # 合并 textMatrix = paste(signif(moduleTraitCor, 2), &quot;\\n(&quot;, signif(moduleTraitPvalue, 1), &quot;)&quot;, sep = &quot;&quot;); dim(textMatrix) = dim(moduleTraitCor) # pdf(&quot;module-traits-bacteria-order.pdf&quot;, width = 100, height = 30) par(mar = c(15, 12, 5, 5)); # Display the correlation values within a heatmap plot labeledHeatmap(Matrix = moduleTraitCor, xLabels = names(bac_traits), yLabels = names(MEs), ySymbols = names(MEs), colorLabels = FALSE, colors = greenWhiteRed(50), textMatrix = textMatrix, # 矩阵单元格上需要显示的信息 setStdMargins = FALSE, cex.text = 0.5, zlim = c(-1,1), main = paste(&quot;Module-trait relationships&quot;)) # dev.off() 结果图如下， Note：右边的图例，代表相关性程度 8）Hub gene的鉴定 2种方法， 与目标module有强关联性的gene（gene significance） 使用module membership来鉴定关键gene，Cor(i,ME)Cor(i,ME)Cor(i,ME) 一般先绘制出以module membership为x，gene significance为y的散点图。 代码如下， # 以相关性最高的模块为例 which(moduleTraitCor == max(moduleTraitCor, na.rm = TRUE), arr.ind = TRUE) # row col # MEskyblue3 25 30 Nitrosomonadales &lt;- as.data.frame(bac_traits[, 30]) names(Nitrosomonadales) = &quot;Nitrosomonadales&quot; modNames = substring(names(MEs), 3) # 去除ME前缀 # 计算module membership # 使用的是WGCNA自带cor函数，使用皮尔逊计算相关性 geneModuleMembership = as.data.frame(cor(datExpr, MEs, use = &quot;p&quot;)); geneModuleMembership[1:5, 1:5] # MElightcyan1 MEdarkolivegreen MEgreenyellow MEcyan MEwhite # Zm00001d001763 -0.045547074 -0.10334193 -0.20030129 0.22671189 0.1501121833 # Zm00001d001766 0.004752275 -0.01548396 -0.11130522 -0.22346048 0.0178421845 # Zm00001d001770 -0.286230379 -0.23340743 -0.39930773 0.22791746 0.0925346831 # Zm00001d001774 0.029505956 -0.06714112 -0.06323784 -0.34302212 0.1677206174 # Zm00001d001775 -0.029767437 -0.07642270 -0.13732818 -0.07876773 -0.0008678361 MMPvalue = as.data.frame(corPvalueStudent(as.matrix(geneModuleMembership), nSamples)); MMPvalue[1:5, 1:5] names(geneModuleMembership) = paste(&quot;MM&quot;, modNames, sep=&quot;&quot;); names(MMPvalue) = paste(&quot;p.MM&quot;, modNames, sep=&quot;&quot;); # 计算gene significance geneTraitSignificance = as.data.frame(cor(datExpr, Nitrosomonadales, use = &quot;p&quot;)); # head(geneTraitSignificance) # nrow(geneTraitSignificance) GSPvalue = as.data.frame(corPvalueStudent(as.matrix(geneTraitSignificance), nSamples)); names(geneTraitSignificance) = paste(&quot;GS.&quot;, names(Nitrosomonadales), sep=&quot;&quot;); names(GSPvalue) = paste(&quot;p.GS.&quot;, names(Nitrosomonadales), sep=&quot;&quot;); module = &quot;skyblue3&quot; column = match(module, modNames); # match(x, y)，找到y中x的索引位置 moduleGenes = mergedColors==module; # length(mergedColors) # nrow(geneModuleMembership) # nrow(geneTraitSignificance) # table(mergedColors) table(moduleGenes) sizeGrWindow(7, 7); par(mfrow = c(1,1)); verboseScatterplot(abs(geneModuleMembership[moduleGenes, column]), abs(geneTraitSignificance[moduleGenes, 1]), xlab = paste(&quot;Module Membership in&quot;, module, &quot;module&quot;), ylab = &quot;Gene significance for Nitrosomonadales&quot;, main = paste(&quot;Module membership vs. gene significance\\n&quot;), cex.main = 1.2, cex.lab = 1.2, cex.axis = 1.2, col = module) 散点图结果如下，可以看到module membership值越高的gene，其gene significance也越高。 其他 绘制gene module eigengenes与samples之间的热图 library(&quot;pheatmap&quot;) # Heatmap of old module eigen-genes and samples pdf(file=&quot;oldMEs.pdf&quot;,heigh=80,width=20) row.names(merge$oldMEs)=names(data0) # oldMEs，是一个矩阵 pheatmap(merge$oldMEs,cluster_col=T,cluster_row=T,show_rownames=T,show_colnames=T,fontsize=6) dev.off() # Heatmap of new module eigen-genes and samples # pdf(file=&quot;newMEs.pdf&quot;,heigh=60,width=20) row.names(merge$newMEs)=names(data0) pheatmap(merge$newMEs,cluster_col=T,cluster_row=T,show_rownames=T,show_colnames=T,fontsize=6) # dev.off() 参考资料 [1] https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/Rpackages/WGCNA/Tutorials/ [2] https://github.com/PengYuMaize/Yu2021NaturePlants [3] 跟着Nature Plants学数据分析：R语言WGCNA分析完整示例 [4] https://www.youtube.com/watch?v=BzYfg1lO3jw [5] https://www.biostars.org/p/288153/ [6] https://peterlangfelder.com/2018/11/25/signed-or-unsigned-which-network-type-is-preferable/ [7] https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/Rpackages/WGCNA/TechnicalReports/signedTOM.pdf ","link":"https://grideaBlog/post/WGCNA/"},{"title":"这可能是我写过最全的GATK笔记","content":"软件安装 官方网站：https://gatk.broadinstitute.org/hc/en-us 点击“Download GATK4”： 选择zip安装包： wget https://github.com/broadinstitute/gatk/releases/download/4.2.5.0/gatk-4.2.5.0.zip unzip gatk-4.2.5.0.zip 其他 安装GATK Best Practice所需要的软件：https://gatk.broadinstitute.org/hc/en-us/articles/360041320571--How-to-Install-all-software-packages-required-to-follow-the-GATK-Best-Practices 其他分析流程（bwa替代流程）：https://gatk.broadinstitute.org/hc/en-us/articles/4407897446939--How-to-Run-germline-single-sample-short-variant-discovery-in-DRAGEN-mode GATK：Jave选项 GATK实际使用的命令为：java -jar program.jar，但是为了GATK的开发者为了方便将其添加到环境变量，对其进行了封装，即使用安装目录下的gatk可执行脚本可直接运行： 下面给出一个封装前后，设置额外参数的例子： 未进行封装前，设置额外参数的方式为java -Xmx4G -jar gatk-package-4.2.2.0-local.jar 进行封装后，设置额外参数的方式为gatk --java-options &quot;-Xmx4G&quot; 官方文档如下： [1] https://gatk.broadinstitute.org/hc/en-us/articles/360035531892-GATK4-command-line-syntax GATK：HallotypeCaller 关于HallotypeCaller（GATK 3.6之后的版本）是否还需要使用重比对？ 答案是不需要。 细节请看： [1] https://github.com/broadinstitute/gatk-docs/blob/master/blog-2012-to-2019/2016-06-21-Changing_workflows_around_calling_SNPs_and_indels.md?id=7847 GATK：GenomicsDBImport 在完成gatk HallotypeCaller分析这一步之后，可以选择GenomicsDBImport将生成的gvcf文件进行整合，便于后续的joint genotyping。 【标注】 “GATK4 Best Practice for SNP and Indel”一般都选择GenomicsDBImport（而不是CombineGVCFs）进行gvcf文件的合并。GenomicsDBImport有一套独立的数据存储系统； GenomicsDBImport与CombineGVCFs功能类似 —— 合并gvcf文件。前者将genomic loci作为划分依据（e.g. chromosome, scaffold, contig），后者使用sample； 可以使用SelectVariants，对GenomicsDBImport产生的数据库内容进行访问； 要求输入数据 GenomicsDBImport所要求的输入数据，为HallotypeCaller添加-ERC GVCF或-ERC BP_RESOLUTION参数生成的结果文件，即gvcf文件。 参数设置（强制要求） 运行GenomicsDBImport的时候，有一些参数是必须的，还有一些参数是额外设置，可以用于增大文件读取速度或者进行个性化分析。 --genomicsdb-workspace-path # 构建GenomicsDatabase的目标文件夹 -V # gvcf文件名 --sample-name-map # 一个包含所有gvcf ID的文本，使用tab分隔符，第一列为sample ID，第二列为gvcf ID -L | --intervals # 选择需要合并的基因组区域（每一行一个染色体编号） 额外参数设置： --batch-size # 代表每批次能够读入多少个样本，默认为0，表示一次性全部读入。当样本数超过100时需要注意 输入文件 GenomicsDBImport特殊的数据存储格式（e.g. ） GenomicsDBImport：示例代码 # 每一个sample gvcf作为输入文件 gatk --java-options &quot;-Xmx4g -Xms4g&quot; GenomicsDBImport \\ -V data/gvcfs/mother.g.vcf.gz \\ -V data/gvcfs/father.g.vcf.gz \\ -V data/gvcfs/son.g.vcf.gz \\ --genomicsdb-workspace-path my_database \\ --tmp-dir=/path/to/large/tmp \\ -L 20 # 将所有sample gvcf名称放入map文件中 # 对应参数：--sample-name-map gatk --java-options &quot;-Xmx4g -Xms4g&quot; \\ GenomicsDBImport \\ --genomicsdb-workspace-path my_database \\ --batch-size 50 \\ -L chr1:1000-10000 \\ --sample-name-map cohort.sample_map \\ --tmp-dir /path/to/large/tmp \\ --reader-threads 5 # 将新sample添加到GenomicsDBImport数据库中 # 对应参数：--genomicsdb-update-workspace-path gatk --java-options &quot;-Xmx4g -Xms4g&quot; GenomicsDBImport \\ -V data/gvcfs/mother.g.vcf.gz \\ -V data/gvcfs/father.g.vcf.gz \\ -V data/gvcfs/son.g.vcf.gz \\ --genomicsdb-update-workspace-path my_database \\ --tmp-dir /path/to/large/tmp \\ 【标注】sample map文件使用tab分隔符，每一行一个gvcf文件名 sample1 sample1.vcf.gz sample2 sample2.vcf.gz sample3 sample3.vcf.gz 还有一些细节就看看官方文档吧~ [1] https://gatk.broadinstitute.org/hc/en-us/articles/360057439331-GenomicsDBImport [2] https://github.com/GenomicsDB/GenomicsDB/wiki GATK：CombineGVCFs CombineGVCFs的主要功能就是将HaplotypeCaller产生的gvcf文件，给合并。 【标注】 有且只能是HaplotypeCaller产生的gvcf可以用作输入文件 VCF文件的合并，使用的是MergeVcfs，是Picard下的一个工具 官方给出的说明是：1000+以上的样本推荐使用GenomicsDBImport CombineGVCFs：示例代码 gatk CombineGVCFs \\ -R reference.fasta \\ --variant sample1.g.vcf.gz \\ --variant sample2.g.vcf.gz \\ -O cohort.g.vcf.gz # 将染色体拆分运行 gatk --java-options &quot;-Xmx4g -Xms4g&quot; CombineGVCFs -L 1 -V DRR083661.g.vcf.gz -O chr1.vcf.gz -R /home/chphl/2022_3_4_WGS/00.ref/arab_ref.fa 官方说明文档如下： [1] https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs GATK：GenotypeGVCFs 这边有一个非常关键词，“joint genotyping”。 genotyping，实际上就是发现给定群体（数据）中的DNA变异，包括SNP、INDEL、non-variation位点等。 要求的输入数据是HaplotypeCaller附加“-ERC GVCF”或“-ERC BP_RESOLUTION”参数，所产生的gvcf文件 输出文件：VCF GenotypeGVCFs：示例代码 gatk --java-options &quot;-Xmx4g&quot; GenotypeGVCFs \\ -R Homo_sapiens_assembly38.fasta \\ -V input.g.vcf.gz \\ -O output.vcf.gz # 当使用GenomicsDBImport作为合并HaplotypeCaller输出数据的工具时， gatk GenotypeGVCFs \\ -R data/ref/ref.fasta \\ -V gendb://my_database \\ -O test_output.vcf 一些常用参数： --TMP_DIR # 使用暂时文件夹对结果文件进行保存 --max-genotype-count # 每一个位置的genotype数量上限，默认为1024 # 其他参数 --sample-ploidy # 在混池测序的时候需要注意，设置为“Number of samples in each pool * Sample Ploidy” 官方说明文档如下： [1] https://gatk.broadinstitute.org/hc/en-us/articles/360037057852-GenotypeGVCFs [2] https://gatk.broadinstitute.org/hc/en-us/articles/360035889971--How-to-Consolidate-GVCFs-for-joint-calling-with-GenotypeGVCFs GATK：SelectVariants SelectVariants用于选择给定VCF文件中的个体 &amp; SNP类型（Select a subset of variants from a VCF file），功能可以概述为： 1、从给定VCF文件中，挑选个体（参数：-sn） 2、从给定VCF文件中，挑选对应区域（参数：--intervals） 3、选择不同类型的SNP（参数：--select-type） e.g. 只挑选“INDELs” 一些常用参数： -R | --reference # 给定参考基因组（官网说明文档给的是NULL，可有有无？） --sn # 选择给定的样本，样本名保存在以.args结尾的文本中（一定要.args结尾！！！） -xl-sn # 反向选择样本 --intervals # 选择给定区域 --select-type-to-include | -select-type # 选择保留给定类型的一种variants（注意，只有一种） # 没有给定的情况下，默认保留所有位点 # ！！！多次设置来选择多种类型的位点 # 包括：INDEL, SNP, MIXED, MNP, SYMBOLIC, NO_VARIATION # INDEL：insertion和deletion # SNP：single nucleotide polymorphism # MIXED：同一个位置上，不仅有SNP，还有IDEL（在多个个体水平） # MNP：由相邻物理位置上组合成的SNP集合 # SYMBOLIC： # NO_VARIATION：非多态性位点 --restrict-alleles-to # 限制variants的类型为“ALL”，“BIALLELIC”，“MULTIALLELIC”其中的一种 --exclude-filtered # 启用该FLAG值时，输出结果中只会包括有“PASS”标记的位点 --set-filtered-gt-to-nocall # 默认情况下不开启，开启之后将“./.”位点全部过滤 其他参数： --selectExpressions # 选择variants的标准（自行定义） 需要注意的问题：IndexFeatureFile 在服务器中移动VCF文件时，转移到一个文件夹的时候忘记把对应VCF文件的索引也转移过来了 或者，本身就没有对该VCF创建索引 可以使用如下命令，对目标VCF文件建立索引 gatk --java-options &quot;-Xmx4G&quot; IndexFeatureFile --input input.vcf 需要注意的问题：CreateSequenceDictionary 在输入参考基因组（fasta格式）进行辅助分析的时候，不仅需要使用samtools对fasta文件进行fai索引的构建，还需要使用属于Picard工具包的CreateSequenceDictionary对其进行dict文件的构建： samtools faidx ref.fasta # 结果文件：ref.fasta.fai gatk CreateSequenceDictionary R=ref.fasta O=ref.dict # 结果文件：ref.dict 官方说明文档： [1] https://gatk.broadinstitute.org/hc/en-us/articles/360036362532-SelectVariants [2] https://gatk.broadinstitute.org/hc/en-us/articles/360035530752-What-types-of-variants-can-GATK-tools-detect-or-handle- [3] https://gatk.broadinstitute.org/hc/en-us/articles/360035890771-Biallelic-vs-Multiallelic-sites [4] https://www.strand-ngs.com/files/manual/reference/snp.html GATK：VariantFiltration VariantFiltration命令主要的功能是设置阈值，对SNP进行硬过滤，在FILTER这一列对位点进行标记。 一般被过滤的位点，被标记为自行给定的FILTER名称 没有被过滤的位点，被标记为PASS VariantFiltration：示例代码 gatk VariantFiltration \\ -R reference.fasta \\ -V input.vcf.gz \\ -O output.vcf.gz \\ --filter-name &quot;my_filter1&quot; \\ --filter-expression &quot;AB &lt; 0.2&quot; \\ --filter-name &quot;my_filter2&quot; \\ --filter-expression &quot;MQ0 &gt; 50&quot; 一些参数的说明 官方推荐将SNP和INDEL单独提取出来之后，再分别进行VariantFiltration。 【分析标注】SelectVariants使用“-select-type”对变异位点进行筛选时，只会根据给定类型，筛选单一子集（e.g. 给定选择SNP，就只会筛选SNP，不会选择SNP和INDEL共存的位点）。若需要选择混合类型的位点，使用参数“MIXED” # 示例hard-filtering gatk VariantFiltration \\ -V snps.vcf.gz \\ -filter &quot;QD &lt; 2.0&quot; --filter-name &quot;QD2&quot; \\ -filter &quot;QUAL &lt; 30.0&quot; --filter-name &quot;QUAL30&quot; \\ -filter &quot;SOR &gt; 3.0&quot; --filter-name &quot;SOR3&quot; \\ -filter &quot;FS &gt; 60.0&quot; --filter-name &quot;FS60&quot; \\ -filter &quot;MQ &lt; 40.0&quot; --filter-name &quot;MQ40&quot; \\ -filter &quot;MQRankSum &lt; -12.5&quot; --filter-name &quot;MQRankSum-12.5&quot; \\ -filter &quot;ReadPosRankSum &lt; -8.0&quot; --filter-name &quot;ReadPosRankSum-8&quot; \\ -O snps_filtered.vcf.gz gatk VariantFiltration \\ -V indels.vcf.gz \\ -filter &quot;QD &lt; 2.0&quot; --filter-name &quot;QD2&quot; \\ -filter &quot;QUAL &lt; 30.0&quot; --filter-name &quot;QUAL30&quot; \\ -filter &quot;FS &gt; 200.0&quot; --filter-name &quot;FS200&quot; \\ -filter &quot;ReadPosRankSum &lt; -20.0&quot; --filter-name &quot;ReadPosRankSum-20&quot; \\ -O indels_filtered.vcf.gz 再给出一篇文章中的过滤参数： 官方说明文档如下： [1] https://gatk.broadinstitute.org/hc/en-us/articles/360037434691-VariantFiltration [2] https://gatk.broadinstitute.org/hc/en-us/articles/360035531112--How-to-Filter-variants-either-with-VQSR-or-by-hard-filtering【主要看“2. Hard filter a cohort callset with VariantFiltration”这部分就行】 其他GATK命令 GATK：碱基质量矫正 碱基质量矫正（Base Quality Score Recalibration/BQSR），这一步分析主要用于检测由于测序仪器造成的系统误差。 BQSR理解 可以从以下2个方面对BQSR进行理解： 1、首先需要明确的一点是，BQSR是对碱基质量值进行矫正，而不是碱基。碱基质量值代表了该碱基的可信度（有百分之多少的概率，我们可以相信这个位点的测序情况是正确的，或者说有百分之多少的概率，认为这个位点是测错的），而base quality又与后续的SNP检测有关，因此对碱基质量值进行矫正非常关键。 【标注】也就是说，我们不能够决定一个低质量的A碱基，其原本是不是一个T，但是我们可以决定相信这个A的程度（基于base quality） 2、BQSR通过机器学习的方法对base quality进行矫正（本质都是回归） 【标注】 作者给出的例子，当AA在序列中连着出现的时候，认为后续出现的碱基都增加了1%被测序的概率，因此对应的base quality也需要下降 上述影响是具有累加性的 BQSR分析过程 1、基于输入数据（bam），建立一个协变模型（这个名词有待商榷） 结果文件为一个recalibration file 2、使用ApplyBQSR对原始输入文件的base quality进行矫正，结果文件为bam文件（新生成），同时在上述步骤要求输入先验SNP位点。具体过程： 统计全局差异（observed/reported quality vs expected/empirical quality） 每一个位点加上窗口计算的差异 + 每一个位点加上循环计算 &amp; dinucleotide差异 【分析标注】 “dinucleotide”【待解决】 推荐二次建立模型，看看recalibration对整体结果的影响变化。 BQSR：示例代码 # 还没学，用不到 官方文档如下： [1] https://gatk.broadinstitute.org/hc/en-us/articles/360035890531-Base-Quality-Score-Recalibration-BQSR- [2] https://gatk.broadinstitute.org/hc/en-us/articles/360036898312-BaseRecalibrator GATK：VariantRecalibrator &amp; ApplyVQSR VariantRecalibrator采用机器学习的方式，计算位点的VQSLOD值（取代该位点原本的QUAL），作为后续分析的指标，一般分为以下2个步骤： 使用高质量的VCF数据集，训练模型 将模型应用到我们自己的数据当中去 从上述的话，其实也能很明显地看出来，VariantRecalibrator &amp; ApplyVQSR这套方法的缺陷 —— 需要先验的高质量VCF数据集。 输入数据的要求： VariantRecalibrator：示例运行 # exome data gatk VariantRecalibrator \\ -R Homo_sapiens_assembly38.fasta \\ -V input.vcf.gz \\ --resource hapmap,known=false,training=true,truth=true,prior=15.0:hapmap_3.3.hg38.sites.vcf.gz \\ --resource omni,known=false,training=true,truth=false,prior=12.0:1000G_omni2.5.hg38.sites.vcf.gz \\ --resource 1000G,known=false,training=true,truth=false,prior=10.0:1000G_phase1.snps.high_confidence.hg38.vcf.gz \\ --resource dbsnp,known=true,training=false,truth=false,prior=2.0:Homo_sapiens_assembly38.dbsnp138.vcf.gz \\ -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR \\ -mode SNP \\ -O output.recal \\ --tranches-file output.tranches \\ --rscript-file output.plots.R # Allele-specific version of the SNP recalibration gatk VariantRecalibrator \\ -R Homo_sapiens_assembly38.fasta \\ -V input.vcf.gz \\ -AS \\ --resource hapmap,known=false,training=true,truth=true,prior=15.0:hapmap_3.3.hg38.sites.vcf.gz \\ --resource omni,known=false,training=true,truth=false,prior=12.0:1000G_omni2.5.hg38.sites.vcf.gz \\ --resource 1000G,known=false,training=true,truth=false,prior=10.0:1000G_phase1.snps.high_confidence.hg38.vcf.gz \\ --resource dbsnp,known=true,training=false,truth=false,prior=2.0:Homo_sapiens_assembly38.dbsnp138.vcf.gz \\ -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR \\ -mode SNP \\ -O output.AS.recal \\ --tranches-file output.AS.tranches \\ --rscript-file output.plots.AS.R 官方说明文档如下： [1] https://gatk.broadinstitute.org/hc/en-us/articles/360036510892-VariantRecalibrator [2] https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels- GATK：MergeVCFs MergeVCFs是包含在Picard内的一个工具，用于合并不同区域的VCF文件 对输入文件的要求如下： 每个VCF文件所包含的sample要一致 Input file headers must be contain compatible declarations for common annotations (INFO, FORMAT fields) and filters，即一些通用信息要包含（e.g. INFO, FORMAT, filters） 每个VCF文件包含的SNP，要求经过排序 MergeVCFs：示例代码 java -jar /path/picard.jar MergeVcfs \\ I=input_variants.01.vcf \\ I=input_variants.02.vcf.gz \\ O=output_variants.vcf.gz 题外话 GATK：什么是read group &amp; 如何添加read group信息 在进行bwa mem比对时，需要向bam/sam文件中，添加read group信息。 如果bam/sam文件中，没有对应信息，则会报错。 （1）GATK需要的read group信息是什么？ ID = Read group identifier # 每一个read group 独有的ID，每一对reads 均有一个独特的ID，可以自定义命名； PL = Platform # 测序平台：ILLUMINA, SOLID, LS454, HELICOS and PACBIO，不区分大小写； SM = sample # reads属于的样品名；SM要设定正确，因为GATK产生的VCF文件也使用这个名字; LB = DNA preparation library identifier # 对一个read group的reads进行重复序列标记时，需要使用LB来区分reads来自那条lane;有时候，同一个库可能在不同的lane上完成测序;为了加以区分， # 同一个或不同库只要是在不同的lane产生的reads都要单独给一个ID. 一般无特殊说明，成对儿read属于同一库，可自定义，比如：library1 （2）如何进行read group信息添加？ 图示： @A00312:194:HFK5KDSX2 # 测序仪器 4 # lane ID 1101 # tail坐标 24469 # tail x坐标 13150 # tail y坐标 # bwa比对时，对read group信息进行添加 bwa -R '@RG\\tID:lane_${lane}\\tPL:illumina\\tSM:${sample}' 参考资料如下： https://www.jianshu.com/p/c41e8f3266b4 写在最后 变异分析流程的软件有很多，金标准只有GATK一个（针对于WGS和WES），但是GATK自己本身就不断在更新。所以说，只学习别人软件的用法，这样还是走不长远的，我觉得还是需要以后自己有能力去做一款软件，靠这个吃饭，我觉得比较ok。 ","link":"https://grideaBlog/post/GATK-BESTPRACTICE/"}]}