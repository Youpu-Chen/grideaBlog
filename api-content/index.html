{"posts":[{"title":"内向人的“自救指南”（一）","content":"骨子里的我是一个内向人。不爱与人沟通，更喜欢独处，喜欢自己一个人做自己的事情。 但是这影响我本来的正常生活吗？不影响。因为内向从来都不是一种病，内向只是一种性格特征。而与人social也不是外向人的“特权”，内向人通过训练也是能够学会的。所谓的人情世故，在一个人身上的体现来自两个方面，1）原生家庭的影响，比如父母传授给孩子的技能；2）后天人在环境中的适应而发展出来的技能。注意，这只是技能，只要是技能，那么就可以通过刻意练习的方式提升，一切皆有规律可循。 若是为了自己感兴趣的事情去努力，内向的人一样可以很外向。—— 沃滋基·硕得 总想要结合个人经历，但是总觉得个人经历没有普适性，也越来越不愿意分享自己的所谓见闻。可能是觉得没啥好分享的，更想要自己消化，自己慢慢咀嚼，可能还时时反刍。但总归有很多事情需要自己去努力。 虽然内向的人，在一开始向外时，会很笨拙，但是可以慢慢练习。 因为如果不去向外，有的利益自己就会争取不到，做不到自己想要做的事情，比外向地去和人交流更痛苦，所以就要外向去交流。但是诸如此类的外向交流会很消耗自己的能力，要允许自己停下来，去补充这部分的能量。 写在后面 最近在看觉醒年代（是的，我就是不喜欢在跟风看电视剧，我喜欢在这类题材经历过了时间的检验之后，再花费自己的时间），里面有一句话让我印象深刻：“没有钱没有银子，你再大的光芒放不出来” 推荐阅读 阿巛关于内向的看法 ","link":"https://chenyoupu.top/post/innerpersion1/"},{"title":"内向人的“自救指南”（一）","content":"骨子里的我是一个内向人。不爱与人沟通，更喜欢独处，喜欢自己一个人做自己的事情。 但是这影响我本来的正常生活吗？不影响。因为内向从来都不是一种病，内向只是一种性格特征。而与人social也不是外向人的“特权”，内向人通过训练也是能够学会的。所谓的人情世故，在一个人身上的体现来自两个方面，1）原生家庭的影响，比如父母传授给孩子的技能；2）后天人在环境中的适应而发展出来的技能。注意，这只是技能，只要是技能，那么就可以通过刻意练习的方式提升，一切皆有规律可循。 若是为了自己感兴趣的事情去努力，内向的人一样可以很外向。—— 沃滋基·硕得 总想要结合个人经历，但是总觉得个人经历没有普适性，也越来越不愿意分享自己的所谓见闻。可能是觉得没啥好分享的，更想要自己消化，自己慢慢咀嚼，可能还时时反刍。但总归有很多事情需要自己去努力。 虽然内向的人，在一开始向外时，会很笨拙，但是可以慢慢练习。 因为如果不去向外，有的利益自己就会争取不到，做不到自己想要做的事情，比外向地去和人交流更痛苦，所以就要外向去交流。但是诸如此类的外向交流会很消耗自己的能力，要允许自己停下来，去补充这部分的能量。 写在后面 最近在看觉醒年代（是的，我就是不喜欢在跟风看电视剧，我喜欢在这类题材经历过了时间的检验之后，再花费自己的时间），里面有一句话让我印象深刻：“没有钱没有银子，你再大的光芒放不出来” 推荐阅读 阿巛关于内向的看法 ","link":"https://chenyoupu.top/post/nei-xiang-ren-de-zi-jiu-zhi-nan-yi/"},{"title":"「每日知识储备」gene-environment interaction如何影响heritability估计？","content":" 由于gene-environment interaction本身是一种non-linear effects，因此也可以直接从epistasis等类似的non-linear effects类比过来。 比如geneA对lung cancer有影响，但是在正常生活条件下，其对应的odd ratio为1.0，代表在这种环境下，该geneA对lung cancer的病发没有任何影响。但是当individual有吸烟等情况或者一些没有被测量的情况时，该geneA的odd ratio为5.0，则说明在环境因素的影响下，该geneA对lung cancer的发生有了非常大的影响。 因此，当没有考虑到对应的环境条件时，有的gene effect就不会被检测到，这就造成了heritability的丧失。 参考资料 “Ito et al. (1) showed a significant interaction between smoking status and the apurinic/ apyrimidinic endonuclease 1 protein coding gene (APE1) for lung cancer.” (Murcray et al., 2009, p. 219) (pdf) “Stern et al. (2) found smoking status to be an effect modifier of the association between the XPD codon 751 polymorphism and risk of bladder cancer.” (Murcray et al., 2009, p. 219) (pdf) ","link":"https://chenyoupu.top/post/dailykg005/"},{"title":"「每日知识储备」对variant calling的一点小误解","content":" 以人类参考基因组为例（只提供了一套染色体组的信息，本质是因为同源，两套基本上没有信息上的差异，只存在一些等位基因（即allele）上的差异） 图解 参考资料 人类基因组phasing的原理 从零开始完整学习全基因组测序数据分析：第4节构建WGS主流程 ","link":"https://chenyoupu.top/post/daily004/"},{"title":"「每日知识储备」说说我对PCA的理解","content":" 不讲公式，从个人角度来理解PCA到底做了些什么？ 首先举个例子，解释下为什么要“降维”？ 比如当前有10个individual，对他们进行了WGS测序，得到了他们10个人的variants calling结果。想要分析他们在祖源上的远近程度，如何分析？ 方法1：针对SNP1～5（SNP1，SNP2，...，SNP5），比如用hamming distance，可以计算得到10个individual中有5个individual（A，B，C，D，E）的hamming distance是0～5，而另外5个individual（F，G，H，I，J）与上述5个individual的hamming distance是7～10，那么我们就可以得到两个分群。 但是再换取另一批SNP数据时，比如SNP100～500时，不仅计算量大，还否定了之前的计算结果，那么这10个individual的祖源到底是什么样的呢？ 方法2：针对WGS测序得到的所有variants来重新构建变量，且保证重新构建的每个变量之间是正交的，即不相关，保证原始数据的损失达到最小（这部分用variance来衡量） 重新构建的第一个变量称为PC1，第二个称为PC2，依此类推。而每个主成分代表的variance是依次减少的。若前两个主成分（或者三个）能够解释原始数据比较高的比例，那么我们对数据降维的目的就达到了。 而individual在两个主成分（或者三个）上的distribution，就在一定程度上代表了他们genetic material的差异 保证重新构建的变量之间是正交，是为了极大限度地提升对原始数据的提取 为什么用variance来衡量，就比如一个标准正态分布，若取它的一个截断分布，那么实际上就损失了一部分方差，因此方差是一个很好的指代信息损失量的指标 利用PC来理解regional population structure 当经过PCA重新组合得到的主成分之后，每个主成分的构成实际上是如下的公式， PC1=β∗SNP1+β∗SNP2+...+β∗SNP3+...PC1 = \\beta*SNP_{1} + \\beta*SNP_{2} + ... + \\beta*SNP_{3} + ... PC1=β∗SNP1​+β∗SNP2​+...+β∗SNP3​+... 则两个individual如果在PC1轴上距离很远，说明它们在用于构建PC1的SNP上具有比较大的差异，但是如果在PC2轴上距离很近，则说明它们在用于构建PC2的SNP上比较相似，可能形成了regional population structure。 推荐阅读 阿泽对PCA的讲解，有公式 ","link":"https://chenyoupu.top/post/dailykg003/"},{"title":"「每日知识储备」GWAS中的effect size是什么？","content":"effect size实际上就是利用线性回归构建genetic data和phenotype之间关系的回归系数β\\betaβ。举个例子，比如不同genotype对height的影响， 如果是logistic regression，则是odd ratio SNPaSNP_{a}SNPa​出现时，对身高的影响可能是0.0001 cm；而SNPbSNP_{b}SNPb​出现时，则对身高的影响是-0.001 cm。这就体现出了不同的variants对身高的影响的效应不同和effect的方向性不同，而上述的数值简单理解就是对应了GWAS回归方程中的回归系数。 ","link":"https://chenyoupu.top/post/dailykg002/"},{"title":"「GWAS那些事儿」Odd Ratio的计算方式","content":"Odd Ratio和β\\betaβ｜logistic &amp; linear 区分GWAS summary statistics中的是effect是odd ratio还是β\\betaβ（即回归系数）时，需要考虑进来的一个因素： 经常看到的log(odd ratio)log(odd\\,\\,ratio)log(oddratio)是为了logistic regression的方便求解 GWAS到底是针对binary phenotype开展的还是针对conitnuous phenotype开展的？ 如果是针对binary phenotype，即case和control开展的，那么在构建回归模型来估计每个SNP对phenotype的影响时，使用的是logistic回归，最后计算的是一个阈值（概率），超过了即case，没有超过则是control。 logistic regression中的odd ratio是两个对立事件发生的概率做了一个比值：p1−p\\frac{p}{1-p}1−pp​ 数值的正负决定了variable（e.g.，SNP）和outcome关联的方向性 计算过程中是保持了之前所有units（e.g.，SNP的累积之和）不变的情况下，当前加入的SNP unit对outcome的影响 association test中的odd ratio是基于列联表的计算，两种方法的计算本质实际上是一样的： a∗db∗c\\frac{a*d}{b*c} b∗ca∗d​ 一般考量的是exposure和outcome之间的关系，最常见的即case和control，且列联表中element的位置变了，odd ratio的结果和含义也就都变了。 Odd Ratio｜计算原理 斯坦佛大学这张slides解释的非常清晰：首先计算odd，再计算odd ratio 比如genotype AT出现的情况下，individual患病的概率是0.8。这是一个明显的条件概率问题。而当genotype TT出现的情况下，individual患病的概率则是0.2。 从这里其实就可以观察到不同的genotype对individual是否有对应disease的susceptability是不同的，但是如何量化？答案即odd ratio。 针对同一个genotype，其出现的情况下，患病和不患病之间的差距是多大？ odd=P(Disease∣GAT)P(non−Disease∣GAT)odd = \\frac{P(Disease|G_{AT})}{P(non-Disease|G_{AT})}odd=P(non−Disease∣GAT​)P(Disease∣GAT​)​ 针对两个genotype，患病和不患病之间的差距有多大？ odd ratio=P(Disease∣GAT)P(non−Disease∣GAT)P(Disease∣GTT)P(non−Disease∣GTT)odd\\,\\,ratio = \\frac{\\frac{P(Disease|G_{AT})}{P(non-Disease|G_{AT})}}{\\frac{P(Disease|G_{TT})}{P(non-Disease|G_{TT})}}oddratio=P(non−Disease∣GTT​)P(Disease∣GTT​)​P(non−Disease∣GAT​)P(Disease∣GAT​)​​ 上述的概率，在logistic regression是可以直接拟合出来的。但是在association test中实际上一样， 下述计算odd ratio的意义是为了比较C allele相较于T allele对disease occurrence的影响， 而这个式子（ORC=875∗19401860∗675OR_{C} = \\frac{875*1940}{1860*675}ORC​=1860∗675875∗1940​）可以拆开看： 在C allele出现的情况下，individual患病的概率可以直接通过古典概型计算：P(Diseaase∣C)=875875+1869P(Diseaase|C)=\\frac{875}{875+1869}P(Diseaase∣C)=875+1869875​，同样的P(Diseaase∣non−C)=1860875+1860P(Diseaase|non-C)=\\frac{1860}{875+1860}P(Diseaase∣non−C)=875+18601860​ 而在计算整体的odd ratio时，分母均被消除了，直接简化成了如下的式子，本质上和logistic regression是一致的。 参考资料 www.ncbi.nlm.nih.gov/pmc... Understanding Logistic Regression — the Odds Ratio, Sigmoid, MLE, et al 笔记 | GWAS 操作流程3：plink关联分析--完结篇 斯坦佛：Odd以及Odd ratio的计算 ","link":"https://chenyoupu.top/post/gwas-or/"},{"title":"「GWAS那些事儿」fine-mapping","content":"为什么要做fine-mapping？ 从GWAS分析结果中，排除LD以及特定生物学背景的噪音，鉴定到casual variants对理解disease发生以及鉴定后续受到影响的gene和pathway有重要作用 “Overcoming LD and identifying the context-specific variants that are causal to a trait is imperative for understanding disease mechanisms and confidently identifying which downstream genes and pathways are affected.” (Broekema et al., 2020, p. 3) (pdf) translate GWAS loci to a functional understanding of the associated trait, while taking cell-type- and disease-specific context into account. zhuanlan.zhihu.com/p/106... 参考资料 zhuanlan.zhihu.com/p/106... fine-mapping｜方法 TWAS-based fine-mapping FOCUS：TWAS-fine-mapping methods github.com/bogdanlab/foc... ","link":"https://chenyoupu.top/post/gwas-fine-mapping/"},{"title":"「GWAS那些事儿」incomplete LD对heritability估计的影响","content":"有一个前提假设： 一个SNPjSNP_{j}SNPj​的效应往往是周围SNP效应之和，而这部分效应则自然由连锁不平衡决定 我的理解， casual variants和SNP对phenotypic variation的贡献程度往往是不一样的，且casual variants本身的特点也与common variants不同，比如casual variants由于具有更高的effect，在群体中的frequecny不会太高，这是由于纯化选择的作用。 而不完全的连锁，则会导致所使用的SNP set对heritability的低估。这部分是考虑到了casual variants和周围具有effect但不是casual的SNPs之间的linkage disequilibrium对SNP effect的影响 出处：番茄泛基因组文章 “Incomplete LD between molecular markers and causal variants leads to the underestimation of heritability9” (Zhou et al., 2022, p. 529) (pdf) 举个例子 在2010年这篇文章中，Yang Jian老师对于casual variants与周围SNP的incomplete LD所产生的effect是这样描述以及做了如下的simulation的， 将casual variants和common SNPs之间存在的incomplete LD导致的SNP effect bias estimation，记做GjkG_{jk}Gjk​和AjkA_{jk}Ajk​之间的差异， GjkG_{jk}Gjk​：基于casual variants所估计出来的effect AjkA_{jk}Ajk​：基于SNPs所估计出来的effect “Lack of complete LD is manifested as a difference between the genomic relationship between each pair of subjects j and k at the causal variants (Gjk) and the relationship between the same individuals calculated from the SNPs (Ajk).” (Yang et al., 2010, p. 2) (pdf) 将SNP set划分为common variants set以及casual variants set。 casual variants数据集中包含的variants，其频率θ\\thetaθ在0.1～05之间，作者构建的emprical β\\betaβ（用于矫正AjkA_{jk}Ajk​）的计算公式为：1−c+1/Nvar(Ajk)1-\\frac{c+1/N}{var(A_{jk})}1−var(Ajk​)c+1/N​ c这个参数比较关键，通过不同的MAF casual variants set，估计出来的数值是不同的，其代表了对SNP effect的低估程度 casual variants到底对common SNP的估计到底有怎么样的影响？ 当casual variants的allele frequency threshold被设置为0.1时，矫正后的heritability为0.8（非常接近familial studies的研究结果） 写在后面 在考虑associative SNP的真实effect时，需要考虑到的假设即，一个SNP的effect是周围SNP effect与自身effect的总和，因此就受到LD的影响。 “当casual variants的allele frequency threshold被设置为0.1时”，并不就代表casual variants的allele frequency就为0.1 参考资料 Common SNPs explain a large proportion of the heritability for human height ","link":"https://chenyoupu.top/post/gwas-incomplete-LD/"},{"title":"「每日知识储备」molecular markrs","content":"molecular markrs是什么？ 指基因组上的一段区域、一段DNA。不同的语境也可以指代不同的意思，比如还可以指代为SNPs。 比如如下的这段话是Yang Jian老师10年的文章中提到的genotyepd SNP，但是在黄三文老师的22年的番茄泛基因组文章中则表述为了“molecular markers” 果然，是我看的文献还不够多 也可以称作genetic markers ","link":"https://chenyoupu.top/post/dailykg001/"},{"title":"「GWAS那些事儿」数量遗传学应该关注谁的文章？","content":"写在前面 持续更新 经过数十年的研究，GWAS已经证实了自己方法本身上的可行性 Peter M. Visscher（关于missing heritability的综述，就是这位大佬写的） Yang Jian 近期代表性工作：GCTA（可用于heritability estitation） David J. Balding 近期代表性工作：LDAK-GBAT ","link":"https://chenyoupu.top/post/gwas-famous/"},{"title":"「GWAS之外」GWAS之外｜还有哪些关联分析方法？","content":" FUSION gusevlab.org/projects/fu... 包括TWAS和RWAS 其他 为什么要发展TWAS？ TWAS的发展是为了弥补GWAS的一些限制： 解释关联：GWAS虽然可以识别与性状或疾病相关的基因座，但通常无法提供这些关联的生物学解释。TWAS通过考虑基因表达数据，有助于更好地理解为什么某些基因与性状相关（SNP的存在是否影响了转录本层面的生物学机制）。 功能注释：TWAS可以用于识别潜在的功能性基因。它不仅告诉我们哪些基因与性状相关，还可以帮助我们了解这些基因在性状或疾病发病机制中的具体角色。 更精确的关联：TWAS可以提供更准确的关联信息，因为它结合了基因表达数据，减少了假阳性的可能性。 总之，GWAS和TWAS是两种互补的遗传学方法，用于研究基因与性状或疾病之间的关系。TWAS的发展有助于增强我们对基因与性状之间关联的理解，并提供更多有关基因功能和生物学机制的信息。 TWAS与LD之间的关系？ #GWAS#​TWAS statistics，在GWAS的基础上考虑了转录组的数据构建的统计量来找casual variants。但是TWAS仍旧会受到LD的影响。 TWAS加入了expression weight，即具有类似expression weight（对同一类转录本有相似影响的SNPs，才能够被当作与目标表型的casual SNPs）。但是由于LD的影响，在计算最终TWAS signal时，周围non-casual SNPs由于LD的影响，也会具有和casual SNPs近似水平的association signal —— LD tagging effect 图解 ","link":"https://chenyoupu.top/post/gwas-other/"},{"title":"「GWAS那些事儿」heritability的估计有哪些工具/模型？","content":"写在前面 持续更新 LDSC：baseline LD model github.com/bulik/ldsc LDAK model dougspeed.com/ldak/，官方series下载链接 github.com/takiy-berrand... ","link":"https://chenyoupu.top/post/gwas-heritability-estimate/"},{"title":"「GWAS那些事儿」heritability和genetic correlation估计中的liability scale","content":"liability scale是什么？ the &quot;liability scale&quot; refers to a hypothetical scale used to model the underlying genetic liability or genetic risk for a specific trait or disease. ","link":"https://chenyoupu.top/post/gwas-liability-scale/"},{"title":"「GWAS那些事儿」linear regression中的Z-Score","content":" Z-score在GWAS summary statistics中的含义是什么？ The Z-score quantifies how many standard deviations an observed association statistic is from the expected null distribution under the assumption of no association. Association Testing: In a GWAS, genetic variants across the genome are tested for their association with a particular trait or disease. The most common statistical test used for this purpose is logistic regression for binary traits (e.g., disease status) or linear regression for continuous traits (e.g., height). Z-Score Calculation: After performing the association test for each SNP, a Z-score is calculated for that SNP. The formula for calculating the Z-score is typically as follows: Z = (β - β₀) / SE Z is the Z-score. β is the estimated effect size or regression coefficient for the SNP, representing the change in the trait value associated with each additional copy of the variant. β₀ is the null hypothesis value for β, often assumed to be zero under the null hypothesis of no association. SE is the standard error of the estimated effect size β. Interpretation: The Z-score measures how many standard deviations (SE) the estimated effect size (β) is away from the null hypothesis value (β₀). A high absolute Z-score indicates a strong association between the SNP and the trait, whereas a low or close-to-zero Z-score suggests a weak or no association. Significance Threshold: Researchers typically establish a significance threshold (e.g., Z-score greater than 5 or corresponding p-value threshold) to determine which associations are statistically significant. Associations with Z-scores exceeding this threshold are considered noteworthy and may indicate a genuine genetic influence on the trait or disease. Positive and Negative Z-Scores: The sign of the Z-score indicates the direction of the effect. A positive Z-score suggests that the variant is positively associated with an increase in the trait (e.g., risk allele for a disease). A negative Z-score suggests a negative association (e.g., protective allele for a disease). ","link":"https://chenyoupu.top/post/gwas-zscore/"},{"title":"「GWAS那些事儿」LDSC｜LD Score的计算与LD regression构建","content":"写在前面 什么是LD score? LD Score衡量了target region的genetic correlation SNPjSNP_{j}SNPj​的LD score可以被定义为该SNP与一定范围内其他SNP的r2r^{2}r2之和。​ LD score衡量了该SNP标记的遗传变异性的大小。 为什么需要开展LD regression analysis？ 在GWAS研究中，多基因性（polygenicity，即若干较小的基因效应）和干扰因素引起的偏差（e.g.，隐性关联 cryptic relatedness，群体分层 population stratification等）都会造成检验的统计量的分布偏高（inflated） 统计量指什么？即经过卡方检验检验得到的对应SNP对目标表型的作用，用χ2\\chi^{2}χ2来衡量 造成association signal（SNP）对应的统计量分布偏高，是否是真的对目标表型有显著作用（有如此高的作用）？ 如何将这部分不知道由于是polygenicity造成，还是由populaton stratification等不准确因素造成的effect给估计出来？答案：LD regression（实际上还有很多别的模型来做这这个heritability估计） 通过LD score regression，我们可以通过研究检验统计量与连锁不平衡（linkage disequilibrium）之间的关系来定量分析每部分的影响，即对干扰因素平均效应的估计值进行计算 LD Score的计算原理是什么？ GWAS检验中，对一个SNP效应量的估计通常也会包含与该SNP成LD的其他SNP的效应，也就是说一个与其他SNP呈现高LD的SNP，通常也会有更高的卡方检验量。 LD regression的原理是什么？ LDSC｜安装 M1芯片不支持（因为LDSC基于Python 2.7） 推荐在Server或者Win虚拟机上运行 git clone https://github.com/bulik/ldsc.git cd ldsc mamba env create --file environment.yml mamba activate ldsc LDSC｜LD Score Calculation 输入文件：要求PLINK处理过后的genotype数据 .bed/.bim/.fam作为输入文件，可以使用PLINK VCF来生成 作者推荐的LD Score估计遗传范围（选择用于计算LD Score遗传距离的窗口）：1 centiMorgan (cM) window 可以使用plink --cm-map来保留遗传距离 在针对MHC等复杂区域（e.g.，LD延展度更高的region），需要设置更大的window size；具有比较高的recombination rate就不需要设置太大的window size It is sensible to use a larger window (as measured in kb) in regions like the MHC where LD spans over tens of megabases than in regions with high recombination rate, where LD doesn't extend beyond ~100kb. 以LDSC官方提供的1kg_eur.tar.bz2来说明toy data的结果。 01）Univariate LD Score python ldsc.py\\ --bfile\\ --l2\\ --ld-wind-cm 1\\ --out 22 --l2，指示计算LD Score --ld-wind-cm代表选择的window size（基于遗传距离），还可以直接基于物理距离、SNP个数等 结果文件解读 1）22.log 包含所使用的cml以及基本运行结果， Beginning analysis at Fri Jan 30 10:58:44 2015 Read list of 19156 SNPs from 22.bim Read list of 379 individuals from 22.fam Reading genotypes from 22.bed After filtering, 19156 SNPs remain ～～～Estimating LD Score.～～～ Writing LD Scores for 19156 SNPs to 22.l2.ldscore.gz 2）22.l2.M, 22.l2.M_5_50 第一个文件包含了目标region所包含的SNP数目 第二个文件包含了过滤MAF（&gt;5%）的SNP数目 LDSC一般基于后者来进行heritability的估计 3）22.l2.ldscore.gz CHR：chromosome number SNP：variantID BP：LDSC自动对variantID对应的物理位置进行排序 gunzip -c 22.l2.ldscore.gz | head CHR SNP BP L2 22 rs9617528 16061016 1.271 22 rs4911642 16504399 1.805 22 rs140378 16877135 3.849 22 rs131560 16877230 3.769 22 rs7287144 16886873 7.226 22 rs5748616 16888900 7.379 22 rs5748662 16892858 7.195 22 rs5994034 16894090 2.898 22 rs4010554 16894264 6.975 02）Partitioned LD Score 略 LDSC｜LD regression 需要的输入文件， GWAS summary statistics 计算好的LD Score（需要针对对应的人群进行计算） 结果文件解读包含：heritability、genetic correlation、LD regression估计出来的intercetp（衡量了polygenicity之外的因素对SNP effect的影响）等 01）genetic correlation &amp; heritability 1）数据下载 &amp; 格式转换 # wget www.med.unc.edu/pgc/files/resultfiles/pgc.cross.bip.zip wget www.med.unc.edu/pgc/files/resultfiles/pgc.cross.scz.zip wget https://data.broadinstitute.org/alkesgroup/LDSCORE/eur_w_ld_chr.tar.bz2 wget https://data.broadinstitute.org/alkesgroup/LDSCORE/w_hm3.snplist.bz2 # 由于GWAS summary statistics不能直接用于ldsc的计算，需要转换 munge_sumstats.py \\ --sumstats pgc.cross.SCZ17.2013-05.txt \\ --N 17115 \\ --out scz \\ --merge-alleles w_hm3.snplist munge_sumstats.py \\ --sumstats pgc.cross.BIP11.2013-05.txt \\ --N 11810 \\ --out bip \\ --merge-alleles w_hm3.snplist 转换之后生成.log文件，包含的信息：使用的命令以及header对应的信息以及对summary statistics过滤的condition信息 一般的过滤条件：INFO &gt; 0.9, MAF &gt; 0.01 and 0 &lt; P &lt;= 1.，去除了INDELs、strand ambiguous SNPs、SNPs with duplicated rs numbers log文件的最后一部分给出了summary statistics的meta信息， Metadata: Mean chi^2 = 1.229 Lambda GC = 1.201 Max chi^2 = 32.4 11 Genome-wide significant SNPs (some may have been removed by filtering). Conversion finished at Mon Apr 4 13:21:29 2016 Total time elapsed: 16.07s 举个例子， Reading list of SNPs for allele merge from w_hm3.snplist Read 1217311 SNPs for allele merge. Reading sumstats from pgc.cross.SCZ17.2013-05.txt into memory 5000000.0 SNPs at a time. Read 1237958 SNPs from --sumstats file. Removed 137131 SNPs not in --merge-alleles. Removed 0 SNPs with missing values. Removed 256286 SNPs with INFO &lt;= 0.9. Removed 0 SNPs with MAF &lt;= 0.01. Removed 0 SNPs with out-of-bounds p-values. Removed 2 variants that were not SNPs or were strand-ambiguous. 844539 SNPs remain. Removed 0 SNPs with duplicated rs numbers (844539 SNPs remain). Using N = 17115.0 Median value of or was 1.0, which seems sensible. Removed 39 SNPs whose alleles did not match --merge-alleles (844500 SNPs remain). Writing summary statistics for 1217311 SNPs (844500 with nonmissing beta) to scz.sumstats.gz. 2）遗传力估计 + 遗传相关性估计 # rg ldsc.py \\ --rg scz.sumstats.gz,bip.sumstats.gz \\ --ref-ld-chr eur_w_ld_chr/ \\ --w-ld-chr eur_w_ld_chr/ \\ --out scz_bip # h2 ldsc.py \\ --h2 scz.sumstats.gz \\ --ref-ld-chr eur_w_ld_chr/ \\ --w-ld-chr eur_w_ld_chr/ \\ --out scz_h2 参数解读 --rg，计算genetic correlation（作者给出的例子中直接以SCZ和bipolar为例） 与使用--h2来计算得到的heritability存在不同 作者对此的解释：总的来说就是所使用的SNP dataset不同，rg由于需要同时计算genetic correlation，所以并不是直接使用对应表型的SNP dataset --w-ld｜--w-ld-chr，在进行SNPjSNP_{j}SNPj​的LD回归的估算时，理论上要考虑到该SNP和别的SNP的r2r^{2}r2之和，但是在实际中LD回归对SNP的权重不敏感，所以使用--ref-ld-chr即可 结果解读 以--rg的分析结果为例 Lambda GC，代表了估计结果受到population stratification的影响程度 计算公式：lambda GC=median chi20.4549lambda\\,\\,GC=\\frac{median\\,\\,chi^{2}}{0.4549}lambdaGC=0.4549medianchi2​， 0.4549，the expected median of a variable from the χ12\\chi^{2}_{1}χ12​ distribution www.ncbi.nlm.nih.gov/pmc... χ2\\chi^{2}χ2，通过比较case和control中的genotype得到的卡方统计量 by comparing the observed genotype frequencies in cases (individuals with the trait or disease) and controls (individuals without the trait or disease). The chi-square statistic measures the extent to which the observed genotype frequencies deviate from what would be expected under the null hypothesis of no association between the SNP and the trait. Intercept，the LD Score regression intercept Ratio（attenuation ratio）：intercept−1mean(chi2)−1\\frac{intercept-1}{mean(chi^2)-1}mean(chi2)−1intercept−1​，用于衡量在LD regression中，polygenicity之外的因素对（理想情况下，应为0）SNP effect的影响（mean χ2mean\\,\\,\\chi^{2}meanχ2） Heritability of phenotype 1 --------------------------- Total Observed scale h2: 0.5907 (0.0484) Lambda GC: 1.2038 Mean Chi^2: 1.2336 Intercept: 1.0014 (0.0113) Ratio: 0.0059 (0.0482) Heritability of phenotype 2/2 ----------------------------- Total Observed scale h2: 0.5221 (0.0531) Lambda GC: 1.1364 Mean Chi^2: 1.1436 Intercept: 1.0013 (0.0094) Ratio: 0.0093 (0.0652) genetic covariance： Genetic Covariance ------------------ Total Observed scale gencov: 0.3644 (0.0368) Mean z1*z2: 0.1226 Intercept: 0.0037 (0.0071) genetic correlaton： Genetic Correlation ------------------- Genetic Correlation: 0.6561 (0.0605) Z-score: 10.8503 P: 1.9880e-27 02）将heritability结果转换到liability scale 由于ldsc估计出来的heritability是obseved scale，为了研究之间的可比较性，需要转换到liability scale， ldsc.py \\ --rg scz.sumstats.gz,bip.sumstats.gz \\ --ref-ld-chr eur_w_ld_chr/ \\ --w-ld-chr eur_w_ld_chr/ \\ --out scz_bip \\ --samp-prev 0.5,0.5 \\ --pop-prev 0.01,0.01 参考资料 LDSC：LD Score的计算 LDSC：heritability + genetic correlation的估计 GWAS Lab：LDSC原理 GWAS Lab：LDSC计算LD Scores 资源 zenodo.org/record/776871... ","link":"https://chenyoupu.top/post/gwas-ldsc/"},{"title":"「有谱talk」002｜遇到问题，解决问题","content":"写在前面 虽然嘴上说着要碰到问题解决问题，但是发现自己好像总是热衷于解决知识上的问题，但是在人际关系上的问题，自己却更倾向于逃避。这一类问题对我来说，又或者说对现在的我来说太难了 个人事务上遇到很大的困境，不知该如何解决，所以总是会选择逃避，或者间断性地麻痹自己 同时又喜欢在人面前“证明”自己，所谓的成熟，害怕别人如何看待我，觉得我不成熟，但是我不成熟和他人有什么关系？所以现在总在学习赤焰男孩，“nobody cares，我就是我自己世界的王” 但是我也的确发现自己的不成熟，对自己的职业发展不够上心，收集行政事务上的信息、与行政上的人沟通等，这些都是我的弱项，需要在往后的生活中锻炼。 “nobody cares，我就是我自己世界的王”，蛮有意思，自我一点，但是不是自我意识过剩一点。 ","link":"https://chenyoupu.top/post/youputalk002/"},{"title":"「有谱音乐说」001｜我是个“健将”","content":"我是冷静的 稳定的 从来不任性的 把水端得最平的 不配赢的 站错了队形的 如影随形的破事 哪怕永远不会停 不撕破脸 我会难为情的 我要深刻 要厚重 要工整 不能有漏洞 要说个道理 我要你用效率去对抗虚无 套上件最花的衣服 却排不出体内的瘀毒 我做不到 羞愧于纯粹的快乐 不敢直说我爱着 我不是品 我就是爱喝 押多少才能算合格 都在笑堂吉诃德 为什么作者写窗帘是蓝色的 因为它就是蓝色的 我做不到 我真的在努力去克制 保持我 I内向人的特质 自己对自己呵斥 可我真的需要一首歌的时间放空 我不在乎这是比赛 我要“发疯” ","link":"https://chenyoupu.top/post/you-pu-yin-le-shuo-001orwo-shi-ge-jian-jiang/"},{"title":"「有谱talk」001｜利益关系就是最纯洁的关系","content":"写在前面 我个人在生活中比较礼貌的表现，实际上是为了防止别人伤害到自己，但是最近发生的很多事情，让我意识到一个点，要学会像小恶魔一样，用自己的弱点来武装自己，也还需要允许自己被伤害，还要在人际交往过程中懂得打“好人牌”和“坏人牌” 让暴风雨来得更猛烈些吧 ——高尔基 很多时候是这样的。不仅是在本科，我去教别人东西，别人会很不客气地说来找我“讨论”，实际上自己什么都不会，就仅仅是为了在女生面前装*，我很好奇这样的意义，甚至觉得这样没有意义，自己难道已经忘了自己的身高以及其他优势吗？何必总是维持自己的“完美形象”和“高大形象” 到了研究生，居然发现还是这样。 我本希望营造一种与我讨论是没有“架子”的感觉，因为从我之前很多的经历里面出发，找人请教往往需要放低自己的身段，求着别人，别人都不一定肯教（但是其实现在大家都是很好交流的，也没有太大的架子） 可现在的实际呢？你教了朋友东西，所谓的朋友仿佛在背后还要捅你一刀，这样的何必所谓朋友呢？也可能是我贪图了，我可能认为别人也愿意与我交朋友。 最纯洁的关系就是利益关系 别做朋友，只在乎利益，这样就是最纯洁的关系。 既然本身的出发点就是利益，希望别人帮自己，那就别谈什么友谊、友情、情谊等形而上学的概念，都是放屁。 你对我有所图，我对你有所图，互相满足利益即可，切忌发展朋友关系，因为有的人属实不值得，别太深交。 写在后面 总归来说，还是提升自己连接soft connections的能力，提升自己拉资源的能力，努力让自己成为资源，减少对所谓的朋友的渴求。 朋友，有你会更好，但抱歉，我自己一人时也足够好。 抱歉，男女平等、女权主义，我不想讨论，我做好我自己，我尽我所能尊重女性，在需要帮助的时候尽我所能，其余时刻，我只想搞我自己的“事业”，做我自己感兴趣的事，别的事情，我一律不关心。 Whatever you said means nothing to me... 尊重他人命运，放下助人情节 ","link":"https://chenyoupu.top/post/youputalk001/"},{"title":"「文献写作工作流」002｜siyuan + Zotero联动 - 一篇manuscript的诞生","content":"写在前面 科研中，写作其实是非常重要的一环，就算分析会做，就算算法会整，不能够将自己的想法给清晰地reticulate出来，还是难免会有些折磨 那么从日常的博客写作中，到底能够在“写一篇文章”上帮助到我多少呢？ 我以一篇review的诞生为例。 idea的产生 “对一个领域的了解程度，至少需要对这个领域内的100篇经典文献完成阅读”。 100篇并不是一个强制要求的数目，但是侧面的说明了一个问题： 这个领域内前人都做了些什么？ 这个领域内的热点和痛点是什么？ 这个领域内目前最大的challenges是什么？ 那么针对写一篇综述的任务，就需要researcher对领域内的文章有一个非常inclusive的理解。 我的工作流：Zotero + Bette Notes（plugin）+ 思源笔记 分别简述一个这两个软件的好处， Zotero：开源，足够多的插件满足我的需求；利用坚果云可以达到很好的跨设备同步，在工位就大电脑主力输出，如果离开工位，则可以使用另一部电脑来查看大电脑的文献笔记等 思源笔记：虽然typora很优秀，但是typora也仅仅只能作为markdown的编辑器来使用，并不是一个非常便捷的笔记管理软件。为了更好的实现第二大脑的概念，思源笔记就是我认为非常好的解决方案： 1）本地，重在写作，写在当下，这些都是非常重要的需求。Notion作为一款打开了笔记管理新时代的软件，打开速度让我感觉到很难受，所以就pass。但是不得不说，Notion的数据库功能是非常好用的，因此在文献条目以及笔记阅读等方面，我还是会采用Database + Tag的方式来管理文献。不追求可以的All in One，各取所需即可。 2）相对便捷的设备同步。利用阿里云、腾讯云等云服务，思源笔记也可以非常轻松的实现跨设备同步，但是由于是对整体进行上传，同步速度在wolai、Notion的比较上略逊一筹。 而我所采用的idea收集方式如下， 将Zotero中对应的条目直接复制到思源笔记当中，保存着跳转链接，这个可以让我非常好的对idea追根溯源 同时我以day为单位，每日对综述撰写的内容进行整理和不断补充，当素材足够的时候，即可以直接开始对应部分的撰写，而这两个超链接让我能够很好的找到出处。 做科研，出处是非常重要的。 manuscript的产生 而在思源笔记当中完成了对草稿的撰写之后，我才用的方式如下， 导出。让vscode的markdown做该做的事， 这部分内容实际上是思源笔记现在对citekeys并没有一个非常好的支持，但是D大也说了如果之后思源笔记的科研用户多了，或许会考虑对该功能的更好实现 Zotero Better Bibtex + Pandoc帮助生成word版本的manuscript 1）better bibtex的配置 citekey format的设置：该部分确定了之后在vscode中要如何对item进行引用 如何想要更好的达到或者说和google scholar保持一致，还有一些内容需要配置， exclusive fields：去除掉导出的bib文件中不需要的field postscript的设置， 别问，问就是抄的。该部分只保留对应的info，但是如果其中出现了中文，提取也会出现问题。目前最好的解决方案也还是使用connector添加item时，就对文献的year/Date进行修正，以免后期增多了之后，自己都懒得整理了。小习惯造就big success if (Translator.BetterTeX) { if(tex.has['eventtitle']){ tex.add({ name: 'booktitle', value: tex.has['eventtitle'].value }); delete tex.has['eventtitle']; } if(tex.has['date']){ tex.add({ name: 'year', value: tex.has['date'].value }); delete tex.has['date']; } if(tex.has['journaltitle']){ tex.add({ name: 'journal', value: tex.has['journaltitle'].value }) delete tex.has['journaltitle']; } else if(tex.has['shortjournal']){ tex.add({ name: 'journal', value: tex.has['shortjournal'].value }) delete tex.has['shortjournal']; } delete tex.has['shortjournal']; } 2）markdown snippet vscode能够有code snippet加快编程效率，那markdown照样也行， markdonw.json的设置 start_paper中代表了一整个code snippet，prefix则是我们需要去定义的提示词 setting.json的设置。由于vscode中没有默认开启markdown的提示词，因此我们需要修改下， 3）pandoc 一款过于经典的工具，各类格式转换都能完成，写这个软件的教授还不是计算机出身，哲学教授嗷！ 在此步骤中还可以加入目标期刊的文献要求格式 pandoc input.md \\ -o test.docx --bibliography test1.bib \\ --citeproc -M reference-section-title=&quot;Reference&quot; 参考资料 ","link":"https://chenyoupu.top/post/wpaper002/"},{"title":"看懂R背后的运行，我五分钟解决了别人一天才能解决的问题","content":"看懂R背后的运行，我五分钟解决了别人一天才能解决的问题 基于Rscript来直接运行脚本，实际上会存在内存上限的问题（一般出现在tidyverse merge数据的过程中），但是在Linux环境下设置运行内存上限的方法和Win是不一样的，我就用一行代码帮助小伙伴解决了一丢丢小问题， library(unix) rlimit_as(1e13) # approximately 120G 结果：三分钟解决问题。。 周末继续总结继续学习继续看文献。 ‍ ","link":"https://chenyoupu.top/post/看懂R背后的运行，我五分钟解决了别人一天才能解决的问题/"},{"title":"「R | parallel」第一篇｜gene level的FST计算","content":"写在前面 聊聊我自己。群体遗传学研究的对象是allele，而针对一个population，就算是一个population，其内部存在的variants也是相当多的（由于genetic drift的影响），所以要处理的下游数据难免就会呈现records多到“上天”的情况 ‍ R | parallel的使用 虽然R语言是一门针对统计学的语言，但并不就代表它不可以实现最基本的计算功能，multiprocessing。 需要使用的packages，parallel​。 就拿我自己做的分析来举例： 现存一个vcftools计算得到的FST结果文件，分别记录了从chromosome1到chromosome21的per-site FST计算结果； 我的reference gene BED从UCSC Genome Table Browser下载得到，从共记录了两万多条gene records 如何计算得到每一个gene的FST？ 前提条件说了很多了，只是为了希望做分析的人能够理解这个问题。 但是在给出自己的代码之前，先给出R parallel的基本使用。 R | parallele | multiprocessing 直接使用multiprocessing方法时，不需要自行创建cluster来预设对应包含线程数的sockets，而用起来就像lapply一样，但是与其不同的是，lapply是将所有的函数依次应用到输入的每一个list上，而multiprocessing的lapply则是同时发动所有的任务， 使用函数， mclapply() mcmapply() R | parallele | sockets sockets类型的multiprocessing与上述提到的基本方法，在使用上的不同需要首先创建一个clusters，同时在完成运算之后需要释放这个cluster， 同时还需要将应用的数据export到cluster中 cluster_1 &lt;- makeCluster(16) # 可以使用detectCores()来检测总共有多少个核 stopCluster(cluster_1) ‍ R | 我的population genetics实战 核心模块， chr.vector​，是包含了chr1tochr22的chromosome tag的向量，需要先将其输出到cluster，才能够应用于cluster计算 但是我还存在的疑问：为什么input_df和gene_bed就不export就不会报错了呢？ Note：这里的理解，就好比神经网络模型使用CUDA时，也需要将对应张量加载到GPU上 parLapply(cluster_1, chr.vector, df = input_df, fun = function(input_df, chr_number, gene_bed){ # retrieve output dataset using chromosome tag {1..21} to split the FST dataset chr_number_input_df &lt;- input_df %&gt;% filter(chr==chr_number) ... } cluster_1 &lt;- makeCluster(16) # using 16 cores to speed up the FST calculation process. clusterExport(cluster_1, &quot;chr.vector&quot;) # export chromosome vector the sockets # FST_list &lt;- parLapply(cluster_1, chr.vector, df = input_df, fun = function(input_df, chr_number, gene_bed){ # retrieve output dataset using chromosome tag {1..21} to split the FST dataset chr_number_input_df &lt;- input_df %&gt;% filter(chr==chr_number) new_chr_number_input_df = chr_number_input_df %&gt;% left_join(gene_bed, by=c(&quot;chr&quot;)) # only save the validated records new_chr_number_input_df &lt;- new_chr_number_input_df %&gt;% filter((pos &gt;= start_pos &amp; pos &lt;= end_pos)) # using 0 to replace negative value new_chr_number_input_df$weight_FST[new_chr_number_input_df$weight_FST &lt; 0] &lt;- 0 # calculate FST for each gene cal_df &lt;- new_chr_number_input_df %&gt;% group_by(gene) %&gt;% summarize( Mean_Fst = mean(weight_FST, na.rm = TRUE) ) return(cal_df) }) # close cluster stopCluster(cluster_1) 推荐阅读 bookdown.org/rdpeng/rpro... www.r-bloggers.com/2019/... 创建sockets还可以使用不同的模式。。 ‍ ","link":"https://chenyoupu.top/post/R  parallel  第一篇/"},{"title":"「Population Genetics随笔」002｜FST - By Weir and Cockerham","content":"Weir and Cockerham | Weighted FSTF_{ST}FST​计算公式 Wright提出的FST原始版本，并没有考虑到sample size对FSTF_{ST}FST​计算的影响，因此才有了之后Weir and Cockerham对FSTF_{ST}FST​计算的改进。 而在展开介绍Weir和Cockerhem是如何计算FSTF_{ST}FST​之前，先看看Wright是如何计算， F，the correlation of genes within individuals（inbreeding） θ，the correlation of genes of different individuals in the same population f，the correlation of genes within individuals within populations θ，即指代FSTF_{ST}FST​。 在WC FSTF_{ST}FST​ estimator中，其计算公式如下， Note：该版本计算得到的FST，与vcftools略有不同，尚且认为是自己写作了吧。 但是上述文献中最终给出的，也仅仅是FST的最终计算公式，没有给出一个更加详尽的推导。 ‍ Weir | FSTF_{ST}FST​推导 如下的理解来自02年Weir发表对应文献/tutorial。 首选需要阐明的几个点， θθθ（FSTF_{ST}FST​），从最简单的意义上可以理解为两个allele来自同一个ancestor的概率，即IBD（identity by descent），但是更为直观的general，则为从任意一个或不同的population中抽样得到两个allele，他们之间的correlation coefficient 1）假设现在一个sample中，第j个allele，若其碱基类型为μ，则等于1；若其碱基类型不为μ，则等于0。用xjμx_{jμ}xjμ​来表示， ​ 那么现在针对期望population allele frequency为如下的形式， Note：此处公式存在一点问题（个人角度），应该论述为ε(∑n=inxjun)=pμε(\\frac{\\sum_{n=i}^{n}x_{ju}}{n})=p_{μ}ε(n∑n=in​xju​​)=pμ​ ε，代表E(X)E(X)E(X) ε(xju xj′μ)=E(xju xj′μ)=E(xju)E(xj′μ))+Cov(xju,xj′μ)ε(x_{ju}\\,\\,x_{j^{&#x27;}μ}) = E(x_{ju}\\,\\,x_{j^{&#x27;}μ}) = E(x_{ju})E(x_{j^{&#x27;}μ)}) + Cov(x_{ju},x_{j^{&#x27;}μ})ε(xju​xj′μ​)=E(xju​xj′μ​)=E(xju​)E(xj′μ)​)+Cov(xju​,xj′μ​) 当xju=xj′μx_{ju}=x_{j^{&#x27;}μ}xju​=xj′μ​，则Cov(xju,xj′μ)=Var(xju)=pμ(1−pμ)Cov(x_{ju},x_{j^{&#x27;}μ})=Var(x_{ju})=p_{μ}(1-p_{μ})Cov(xju​,xj′μ​)=Var(xju​)=pμ​(1−pμ​)，但是因为两者不相等，所以需要用θ来平衡他们之间的关系，而从生物学意义的角度来理解， 若第j位和第j'位之间，完全不存在联系，完全不可能是IBD，则ε(xju xj′μ)=E(xju)⋅E(xju)=pμ2ε(x_{ju}\\,\\,x_{j^{&#x27;}μ}) = E(x_{ju})·E(x_{ju}) = p_{μ}^{2}ε(xju​xj′μ​)=E(xju​)⋅E(xju​)=pμ2​ 若第j位和第j'位之间，在一定程度上是IBD，则需要接着考虑θ。 2）但是现在不从一个sample的角度考虑，而是从2个population之间的角度考虑， Note：若前提为non-random mating，则说明两个群体之间存在了divergence、isolation等 因此，此时的θθθ进一步引申为了衡量population differentiation的指标。 而针对sample allele frequency则由如下的公式计算得到， 且由于不同population size，抽样得到的sample计算出来的p~μ\\tilde{p}_{μ}p~​μ​存在偏差（Nicolas提出其服从参数为pu，πμ⋅θμp_{u}，π_{μ}·θ_{μ}pu​，πμ​⋅θμ​的正态分布） 而Weir于1984年提出的Weighted FSTF_{ST}FST​，为了消除不同population size对衡量θθθ的影响，构建了如下的两个平方差， 最终θ^Mμ\\hat{θ}_{Mμ}θ^Mμ​则表示为， ‍ 推荐阅读 Note：我看了很久还是没看懂。。得再看看 Estimating and interpreting FST: The impact of rare variants（Nick Patterson，2013年Review） weirhill_annrevgenetFst_2002.pdf Estimating F-Statistics for the Analysis of Population Structure（1984年，Weir原文） 题外话 用bedtools + R tidyverse来计算每个gene的FST忒慢了（这怎么“Have a nice day”），欢迎有缘人后台留言优秀方法！ ","link":"https://chenyoupu.top/post/popgen_FST/"},{"title":"「Population Genetics随笔」001｜coalescent theory","content":"写在前面 虽然学习群体遗传学的过程中，肯定是需要看教材，更多地阅读书的，但是有的教材中要么就是缺公式，要是就是新手不友好，所以看别人帖子也是一种不为过的学习方式，但是也切记不能忽略了背后的数学描述生物的过程。 这篇文章是对G-CAT上coalescent theory的理解和总结，推荐阅读原博客。 ‍ coalescence | 定义 当一个个体或者一个群体中，发生mutation时，就产生了2个不同碱基类型的allele，这是一个前向（forward）的过程。 而coalescence，是一个逆向（backwards）的过程，即推断群体中两个allele，共享同一个ancestral allele的概率是多大。 ‍ coalescence | 影响因素 将backwards合并理解为一个抽样过程，选定一个allele（A）的时候，即从ancestral population中（有效群体大小为NeN_{e}Ne​）随机抽选一个allele作为ancestral allele的概率为12Ne\\frac{1}{2N_{e}}2Ne​1​。那么再选中一个allele（B），其ancestral allele与A相同的概率还是为12Ne\\frac{1}{2N_{e}}2Ne​1​。 而在current population中，有多少种AB allele组合呢？答案是n!(n−2)!2!\\frac{n!}{(n-2)!2!}(n−2)!2!n!​种。 总而言之，针对一个allele，其coalescent rate和effective population size，针对一整个群体来说更是如此。 而在demographic history中，NeN_{e}Ne​并不是一成不变的，会受到环境的各种影响，而反映在群体上的则有population bottleneck、population expansion。 而针对具有不同NeN_{e}Ne​的群体来说，更是如此， ‍ coalescence | how to apply or what its application? 1）bottlenecks testing 既然coalescent rate会随着有效群体大小的变化而变化，那其实也就说明了它可以反映生物学背后的故事，那么是什么故事呢？ 当群体因为外界环境影响，有效群体大小变得特别小的时候，the number of alleles in total减少，群体水平的variants variation变小了，从图上看长这样， 2）introgression testing 当两个population存在introgression或者horizontal gene transfer时，就会存在一些allele，它们backward collapse的时间，相较于population的spliting time是更早的， ‍ 3）spliting time testing 一句话结束：population divergence或者说speciation发生的时候，coalescent events出现的次数也升高到了最高， 但是有的coalescent可能发生在spliting之前，又可能发生在spliting之后，即基于coalescence估计出来的divergence time是approximation，只能是不断的逼近，一种永远无法抵达的真实。 ‍ 推荐阅读 theg-cat.com/2018/11/09/... ‍ ","link":"https://chenyoupu.top/post/popgen_coalscent/"},{"title":"「文献写作工作流」001｜Zotero - A Better Way to Organize Your Literature","content":"Zotero | A Better Way to Organize Your Literature | 第1弹来了 写在前面 作为一个尚且还在搞科研的科研狗，当然还是得管理好自己的学习笔记。 作为早期体验过Mendeley的一批小众人，我觉得还是对文献的管理存在一定的要求。 App要美观 文献组织要合理 笔记功能 这里比较鸡肋的还是笔记功能，虽然市面上已经存在很多笔记软件，但是就算是好用的软件，还是得在两个App之间不断来回反复横跳，略嫌麻烦。因此有没有更好的解决方案呢？ ‍ better notes | 内嵌式的markdown笔记 为了避免发生，一边看文献还得一边打开一个Word或者外置markdown软件（比如typora），能够自如地在Zotero内部进行笔记管理，better notes非常好的满足了我的需求， 内嵌式的markdown类笔记软件，让我不需要再不同的笔记软件之前切换 提供了自定义模板等功能。让我能够更好地以“我”的方式去读文献，总结知识。 另外两个点， 就算想要用自己常用的笔记软件再来作为复盘，也是可以直接将better notes工作区内的内容之间复制到目标笔记软件中，即可以实现跳转到better notes笔记内，进而再跳转到对应的页面。 Quicker，则是一款轻量级的工具包，能够实现将Zotero中的文献选中，“复制”到笔记再实现跳转和双链笔记 ‍ color tags | 最简单，但却最实用 有很多各种华丽呼哨的笔记记录方式，仿佛color tags，已经被相忘于江湖。 但可别小瞧了最简单的方法，最简单的，往往却最实用。 我自己总结的color tags阅读文献的方式如下， 绿色：Methods部分 / 文章中方法的细节部分 黄色：文章中所使用的统计量、计算方法 红色：Results部分 / 文章的重要结论 蓝色：文章中，不清晰的知识点 紫色：文章中引用的、后续可以继续阅读的参考文献 橙色：想要学的英语语法、词组 而针对一篇文献，又可以很轻松地使用“插入笔记模板”的格式，直接插入到better notes当中，再做进一步的整理， 示例 | color tags导入到better notes ​ ​ ‍ 隐藏功能 | Zotero Dark themes 对于需要长时间坐在办公桌前的同学，Zotero原始配色就算不上什么有益眼睛健康的配色了，从而转向是否存在绿色、黑色皮肤的存在。 打开咕咕，直接搜索即可食用。 链接如下：github.com/Rosmaninho/Zo... ‍ 我的资源 [1] https://github.com/Youpu-Chen/Zotero_configs ‍ ","link":"https://chenyoupu.top/post/wpaper001/"},{"title":"「Theoretical Evolutionary Genetics」读书笔记（二）Linkage Disequilibrium","content":"先用一个简答的例子，来理解linkage disequilibrium，即LD， “chromosomes are mosaics”，这句话如何理解？ 当一段序列最初始的状态，呈现在眼前的时候，我们称其为“ancestral state”，即祖先状态， 就像人会经历各种各样的事情被改变，DNA序列也是一样的， recombination mutation population size natural selection 上述这些因素都是会造成现如今的DNA呈现在我们眼前如此多样的原因。 来看张图理解， 如何衡量Linkage Disequilibrium？ 引入D统计量， Note：D存在多种变式，比如D′D&#x27;D′、r2r^{2}r2 假设在某条染色体上存在两个相邻locus，第一个locus的gene有A和a，第二个locus的gene有B和b，那么它们在理想群体下的frequency是多少？ 如果locus之间是存在linkage disequilibrium效应的呢？则会出现目标gamete type的期望频率和实际频率的偏离，用DDD来衡量。以DABD_{AB}DAB​为例， DAB=pAB−pApBD_{AB} = p_{AB} - p_{A}p_{B} DAB​=pAB​−pA​pB​ 针对每种gamete type frequency的偏离，其计算表达式如下， DAb=pAb−pApb DaB=paB−papBDab=pab−papb\\,\\,\\,D_{Ab} = p_{Ab}-p_{A}p_{b} \\\\ \\,\\,\\,D_{aB} = p_{aB}-p_{a}p_{B} \\\\ D_{ab} = p_{ab}-p_{a}p_{b} \\\\ DAb​=pAb​−pA​pb​DaB​=paB​−pa​pB​Dab​=pab​−pa​pb​ Note：DAB=Dab=−DAb=−DaBD_{AB}=D_{ab}=-D_{Ab}=-D_{aB}DAB​=Dab​=−DAb​=−DaB​如何理解？ 默认情况下，将A、B看作是common allele，而a、b看作是rare allele即可反映上述关系。 该系数没有消除量纲，因此取值范围没有被限定。 Linkage Disequilibrium会消失吗？| recombination rate的引入 为了更好的理解，连锁不平衡效应在代际变化下的变化趋势，就需要引入一个另外一个例子。 首先注意阐明一个点，由于现在是计算2个locus，为了表示代际之间的gene frequency、genotype frequency，需要引入另一个中间变量 —— gamete frequency。 为什么要引入？ 举个例子，以携带AABB这个genotype的个体为例，该个体产生的gamete只存在AB这种类型， 那如果是携带AaBb这个genotype的个体呢？如果不提前表明配子类型如何，该如何知道，究竟是Ab还是AB呢？ 因此就需要将AaBb这种情况给“phasing”开，即用AB/ab和Ab/aB这两种形式来表示。 开始推导。 假设在当前世代t，群体中能够产生或者说AB gamete占总gamete数的比例为PABP_{AB}PAB​，下一代，AB gamete的占比，PAB′P&#x27;_{AB}PAB′​是多少？ 同时引入r，即recombination rate，表示配子之间的gene是可以互换的，就比如AB/ab原本只能够产生AB和ab这两种配子，但由于recombination，现在也可以产生Ab、aB另外两种配子，而重组发生的事件，我们定义为r。 那么下一代的PAB′P&#x27;_{AB}PAB′​即为， PAB′=(1−r)PAB+rpApBP&#x27;_{AB} = (1-r)P_{AB}+rp_{A}p_{B} PAB′​=(1−r)PAB​+rpA​pB​ 变式，可以得到如下的式子， PAB′−pApB=(1−r)PAB+rpApB−pApB =(1−r)(PAB−pApB)PAB−pApB，即为实际gametefrequency和期望frequency之间的差异，记为DABP&#x27;_{AB} - p_{A}p{B} = (1-r)P_{AB} + rp_{A}p_{B} - p_{A}p_{B} \\\\ \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,= (1-r)(P_{AB}-p_{A}p_{B}) \\\\ P_{AB}-p_{A}p_{B}，即为实际gamete frequency和期望frequency之间的差异，记为D_{AB} PAB′​−pA​pB=(1−r)PAB​+rpA​pB​−pA​pB​=(1−r)(PAB​−pA​pB​)PAB​−pA​pB​，即为实际gametefrequency和期望frequency之间的差异，记为DAB​ 而DABD_{AB}DAB​在t个世代之后的关系式，则为， DAB(t)=(1−r)DAB(t−1) =(1−r)tDAB(0)D_{AB}(t)=(1-r)D_{AB}(t-1) \\\\ \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,=(1-r)^{t}D_{AB}(0) DAB​(t)=(1−r)DAB​(t−1)=(1−r)tDAB​(0) 而随着t不断增大，等式右边趋近于0，即表示实际gamete frequency和期望frequency之间的差异消失了，LD不见了！ gamete frequency怎么变化？ 从LD的角度来理解gamete frequency变化 理解了这个，就理解了haplotype frequency如何变化。 从上述式子，再反推回来，我们可以得到如下的式子， DAB(t)=(1−r)DAB(t−1) =(1−r)tDAB(0)反推得到的式子，PAB=pApB+(1−r)t(PAB−pApB)进一步推导，PAB=PAB(t)=PAB(0)(1−r)t+[1−(1−r)t]pApBD_{AB}(t)=(1-r)D_{AB}(t-1) \\\\ \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,=(1-r)^{t}D_{AB}(0) \\\\ 反推得到的式子，P_{AB} = p_{A}p_{B}+(1-r)^{t}(P_{AB}-p_{A}p_{B}) \\\\ 进一步推导，P_{AB}=P_{AB}(t)=P_{AB}(0)(1-r)^{t}+[1-(1-r)^{t}]p_{A}p_{B} DAB​(t)=(1−r)DAB​(t−1)=(1−r)tDAB​(0)反推得到的式子，PAB​=pA​pB​+(1−r)t(PAB​−pA​pB​)进一步推导，PAB​=PAB​(t)=PAB​(0)(1−r)t+[1−(1−r)t]pA​pB​ 上述的式子非常好理解， 当t趋近于∞时，PAB=0P_{AB}=0PAB​=0，即说明了A和B一起遗传的概率为0，它们之间的linkage被抹除了 recombination的可能性不断增加，代表了群体内遗传的随机性水平也在不断增加 同时，在linkage equilibrium的情况下，使用gamete frequency可以帮助我们节省很多的计算量， 比如现存在20个locus，每一个locus存在2个allele，那么就可以组成3个genotype（不区分Aa和aA）， 如果直接保留genotype的计算结果，那我们就需要从3203^{20}320个genotype中计算最终的gene frequency，但是如果群体满足linkage equilibrium，我们则可以直接使用gamete type来计算gamete frequency，220=1048576&lt;&lt;3202^{20}=1048576&lt;&lt;3^{20}220=1048576&lt;&lt;320。 从gamete frequency本身来理解gameter frequency的变化 懒了，直接上图。 I−47I-47I−47，实际上是从gamete frequency计算genotype frequency再返回gamete frequency的做法 联系这张图，就可以一下子理解 而最终就可以得到，如下这样的一个关系式， PABPab−PAbPaB=PAB−pApBP_{AB}P_{ab} - P_{Ab}P_{aB} = P_{AB}-p_{A}p_{B} PAB​Pab​−PAb​PaB​=PAB​−pA​pB​ 上述的式子反映了这样的一个生物学问题：群体中的LD主要由genotype为AB/ab、Ab/aB这两者之间的frequency差异造成，因此当达到linkage equilibrium时，它们之间的差异消失了，不同的gene之间的linkage也就不见了。 其他 D′D&#x27;D′ D&#039;_{AB}\\left\\{ \\begin{array}{**lr**} \\frac{D_{AB}}{min(p_{A}p_{B},p_{a}p_{b})}, D_{AB}&lt;0 \\\\ \\frac{D_{AB}}{min(p_{A}p_{b},p_{a}p_{B})}, D_{AB}&gt;0 \\end{array} \\right. 取值范围：[−1,1][-1,1][−1,1]。当取值为±1±1±1时，代表haplotype frequency严重偏移（有一种haplotype没有被观测到） 特点：当gene frequency较低时，其结果值也呈现较为极端 r2r^{2}r2 DAB2pA(1−pA)pB(1−pB)=χ22n\\frac{D_{AB}^{2}}{p_{A}(1-p_{A})p_{B}(1-p_{B})} \\\\ =\\frac{χ^{2}}{2n} pA​(1−pA​)pB​(1−pB​)DAB2​​=2nχ2​ 取值范围：[0,1][0,1][0,1] 1，代表loci之间存在完全的连锁关系（e.g. 如果AB为初始gamete，且后续也不存在recombination，将它们之间的连锁关系给它们，那么在当前的群体中，只要出现了A，也就能确定B，即“perfect LD, it means the observation at one marker provides complete information about the other”） 0，代表linkage equilibrium 参考资料 [1] Felsenstein, J., 2005. Theoretical evolutionary genetics joseph felsenstein. University of Washington, Seattle. [2] Xu, S. and Jin, W., 2012. Population Genetics in the Genomic Era. Edited by M. Carmen Fusté, p.137. [3] Biostatistics 666, Abecasis Lab [4] Slatkin, M., 2008. Linkage disequilibrium—understanding the evolutionary past and mapping the medical future. Nature Reviews Genetics, 9(6), pp.477-485. ","link":"https://chenyoupu.top/post/theoretical-evolutionary-geneticslinkage-disequilibrium-linkage-disequilibrium/"},{"title":"「Theoretical Evolutionary Genetics」读书笔记（一）- Random Mating Populations","content":"导入 | 以asexual inheritance和haploid inheritance为例 Asexual inheritance 无性繁殖群体的特征，即offspring，子代个体的genotype与祖先代相同。 现假设群体中存在2种strain，即两种genotype不同的亚群体，设为1和2， 再给定如下的一些参数， NiN_{i}Ni​，strain i在某一个generation t时的种群数量 WtW_{t}Wt​，strain i每一代可产生WtW_{t}Wt​个子代 即可以得到下一代的对应strain的数量如下， N1′=WtN1N_{1}^{&#x27;} = W_{t}N_{1}N1′​=Wt​N1​ N2′=WtN2N_{2}^{&#x27;} = W_{t}N_{2}N2′​=Wt​N2​ 那么在t+1t + 1t+1代中，strain 1和strain2的种群数量分别为， N1′N1′+N2′=WtN1′WtN1′+WtN2′=N1N1+N2N2′N1′+N2′=WtN2′WtN1′+WtN2′=N2N1+N2\\frac{N_{1}^{&#x27;}}{N_{1}^{&#x27;}+N_{2}^{&#x27;}} = \\frac{W_{t}N_{1}^{&#x27;}}{W_{t}N_{1}^{&#x27;}+W_{t}N_{2}^{&#x27;}} =\\frac{N_{1}}{N_{1}+N_{2}} \\\\ \\frac{N_{2}^{&#x27;}}{N_{1}^{&#x27;}+N_{2}^{&#x27;}} = \\frac{W_{t}N_{2}^{&#x27;}}{W_{t}N_{1}^{&#x27;}+W_{t}N_{2}^{&#x27;}} =\\frac{N_{2}}{N_{1}+N_{2}} N1′​+N2′​N1′​​=Wt​N1′​+Wt​N2′​Wt​N1′​​=N1​+N2​N1​​N1′​+N2′​N2′​​=Wt​N1′​+Wt​N2′​Wt​N2′​​=N1​+N2​N2​​ 而第t+1个generation和第t个generation中不同strain的种群数量比例，可以表示为， N1′N2′=WtN1WtN2=N1N2\\frac{N_{1}^{&#x27;}}{N_{2}^{&#x27;}} = \\frac{W_{t}N_{1}}{W_{t}N_{2}}=\\frac{N_{1}}{N_{2}} N2′​N1′​​=Wt​N2​Wt​N1​​=N2​N1​​ 重要推论：每一种strain在群体中的种群数量占比，是不变的， 上述情况拓展到，i个strain，也是一样的， 每一个strain的占比， Haploid inheritance 以下面这个species为例（e.g. 一般是微生物具有类似的生殖模式，即haploid-&gt; diploid -&gt; haploid，如下图），现假设群体中存在两种genotype A和a。不同于二倍体，haploid物种，在单个locus只存在一个allele，要么是A，要么是a。 那么在当前这一代，能够形成如下的几种genotype的二倍体，其frequency（genotype frequency）如下， AA：p2p^{2}p2：均产生A gene Aa：2p(1−p)2p(1-p)2p(1−p)：有一半的概率产生A gene，一半的概率产生a gene aa：(1−p)2(1-p)^{2}(1−p)2：均产生a gene 下一代中对应genotype的haploid个体所对应的frequency如下， A：p2+p(1−p)=pp^{2}+p(1-p)=pp2+p(1−p)=p a：(1−p)2+p(1−p)=1−p(1-p)^{2}+p(1-p)=1-p(1−p)2+p(1−p)=1−p Note：上述即为HW equilibrium，其有一个非常重要的假设，即infinite population，而这又可以进一步推导出每一种genotype可以产生的子代数量是一致的，如下， frequency of A in t generation=p2Wt+p(1−p)Wtp2Wt+2p(1−p)Wt+(1−p)2Wt=pfrequency\\,\\,of\\,\\,A\\,\\,in\\,\\,t\\,\\,generation=\\frac{p^{2}W_{t}+p(1-p)W_{t}}{p^{2}W_{t}+2p(1-p)W_{t}+(1-p)^{2}W_{t}} = p frequencyofAintgeneration=p2Wt​+2p(1−p)Wt​+(1−p)2Wt​p2Wt​+p(1−p)Wt​​=p HW equilibrium的引入 Diploid with two alleles 现假定一个存在二倍体群体，且某一条常染色体上存在两个可进行segregating（服从孟德尔分离定律）的allele，同时这个群体中不同性别中的genotype frequency需要相同， Note：这个前提非常重要喔，在后续进一步理解sex-linked locus的gene frequency equilibrium非常有用 比如female population中，Aa genotype frequency=0.7，male population中，Aa genotype frequency也要为0.7，即不考虑群体数量对mating的影响，用公式来表示， Pf(Aa)=5001000=0.5Pm(Aa)=7001400=0.5Pall(Aa)=500+7001000+1400=0.5P_{f}(Aa) = \\frac{500}{1000} = 0.5 \\\\ P_{m}(Aa) = \\frac{700}{1400} = 0.5 \\\\ P_{all}(Aa) = \\frac{500+700}{1000+1400} = 0.5 Pf​(Aa)=1000500​=0.5Pm​(Aa)=1400700​=0.5Pall​(Aa)=1000+1400500+700​=0.5 总结一下，该diploid population需要满足的前提， 不同性别中，genotype frequency要一致 每一个个体产生配子的概率是要相同的，且不同配子之间形成合子是独立、随机的 而 这样的群体，在经过一代的随机交配之后，即可达到Hardy-Weinberg equilibrium， gene frequency -&gt; 常量 genotype frequency -&gt; 常量 同时需要注意的是， “Mendelian reproduction in a random mating population has no inherent tendency to favor one allele or the other.” 上面这句话就对应了“genotype frequency -&gt; 常量”这个结果，即孟德尔遗传只是一种本质上的规律，是机制，而真正能够改变gene frequency的驱动力，是natural selection和genetic drift。 Multi-alleles的情况一样吗？ 这个问题就比如将二分类问题推广到多分类问题，本质上是一样的。 群体动态变化 | Overlapping generations的引入 之前讨论的模型，都是建立在每一代每一代完全分隔，群体中的每一个个体都知道，“爷今天出生了”、“爷明天要死了”的情况，大家出生和死的时间都是非常一致的，但是真实的情况应该是，“我先逝了，你慢点逝”等。 数学语言怎么建立？ 每一代随机选择一部分的个体去“逝世”，并被新的individual所取代，该比率为∂t\\partial{t}∂t； 对一个个体来说，其能够存活t个单位时间的概率如下， (1−∂t)t∂t(1-\\partial{t})^{\\frac{t}{\\partial{t}}} (1−∂t)∂tt​ 1−∂t1-\\partial{t}1−∂t，为单次存活下来的概率 t∂t\\frac{t}{\\partial{t}}∂tt​，为逃过一劫的次数 怎么体现genotype frequency随时间变化而变化？ 以genotype AA为例， 当前群体AA genotype的frequency用PAA(t)P_{AA}(t)PAA​(t)表示，A gene frequency用pA(t)p_{A}(t)pA​(t)； 在单位时间内（∂t\\partial{t}∂t），被替代的AA个体的frequency为∂t⋅[pA(t)]2\\partial{t}·[p_{A}(t)]^{2}∂t⋅[pA​(t)]2； 那我们就可以列出下面的公式，即单位时间内AA genotype的变化幅度， PAA(t+∂t)=PAA(t)(1−∂t)+[pA(t)]2∂tP_{AA}(t+\\partial{t})=P_{AA}(t)(1-\\partial{t})+[p_{A}(t)]^{2}\\partial{t} PAA​(t+∂t)=PAA​(t)(1−∂t)+[pA​(t)]2∂t 变化形式之后可以得到， PAA(t+∂t)−PAA(t)∂t=[pA(t)]2−PAA(t)dPAA(t)dt=[pA(t)]2−PAA(t)\\frac{P_{AA}(t+\\partial{t})-P_{AA}(t)}{\\partial{t}}=[p_{A}(t)]^{2} - P_{AA}(t) \\\\ \\frac{dP_{AA}(t)}{dt} = [p_{A}(t)]^{2} - P_{AA}(t) ∂tPAA​(t+∂t)−PAA​(t)​=[pA​(t)]2−PAA​(t)dtdPAA​(t)​=[pA​(t)]2−PAA​(t) 为了后续进行定积分，我们进一步转换，得到下列的式子， dPAA(t)[pA(t)]2−PAA(t)=dt∫1[pA(t)]2−PAA(t)⋅dPAA(t)=∫dt求解上述式子，得到−loge[pA(t)2−PAA(t)]=t+C\\frac{dP_{AA}(t)}{[p_{A}(t)]^{2} - P_{AA}(t)} = dt \\\\ \\int{\\frac{1}{[p_{A}(t)]^{2} - P_{AA}(t)}·dP_{AA}(t)} = \\int{dt} \\\\ 求解上述式子，得到-log_{e}[p_{A}(t)^{2} - P_{AA}(t)] = t + C [pA​(t)]2−PAA​(t)dPAA​(t)​=dt∫[pA​(t)]2−PAA​(t)1​⋅dPAA​(t)=∫dt求解上述式子，得到−loge​[pA​(t)2−PAA​(t)]=t+C 为了解开上述的式子，解出C的值非常重要，可以通过设定t=0，来进行求解，即C=−loge(pA(t)2−PAA(t))C=-log_{e}(p_{A}(t)^{2} - P_{AA}(t))C=−loge​(pA​(t)2−PAA​(t))。 进一步转化得到， −loge[pA(t)2−PAA(t)]=t−loge(pA(t)2−PAA(t))-log_{e}[p_{A}(t)^{2} - P_{AA}(t)] = t - log_{e}(p_{A}(t)^{2} - P_{AA}(t)) −loge​[pA​(t)2−PAA​(t)]=t−loge​(pA​(t)2−PAA​(t)) 对数转换之后且转换之后，可以得到最终的式子， PAA(t)=PAA(0)(e−t)+pA2(1−e−t)P_{AA}(t) = P_{AA}(0)(e^{-t}) + p_{A}^{2}(1-e^{-t}) PAA​(t)=PAA​(0)(e−t)+pA2​(1−e−t) 从这个式子，可以得出什么结论？ 当t趋近于∞的时候，PAA(t)=pA2P_{AA}(t)=p_{A}^{2}PAA​(t)=pA2​，即达到Hardy-Weinberg equilibrium 性别的不均匀贡献 | sex-linked locus的引入 sex differentiation的引入 现假定female population中，A gene frequency为pfp_{f}pf​，male population中为pmp_{m}pm​， generation之间的计算规律如下， 下一代中，female、male population中的A gene frequency为， pf′=pm′=pfpm+12[pf(1−pm)+pm(1−pf)] =pfpm+12pf−12pfpm+12pm−12pfpm=12pf+12pm p&#x27;_{f}=p&#x27;_{m}=p_{f}p_{m}+\\frac{1}{2}[p_{f}(1-p_{m})+p_{m}(1-p_{f})] \\\\ \\,\\,\\,=p_{f}p_{m}+\\frac{1}{2}p_{f}-\\frac{1}{2}p_{f}p_{m}+\\frac{1}{2}p_{m}-\\frac{1}{2}p_{f}p_{m}\\\\ = \\frac{1}{2}p_{f} + \\frac{1}{2}p_{m}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, pf′​=pm′​=pf​pm​+21​[pf​(1−pm​)+pm​(1−pf​)]=pf​pm​+21​pf​−21​pf​pm​+21​pm​−21​pf​pm​=21​pf​+21​pm​ 从这个式子，可以得出什么结论？ 从t generation到t+1generation，父母本都只有一半的gametes可以传递下来，对应式子中的1/2 diploid population，且同时引入了sex differentiation的情况，无法通过一代达到Hardy-Weinberg equilibrium， 即t+1代中的PAA=pmpfP_{AA}=p_{m}p_{f}PAA​=pm​pf​，不等于该代的EAA=(12pf+12pm)2E_{AA}=(\\frac{1}{2}p_{f} + \\frac{1}{2}p_{m})^{2}EAA​=(21​pf​+21​pm​)2 sex-linked locus的引入 这种情况有一个非常重要的前提：该sex-linked locus位于X chromosome上，且Y chromosome上没有对应的同源区段。 同样的假设，female population中，A gene frequency为pfp_{f}pf​，male population中为pmp_{m}pm​， 那么即可以得到在t+1 generation的female population中，3种genotype分别对应的频率为， AA，pfpmp_{f}p_{m}pf​pm​ Aa，pf(1−pm)+pm(1−pf)p_{f}(1-p_{m})+p_{m}(1-p_{f})pf​(1−pm​)+pm​(1−pf​) aa，(1−pm)(1−pf)(1-p_{m})(1-p_{f})(1−pm​)(1−pf​) 带入上一个部分的公式，可以得到female population中A gene的frequency，为12pf+12pm\\frac{1}{2}p_{f} + \\frac{1}{2}p_{m}21​pf​+21​pm​。 在male population中，对应的genotype frequency为， AY，pfp_{f}pf​ aY，(1−pf)(1-p_{f})(1−pf​) 而对于male来说，A gene frequency即为pfp_{f}pf​。 这还远远没有达到平衡，那么平衡时的公式如何呢？ pmean=23pf+13pmp_{mean} = \\frac{2}{3}p_{f}+\\frac{1}{3}p_{m} pmean​=32​pf​+31​pm​ 来几道题 | 起始pf, pmp_{f},\\,\\,p_{m}pf​,pm​分别为(1,0), (0,1), (0.5, 0.5)的情况下，最终群体的A gene frequency在什么数值收敛？ 我的代码， pf &lt;- 1; pm &lt;- 0 # pf &lt;- 0; pm &lt;- 1 # pf &lt;- 0.5; pm &lt;- 0.5 generation.vector &lt;- c(pf, pm) # simulate the pf and pm in 200 loops for (i in seq(3, 150, by = 2)){ tmp.pf &lt;- (generation.vector[i-2] + generation.vector[i-1]) / 2 tmp.pm &lt;- generation.vector[i-2] generation.vector &lt;- c(generation.vector, tmp.pf, tmp.pm) } length(generation.vector) # retrieve the pf and pm pf.vector &lt;- c() pm.vector &lt;- c() for (i in seq(1,150, 2)) { pf.vector &lt;- c(pf.vector, generation.vector[i]) } for (i in seq(2,150, 2)) { pm.vector &lt;- c(pm.vector, generation.vector[i]) } p.dat &lt;- data.frame(&quot;Pf&quot;=pf.vector, &quot;Pm&quot;=pm.vector, &quot;Generation&quot;=seq(1:75)) tail(p.dat) # plot(p.dat$Pf) # plot(p.dat$Pm) library(ggplot2) p &lt;- ggplot(data = p.dat, aes(x = Generation)) + geom_line(aes(y = Pf, color = &quot;Pf&quot;)) + geom_line(aes(y = Pm, color = &quot;Pm&quot;)) + theme_classic() + scale_y_continuous(limits = c(0, 1)) + scale_x_continuous(limits = c(0, 15)) + ylab(&quot;&quot;) p # ggsave(&quot;sim-HWprinciple-sex-linked-locus.pdf&quot;, p, height = 6, width = 10) 当pf=1,pm=0p_{f}=1, p_{m}=0pf​=1,pm​=0的初始情况下，PAAP_{AA}PAA​在0.667收敛，如下图， 其他 公式推导 | lim∂t−&gt;0(1−∂t)t∂tlim_{\\partial{t}-&gt;0}(1-\\partial_{t})^{\\frac{t}{\\partial{t}}}lim∂t−&gt;0​(1−∂t​)∂tt​ lim∂t−&gt;0(1−∂t)t∂t=lim⁡∂t−&gt;0et∂tln(1−∂t)=elim⁡∂t−&gt;0t⋅ln(1−∂t)∂t应用洛必达法则，得elim⁡∂t−&gt;0t⋅−11−∂t1=elim⁡∂t−&gt;0t⋅−11−∂t即当∂t趋近于0时，该函数极限值为e−tlim_{\\partial{t}-&gt;0}(1-\\partial{t})^{\\frac{t}{\\partial{t}}} \\\\ =\\lim_{\\partial{t}-&gt;0}e^\\frac{t}{\\partial{t}}{ln(1-\\partial{t})} \\\\ =e^{\\lim_{\\partial{t}-&gt;0}\\frac{t·ln(1-\\partial{t})}{\\partial{t}}} \\\\ 应用洛必达法则，得e^{\\lim_{\\partial{t}-&gt;0}\\frac{t·\\frac{-1}{1-\\partial{t}}}{1}}=e^{\\lim_{\\partial{t}-&gt;0}t·\\frac{-1}{1-\\partial{t}}} \\\\ 即当\\partial{t}趋近于0时，该函数极限值为e^{-t} lim∂t−&gt;0​(1−∂t)∂tt​=∂t−&gt;0lim​e∂tt​ln(1−∂t)=elim∂t−&gt;0​∂tt⋅ln(1−∂t)​应用洛必达法则，得elim∂t−&gt;0​1t⋅1−∂t−1​​=elim∂t−&gt;0​t⋅1−∂t−1​即当∂t趋近于0时，该函数极限值为e−t ","link":"https://chenyoupu.top/post/theoretical-evolutionary-genetics-randommating/"},{"title":"「基因组」001｜基因组组装之前要做的：Genome Survey","content":"基因组组装之前，有一些问题还是需要注意的， genome size是多少？ 评估得到的genome heterozygosity是多少？ 重复序列的占比是多少？ 可以系统性地称为genome survey，这是一个非常简单的分析，但是其实有一些问题是值得注意的 Genome Survey一般基于Illumina short reads进行分析，因为二代测序便宜，先测出来试试水， 再判断三代的数据量，这应该算是一个非常经济实惠的做法。 分析流程 1）fastp、Trimmomatic等软件挑一个过滤低质量序列 2）Jellyfish 2.3.0、KMC3 我个人其实比较喜欢KMC，因为可以直接读取.gz文件（绝对不是因为之前KMC作者帮助我愉快地解决了一个Bug），但是解决jellyfish脚本的过程中也让我对Shell Kernel有了一个更深刻的理解。 3）Genome Scope 2.0、GCE等软件，挑一个进行genome size、heterozygosity等指标的估计 我个人比较熟悉的还是Genome Scope 2.0，因为这个软件可以用于判断auto-tetraploid和allo-tetraploid， 同时作者Michael C. Schatz的实验室还开发了FALCON~ fastp vim fastp.sh sh fastp.sh 2&gt;fastp.err.log &amp; Shell script: #!/bin/bash # Preset dir=&lt;specicy_path_of_your_rawdata&gt; echo &quot;The raw dataset is placed at $dir&quot; echo &quot;Now running Quality control&quot; thread=24 # set 24 threads quality=20 # set quality cutoff to 20 based on Phred33 # fastp -w $thread -q $quality -i $dir/sample1.fq.gz -I $dir/sample2.fq.gz -o ./sample_clean_1.fq.gz -O ./sample_clean_2.fq.gz Jellyfish: count k-mer vim jellyfish.sh chmod 777 jellyfish.sh # key step, otherwise the script below will report syntax error ./jellyfish.sh 2&gt;jellyfish.err.log &amp; “哼男人，嘴上说着喜欢KMC，但是却用Jellyfish”， Shell script: # Preset dir=&lt;specicy_path_of_your_cleandata&gt; echo &quot;The clean dataset is placed at $dir&quot; echo &quot;Now running Jellyfish Kmercount&quot; # 17mer echo &quot;Now running 17mer counting&quot; content1=&quot;jellyfish count -C -m 17 -o ./sample.17mer.jf -s 10G -t 24 &lt;(pigz -dc $dir/sample_clean_1.fq.gz) &lt;(pigz -dc $dir/sample_clean_2.fq.gz)&quot; echo &quot;The command is $content1&quot; jellyfish count \\ -C \\ -m 17 \\ &lt;( pigz -dc $dir/sample_clean_1.fq.gz ) &lt;( pigz -dc $dir/sample_clean_2.fq.gz ) \\ -o ./sample.17mer.jf -s 10G -t 24 # 21mer: recommanded by author echo &quot;Now running 21mer counting&quot; content2=&quot;jellyfish count -C -m 21 -o ./sample.21mer.jf -s 10G -t 24 &lt;(pigz -dc $dir/sample_clean_1.fq.gz) &lt;(pigz -dc $dir/sample_clean_2.fq.gz)&quot; echo &quot;The command is $content2&quot; jellyfish count \\ -C \\ -m 21 \\ &lt;( pigz -dc $dir/sample_clean_1.fq.gz ) &lt;( pigz -dc $dir/sample_clean_2.fq.gz ) \\ -o ./sample.21mer.jf -s 10G -t 24 注意点：为什么要用chmod 777？ 答：未经777赋予可执行权限的脚本，仍为shell脚本，需要指定bash或者sh来运行程序，即不可从jellyfish直接开始运行程序， 就好比原本的运行方式为bash &lt;script_name&gt;.sh，现在要修改成为./&lt;script_name&gt;.sh的运行方式，不然就会出现syntax errror Jellyfish: k-mer spectrum generation jellyfish histo -t 24 -l 1 -h 500000 sample.17mer.jf &gt; sample.17mer.histo &amp; jellyfish histo -t 24 -l 1 -h 500000 sample.21mer.jf &gt; sample.21mer.histo &amp; 注意点：upper limitation的修改。 Genome Scope 2.0的分析需要将k-mer spectrum的upper limit设置得高一些，不然后续genome size估计塌缩比例会特别大。 Genome Scope 2.0 + Smudeplot 1）The estimation of genome size, heterozygosity, etc. vim genomescope.sh chmod 777 genomescope.sh ./genomescope.sh 2&gt;genomescope.err.log &amp; Shell script: script=&lt;path_to_your_genomescope_repo&gt;/genomescope.R dir=&lt;where_kmerspectrum_deposited&gt; Rscript $script -i &lt;input_histo&gt; -o ./ -n &lt;outputname&gt; -p &lt;ploidy_level&gt; -k &lt;kmer_used&gt; 2）Kmer-pair plot 这部分官网其实给出了比较好的流程，我就只是简单概括走一下， dir=&lt;specify_your_kmercount_database&gt; L=$(smudgeplot.py cutoff &lt;histo_from_kmc&gt; L) U=$(smudgeplot.py cutoff &lt;histo_from_kmc&gt; U) echo $L $U # these need to be sane values # conda activate genomesurvey kmc_tools transform $dir/&lt;kmc_db&gt; -ci&quot;$L&quot; -cx&quot;$U&quot; dump -s smudgeplot_kmc_db/&lt;kmc_db&gt;.kmc_L&quot;$L&quot;_U&quot;$U&quot;.dump # conda activate smudgeplot smudgeplot.py hetkmers -o smudgeplot_kmercounts/&lt;kmc_db&gt;.kmc_L&quot;$L&quot;_U&quot;$U&quot; &lt; smudgeplot_kmc_db/&lt;kmc_db&gt;.kmc_L&quot;$L&quot;_U&quot;$U&quot;.dump # Plot smudgeplot.py plot &lt;kmc_db&gt;._L&quot;$L&quot;_U&quot;$U&quot;_coverages.tsv 结果示意图如下， 基因组大小估计需要注意的点， 0）三代数据不适合用于Kmer分析，因为测序错误率高了很多，会对分析结果产生非常大的影响， 但是HiFi reads以及canu和falcon产生的corrected reads可以很好的适用于Genome Scope分析 1）jellyfish histo输出时指定的maximum kmer-freq，会极大地影响到genome size的估计，因此需要根据自己的数据进行调整，一般100000再往上也可 2）genonomescope.R的-p以及--kcov的设置，都会影响到genome size的估计 比如在genomescope2.0 model下，如果在输入数据和模型都已经定下来的基础上，将--kcov设置为原本的一倍，则genome size的大小估计会减半（此处感兴趣的，我建议还是自行搜索下基于kmer计算genome size的公式） 3）结果中的transformed_linear_plot和linear_plot有什么区别？ 前者的observed曲线经过了一个转换，越往后的peak其峰值越大，即在原始的kmer freq上乘了一个n（n代表第n个peak）， 后者的observed曲线为实际观测到的一个数值，没有经过上述转换 transformed linear： linear plot： 4）Genome Scope 2.0分析时，如果将过多的kmer判定为了error，最终的genome size就会小了特别多（基于genome size的计算公式） 背后的原理 首先需要明确的一个点是：Genome Scope 是基于diploid进行编写的。 关于二倍体物种的基因组大小估计，如何理解。我想要举一个非常简单的例子来理解： 给个用kmer将genome给“划分”开的示意， kmer: ---A-- --A--- -A---- A----- genome: ---A----------------------------------------------- 假设当前的基因组非常纯合（-&gt; homozygous, not -&gt; heterozygous），kmer会在某一个频数上呈现一个峰值， 但是如果当前的基因组杂合度上升了，也就是我们一般在文献中看到的heterozygosity，kmer在另一个频数较小的区域，也会呈现一个峰值，也就是Genome Survey中提到的杂合峰 就比如下图中的T，是A对应的allele。该种情况的存在，会导致kmer出现另一种情况，从而降低了纯合峰的高度，比如下图的例子就表示了对应位置kmer的频数，从4降低到了2， 即可以将diploid的kmer topology理解为：aa，ab kmer: ---A-- --A--- genome: ---A----------------------------------------------- T ---T-- --T--- 所以，为了满足polyploid的产生，Genome Scope 2.0被开发 —— 基于负二项分布的Kmer模型，用于估计genome size、heterozygosity等。 三倍体的kmer topolygy：aaa（3种haplotype均一致），aab（有1种haplotype和另外2个haplotype存在区别），abc（3种haplotype各不相同） 四倍体的kmer topology：aaaa，aaab，aabb，abcd 参考资料 [1] https://github.com/schatzlab/genomescope/issues/32 [2] https://github.com/schatzlab/genomescope/issues/43 [3] https://github.com/schatzlab/genomescope/issues/48 [4] https://github.com/schatzlab/genomescope [5] https://github.com/KamilSJaron/smudgeplot ","link":"https://chenyoupu.top/post/genomesurvey/"},{"title":"「生物信息学中的Math」001｜浅谈统计检验效能和FDR","content":"什么是统计检验的效能（power）？ 我们首先来回顾一下，在假设检验中，我们需要什么。 （1）原假设（H0H_{0}H0​）：可以认为是辩论赛中，保守一方的观点（e.g. 超级英雄和普通人没啥区别） （2）备择假设（HA/H1H_{A}/H_{1}HA​/H1​）：可以类比为辩论赛中，激进一方的观点（e.g. 超级英雄就是爷，就是比普通人牛） 在做计算的过程中，会涉及到一个非常重要的数值 —— ααα（一般取0.05 或 0.01） ααα的含义是：显著性水平、一类错误发生的概率（Type I Error）、弃真错误发生的概率。 Note：α —— 一类错误、弃真错误、假阳性错误 从这里有需要再引入几个概念，如下表： H0H_{0}H0​是正确的 H0H_{0}H0​是错误的 接受H0H_{0}H0​ 接受H0H_{0}H0​（1−α1-α1−α） βββ错误 拒绝H0H_{0}H0​ ααα错误 接受H1H_{1}H1​（1−β1-β1−β） β从α衍生出来，是二类错误发生的概率（Type II Error）、取伪错误发生的概率。 在假设检验过程中，我们把拒绝原假设后，接受正确的备择假设的概率称为统计检验的效能/功效（statistical power），因此其在数值上等于1−β1-β1−β。 【个人理解】1−β1-β1−β，实际上就是确定两个样本的总体有差异之后，假设检验能够顺利根据样本推断出真实的总体信息的概率。 什么是假阳性（false positive rate）？ 结合时事，我就拿新冠检测作为例子。 假设对100个人进行核酸检测，检测结果分别如下： 被测对象真实情况：阳性 被测对象真实情况：阴性 新冠检测结果：阳性 5 2 新冠检测结果：阴性 3 90 可以得到2个指标的计算结果，如下： （1）true positve rate（sensitivity，即灵敏度）：检测出的真实阳性样本数 除以 所有真实阳性样本数 55+3=0.625 \\frac{5}{5+3} = 0.625 5+35​=0.625 （2）false positive rate：检测出的假阳性样本数 除以 所有真实阴性样本数 22+90=0.0217 \\frac{2}{2+90} = 0.0217 2+902​=0.0217 还有一个非常重要的计算指标，是特异度（specificity），计算公式如下： 9090+2=0.9783 \\frac{90}{90+2}= 0.9783 90+290​=0.9783 一张找来的总结表： 参考资料 [1] https://www.jianshu.com/p/d5ea74ca61f8 [2] https://blog.csdn.net/fish2009122/article/details/110040002 [3] 统计功效, 百度百科 [4] 真阳性率, 百度百度 [5] 假阳性率, 百度百科 [6] https://zhwhong.cn/2017/04/14/ROC-AUC-Precision-Recall-analysis/ ","link":"https://chenyoupu.top/post/statistic-powerandfalse-positive/"},{"title":"「一文搞定序列比对算法」Global以及Local Alignment序列比对算法的实现","content":"序列比对是什么以及序列比对主要的作用是什么，本篇博客就一笔带过，因为不是主要分享内容。 序列比对，此处引申为pairwise alignment会更加恰当一些，用于比较2条序列之间的相似程度，推断它们之间的相似程度，进而探索对应功能以及系统发育关系。 接下来大体分为2个部分，1）全局比对，2）局部比对 首先要明确一个概念：序列比对想要达到的目的是什么？ 引一张图来说明序列比对的目的以及全局比对、局部比对之间的区别， 总的来说，也就是全局比对和局部比对想要达到的目的是不一样的， 全局比对想要得到的是2条序列最佳的匹配结果（e.g. 最多的match数量、最高的比对得分、最高的identity），局部比对想要得到的是2条序列中最佳匹配片段（注意：最佳的匹配结果需要建立在相对较少的序列修改上） 全局比对更适用于evolution关系上更加靠近的（e.g. 粳稻和籼稻），而局部比对更加适用于evolution关系上关系比较远的（e.g. 水稻和葡萄） 「步骤拆解」Global Alignment 利用动态规划来解决问题，最关键的一步就是列出动态规划公式，只要能列出公式，后面的编程也都只是时间问题。 但是，我并不想一上来就列出数学公式，我认为以一个简单的例子入手更有利于序列比对问题中的动态规划应用。 接下来，先理一理基于动态规划的序列比对的过程。 （1）Intialization + Matrix Filling 假设现在有2条长度分别为n、m的序列， 那则需要构建行数为n+1，列数为m+1的矩阵， 而“Filling”这个过程，即将第一列和第一行进行填充，从数学公式的角度来理解的话， 第一列的填充：$$g*length(matrix[1:i, 1])$$，i~[1, n] 第一行的填充：$$g*length(matrix[1, 1:j])$$，j~[1, m] （2）Tracing Back 每一个单元的填充模式如下， 横向和竖向的移动代表了gap open（horizontal，vertical） 但更加复杂的情况应该考虑到gap在哪一条序列打开 对角线的移动则可以分为1）match（从大的数值回溯到小的数值），2）mismatch（从小的数值回溯到大的数值） Note：数值增大代表替换矩阵中，该碱基对应关系为match，而数值减小，代表替换矩阵中碱基对应关系为mismatch 「公式」Global Alignment 引入gap extension 出现4个单位长度的一个完整gap将两条序列给比对上，或者4个单位长度的单独gap将两条序列给比对上是更符合生物学原理的？ 上述的文字情况如下所示， # 1. ATCGATCGATCGATCG---- AGCTAGCTCAGTACGT---- # 2. ATCG-ATCG-ATCG-ATCG-ATCG ATCG-ATCG-ATCG-ATCG-ATCG 答案是前者。这就需要在序列比对中引入另一个非常重要的细节 —— affine gap penalty。 Note：此处引入的affine gap penalty为“not penalize open with extension”，即在打开一个gap的时候，不会在该gap上同时引入open和extension的罚分 affine gap penalty，即在打开第一个gap的时候引入gap open罚分，而在该gap的基础上进行延续则采用gap extension罚分。 该种做法与原来的常量gap有一定区别，因此就需要改变动态规划公式，同时引入CS中的状态机可以帮助我们更好地理解这个问题。 上图中存在3个状态， 1）M：当前的比对情况下为match或mismatch 2）Ix：当前的比对情况为在seq2上打开一个gap，而seq1上为一个base 3）Iy：当前的比对情况为在seq1上打开一个gap，而seq2上为一个base 三者之间是可以相互转换的，通过d、e、s(x, y)来调整。 因此动态规划公式变为如下的形式， gap extension情况下的动态规划矩阵初始化 M(0,0)=0 Ix(i,0)=d+e∗(i−1)I_{x}(i,0)=d + e*(i-1) Ix​(i,0)=d+e∗(i−1) Iy(j,0)=d+e∗(j−1)I_{y}(j,0)=d+e*(j-1) Iy​(j,0)=d+e∗(j−1) 但由于$$I_{x}$$在第一行以及$$I_{y}$$在第一列的取值都是不存在的，因此定义为-inf。 同时，由于每一个cell都存在一种情况，我们需要建立3个矩阵来存储对应的信息，分别用M、X、Y来表示。 经初始化之后，就可以得到如下3个矩阵： 1）M 2）X 代表在列方向打开一个gap，即seq2上插入一个gap 3）Y 代表在行方向上打开一个gap，即seq1上插入一个gap gap extension情况下的动态规划矩阵回溯 3个矩阵，可以使用1个矩阵来记录当前的cell的数值来源，3种情况如下 来自M，即当前为一个match/mismatch，记录为0 来自X，即当前为一个gap open/gap extension，记录为1 来自Y，即当前为一个gap open/gap extension，记录为2 给个示例的回溯矩阵， 「步骤拆解」Local Alignment 步骤与Global Alignment近似，只是引入了一个0，就可以得到局部的最佳匹配。 公式如下， 代码实现 import numpy as np # Preset variables seq1 = &quot;TCGTAGACGA&quot; seq2 = &quot;ATAGAATGCGG&quot; print(f'The seq1 has length of {len(seq1)}') print(f'The seq2 has length of {len(seq2)}') match = 1 mismatch = -1 gap_open = -2 gap_extension = -1 # global MIN MIN = -float(&quot;inf&quot;) def identity_match(base1, base2): '''Note: this function is used to compare the bases and return match point or mismatch point''' if base1 == base2: return match else: return mismatch def createscorematrix(n, m): '''Note: this function is used to generate the original score function''' # Create match matrix, x matrix and y matrix m_mat = [np.zeros(m+1) for i in range(0, n+1)] x_mat = [np.zeros(m+1) for i in range(0, n+1)] y_mat = [np.zeros(m+1) for i in range(0, n+1)] return m_mat, x_mat, y_mat m_mat, x_mat, y_mat = createscorematrix(len(seq1), len(seq2)) # print(m_mat) # print(x_mat) # print(y_mat) def scorematrix_init(m_mat, x_mat, y_mat, d, e, local=False): '''Note: this function conduct the score matrix initialization''' '''Global Alignment''' if local == False: '''match matrix filling''' for i in range(0, len(m_mat)): for j in range(0, len(m_mat[0])): if i == 0 and j == 0: m_mat[i][j] = 0 elif i == 0 and j &gt; 0: m_mat[i][j] = MIN elif i &gt; 0 and j == 0: m_mat[i][j] = MIN # print(m_mat) for line in m_mat: r_list = [str(i) for i in line] print(' '.join(r_list)) '''x_matrix filling''' for i in range(0, len(x_mat)): for j in range(0, len(x_mat[0])): if i == 0 and j == 0: x_mat[i][j]=0 if i &gt; 0 and j == 0: x_mat[i][j] = d+e*(i-1) x_first_row = [0] x_first_row.extend([MIN]*(len(x_mat[0])-1)) x_mat[0] = x_first_row # print(x_mat) for line in x_mat: r_list = [str(i) for i in line] print(' '.join(r_list)) '''y_matrix filling''' for i in range(0, len(y_mat)): for j in range(0, len(y_mat[0])): if i == 0 and j == 0: y_mat[i][j]=0 elif i &gt; 0 and j == 0: y_mat[i][j] = MIN y_first_row = [0] y_first_row.extend([d+e*(i-1) for i in range(1, len(y_mat[0]-1))]) y_mat[0] = y_first_row # print(y_mat) for line in y_mat: r_list = [str(i) for i in line] print(' '.join(r_list)) return m_mat, x_mat, y_mat '''Local Alignment: Initialization step for Smith-Watermen is useless''' if local == True: return m_mat, x_mat, y_mat m_mat, x_mat, y_mat = scorematrix_init(m_mat, x_mat, y_mat, -2, -1, local=False) # m_mat, x_mat, y_mat = scorematrix_init(m_mat, x_mat, y_mat, -2, -1, local=True) def matrix_filling(m_mat, x_mat, y_mat, d, e, local=False): '''this function is used to create the scoring matrix using three dynamic programming, and building a tracing matrix to restore the paths for the retrieve of aliignments''' '''Global Alignment Activation''' if local == False: # Filling score matrix and record the trace trace_matrix = [np.zeros(len(m_mat[0])) for i in range(0, len(m_mat))] for i in range(1, len(m_mat)): # print(m_mat[0]) for j in range(1, len(m_mat[0])): # print(i, j) m_mat[i][j] = max( m_mat[i-1][j-1] + identity_match(seq1[i-1], seq2[j-1]), x_mat[i-1][j-1] + identity_match(seq1[i-1], seq2[j-1]), y_mat[i-1][j-1] + identity_match(seq1[i-1], seq2[j-1]) ) x_mat[i][j] = max(m_mat[i-1][j] + d, x_mat[i-1][j] + e) y_mat[i][j] = max(m_mat[i][j-1] + d, y_mat[i][j-1] + e) # for line in m_mat: # print(line) # Take the greatest values in these three matrix, # merge into one matrix, # and record the path new_mat = [np.zeros(len(m_mat[0])) for i in range(0, len(m_mat))] for i in range(0, len(m_mat)): for j in range(0, len(m_mat[0])): new_mat[i][j] = max(m_mat[i][j], x_mat[i][j], y_mat[i][j]) # Fill the trace matrix # Note: from match/mismatch is 0, from x_mat (open a gap in seq2) is 1, from y_mat (open a gap in seq1) if m_mat[i][j] == max(m_mat[i][j], x_mat[i][j], y_mat[i][j]): trace_matrix[i][j] = 0 elif x_mat[i][j] == max(m_mat[i][j], x_mat[i][j], y_mat[i][j]): trace_matrix[i][j] = 1 elif y_mat[i][j] == max(m_mat[i][j], x_mat[i][j], y_mat[i][j]): trace_matrix[i][j] = 2 # # Print out the scoring matrix # for line in new_mat: # r_list = [str(i) for i in line] # print('\\t'.join(r_list)) # # Print out the tracing matrix for line in trace_matrix: r_list = [str(i) for i in line] print('\\t'.join(r_list)) return new_mat, trace_matrix '''Local Alignment Activation''' if local == True: # Filling score matrix and record the trace trace_matrix = [np.zeros(len(m_mat[0])) for i in range(0, len(m_mat))] for i in range(1, len(m_mat)): # print(m_mat[0]) for j in range(1, len(m_mat[0])): # print(i, j) m_mat[i][j] = max( m_mat[i-1][j-1] + identity_match(seq1[i-1], seq2[j-1]), x_mat[i-1][j-1] + identity_match(seq1[i-1], seq2[j-1]), y_mat[i-1][j-1] + identity_match(seq1[i-1], seq2[j-1]), 0 ) x_mat[i][j] = max(m_mat[i-1][j] + d, x_mat[i-1][j] + e) y_mat[i][j] = max(m_mat[i][j-1] + d, y_mat[i][j-1] + e) # for line in m_mat: # print(line) # Take the Greatest values in these three matrix new_mat = [np.zeros(len(m_mat[0])) for i in range(0, len(m_mat))] for i in range(0, len(m_mat)): for j in range(0, len(m_mat[0])): new_mat[i][j] = max(m_mat[i][j], x_mat[i][j], y_mat[i][j]) if m_mat[i][j] == max(m_mat[i][j], x_mat[i][j], y_mat[i][j]): trace_matrix[i][j] = 0 elif x_mat[i][j] == max(m_mat[i][j], x_mat[i][j], y_mat[i][j]): trace_matrix[i][j] = 1 elif y_mat[i][j] == max(m_mat[i][j], x_mat[i][j], y_mat[i][j]): trace_matrix[i][j] = 2 # # Print out the scoring matrix # for line in new_mat: # r_list = [str(i) for i in line] # print(' '.join(r_list)) # # Print out the tracing matrix # for line in trace_matrix: # r_list = [str(i) for i in line] # print('\\t'.join(r_list)) return new_mat, trace_matrix score_matrix, trace_matrix = matrix_filling(m_mat, x_mat, y_mat, -2, -1, local=False) # score_matrix, trace_matrix = matrix_filling(m_mat, x_mat, y_mat, -2, -1, local=True) # seq1 = &quot;-TCGTAGACGA&quot; # seq2 = &quot;ATAGAATGCGG&quot; def global_backtracking(matrix, score_matrix): '''this function is used to trace back the input matrix and output the final alignment Note: the input matrix is trace matrix''' ti = len(seq1) tj = len(seq2) alignment1 = '' alignment2 = '' while (ti &gt; 0 or tj &gt; 0): # Choose to go left, up or diagonal cell = matrix[ti][tj] if cell == 0: alignment1 = seq1[ti-1] + alignment1 alignment2 = seq2[tj-1] + alignment2 ti -= 1 tj -= 1 elif cell == 1: alignment1 = seq1[ti-1] + alignment1 alignment2 = '-' + alignment2 ti -= 1 elif cell == 2: alignment1 = '-' + alignment1 alignment2 = seq2[tj-1] + alignment2 tj -= 1 # fmt_alignment = f'{alignment1}\\n{alignment2}' # print(fmt_alignment) # Formt the info info = f&quot;======The Global======\\n {alignment1}\\n {alignment2}\\nSCORE: {score_matrix[len(score_matrix)-1][len(score_matrix[0])-1]}&quot; print(info) global_backtracking(trace_matrix, score_matrix) def local_backtracking(trace_matrix, score_matrix): '''this function does backtracking like FUNCTION global_backtracking, but in the way of local aligment''' # Convert score matrix into Numpy array to find maximum value new_score_matrix = np.array(score_matrix) pos = np.unravel_index(np.argmax(new_score_matrix, axis=None), new_score_matrix.shape) # retrieve the maximum value ti = pos[0] tj = pos[1] # print(f'{ti}\\t{tj}') alignment1 = '' alignment2 = '' while (ti &gt; 0 or tj &gt; 0): if new_score_matrix[ti][tj] == 0: # stop local alignment back tracking when 0 values met break cell = trace_matrix[ti][tj] if cell == 0: alignment1 = seq1[ti-1] + alignment1 alignment2 = seq2[tj-1] + alignment2 ti -= 1 tj -= 1 elif cell == 1: alignment1 = seq1[ti-1] + alignment1 alignment2 = '-' + alignment2 ti -= 1 elif cell == 2: alignment1 = '-' + alignment1 alignment2 = seq2[tj-1] + alignment2 tj -= 1 info = f&quot;======The Local======\\n {alignment1}\\n {alignment2}\\nSCORE: {np.ndarray.max(new_score_matrix)}&quot; print(info) # local_backtracking(trace_matrix, score_matrix) 参考文献 [1] 《组学数据中的统计与分析》，田卫东 [2] https://users.soe.ucsc.edu/~karplus/bme205/f12/Alignment.html [3] https://www.youtube.com/watch?v=ZBD9he4Zp1E [4] Biological sequence analysis: Probabilistic models of proteins and nucleic acids ","link":"https://chenyoupu.top/post/globalandlocal_alignment/"},{"title":"「R语言刷题」数据类型篇 - 从tidyr和dplyr入手","content":"R语言到底可以用来做什么？ 在生物信息学领域，大数据、机器学习等方向，和Python一比，我感觉已经相形见绌， 所以为什么还要学习R语言？ 1）ggplot2的绘图体系非常完美 2）R base能够完成我日常小数据的分析 有很多人说，tidyverse给了R二次生命，但是在我看来它的编程思维，稍显奇怪，不太能够适应， 但是我属于一个钻牛角尖的人，既然学了，而且R我也没有完全丢掉，也是有一定必要学习一些重获新生的R语言。 因此这篇文章就是针对我在最近的R语言刷题中的总结。 数据读入 / 写出 从R base的角度看问题 read.table() read.csv() write.table() write.csv() 从readr的角度看问题 read_csv() read_delim() read_table() write_delim(df, 'filename.csv') write_csv(df, 'filename.csv') write_excel_csv(df, 'filename.csv') write_tsv(df, 'filename.csv') data.frame 从R base的角度看问题 如何随心所欲的操作data.frame？ 使用data.frame()创建数据库， 1）给定colname，再给定该变量对应的element（一般是vector格式），即 colname=c(elements) 2）依此类推，增添每一列 # 1. 数据框的创建 df &lt;- data.frame( &quot;grammer&quot; = c(&quot;Python&quot;,&quot;C&quot;,&quot;Java&quot;,&quot;GO&quot;,NA,&quot;SQL&quot;,&quot;PHP&quot;,&quot;Python&quot;), &quot;score&quot; = c(1,2,NA,4,5,6,7,10) ) 从tidyr的角度看问题 library(tidyr) df &lt;- tibble( &quot;grammer&quot; = c(&quot;Python&quot;,&quot;C&quot;,&quot;Java&quot;,&quot;GO&quot;,NA,&quot;SQL&quot;,&quot;PHP&quot;,&quot;Python&quot;), &quot;score&quot; = c(1,2,NA,4,5,6,7,10) ) # # A tibble: 8 x 2 # grammer score # &lt;chr&gt; &lt;dbl&gt; # 1 Python 1 # 2 C 2 # 3 Java NA # 4 GO 4 # 5 NA 5 # 6 SQL 6 # 7 PHP 7 # 8 Python 10 dplyr的衔接 dplyr提供了一系列用C++重新编写过的函数，能够加快我们处理数据的速度。 主要包含的函数如下， select() filter() mutate() group_by() arrange() 1）数据框列名的重新命名：names(), colnames(), rename() R base对列名进行修改的话，使用如下格式， # R base names(df)[2] &lt;- c('popularity') # dplyr df &lt;- df %&gt;% rename(popularity = score) 2）数据过滤：which(), filter() # R base df[which(df$popularity &gt; 3), ] # dplyr df %&gt;% filter(popularity &gt; 3) # 按照范围也是一样的， df %&gt;% filter(popularity &gt; 3 &amp; popularity &lt;7) 3）选择某一列数据：$, select() # R base df$popularity # dplyr df &lt;- df %&gt;% select(popularity, everything()) Note：tidyverse::everything()的作用，为选择数据框中的所有变量。 与select()函数一起使用，可以改变数据框中变量的排列顺序（e.g. 将变量A从变量B后移动到变量B前），以iris数据集为例， select(iris, everything()) # A tibble: 150 x 5 # Sepal.Length Sepal.Width Petal.Length Petal.Width Species # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; # 1 5.1 3.5 1.4 0.2 setosa select(iris, Species, everything()) # # A tibble: 150 x 5 # Species Sepal.Length Sepal.Width Petal.Length Petal.Width # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 setosa 5.1 3.5 1.4 0.2 可以看到Species被提前了。 4）将一行/多行数据添加到数据框中：rbind() row &lt;- data.frame( &quot;grammer&quot; = c(&quot;Perl&quot;), &quot;popularity&quot; = c(6.6) ) df &lt;- rbind(df, row) 5）分组：aggregate(), group_by R base：aggregate() dplyr：group_by + summarise # R base # 使用aggregate对df进行分组计算 library(flipAPI) data = DownloadXLSX(&quot;https://wiki.q-researchsoftware.com/images/1/1b/Aggregation_data.xlsx&quot;, want.row.names = FALSE, want.data.frame = TRUE) data.agg = aggregate(df, by = list(data$Role), FUN = mean) # dplyr # 使用group_by()分组，并进行相关计算 library(readr) df &lt;- read_csv('pandas120.csv') df %&gt;% group_by(education) %&gt;% # 设置用于分组的变量，此处为education summarise(mean = mean(salary)) 6）构建新的一列变量：mutate() # R base # 使用cbind()函数 # dplyr df &lt;- df %&gt;% mutate(test = paste0(df$education, df$createTime)) Note：paste0，先将变量转换为字符类型 7）将某一列的设置为索引：rownames(), column_to_rownames() # R base rownames(df) &lt;- df$createTime # dplyr # 使用column_to_rownames，该函数将某一列设置为行名后得到 df %&gt;% tibble::column_to_rownames('createTime') 8）数据排序 # R base # sort(): 返回的是排序后的结果 # order() df &lt;- df[order(df$popularity), ] # dplyr # 使用arrange() df &lt;- df %&gt;% arrange(popularity) df &lt;- df %&gt;% arrange(desc(popularity)) 9）数据的总结：summary(), summarize() 略 与data.frame有关的实例 1）填补NA 1）使用R包Hmisc中的impute()函数 2）使用impute函数的一般模式，一为输入的向量，二为如何填充NA值所定义的FUN Note：下列代码中的unlist，只是为了安全作用 library(Hmisc) index &lt;- which(is.na(df$popularity)) df$popularity &lt;- impute(df$popularity, (unlist(df[index-1, 2] + df[index+1, 2]))/2) 2）将宽数据转换为长数据 使用tidyr中的pivot_longer()函数， 一般的使用模式如下， pivot_longer(data, cols, names_to = &quot;name&quot;, # cols，定义一个vector用于选择df中的列并进行合并 values_to = &quot;value&quot;, # 每一行对应的元素 values_drop_na = FALSE) 举个例子， library(tidyr) df %&gt;% select(日期,`开盘价(元)`,`收盘价(元)`) %&gt;% pivot_longer(c(`开盘价(元)`,`收盘价(元)`), names_to='type',values_to='price') %&gt;% ggplot(aes(日期,price,color=type)) + geom_line(size=1.2) + scale_color_manual(values=c('steelblue','orange')) + theme_bw() + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.title = element_blank(), legend.position = c(0.86, 0.9) ) 3）读入数据的测试 假设现在有一个非常大的数据集，但是可以用read.csv等函数进行读入，然后后续用到的数据实际上只占了df的2列， 1）如何找到这两列？ 2）如何在后续的分析中只读取到这两列？ res &lt;- read.csv('数据1.csv',encoding = 'GBK',nrows = 3) # 测试读入3行数据 # 获取每一列的数据类型（e.g. integer、character） classes &lt;- sapply(res, class) classes[-match(c('positionName','salary'), names(classes))] &lt;- rep('NULL', length(classes) - 2) # 保留只想要选择的列 df &lt;- read.csv('数据1.csv', encoding = 'GBK', nrows = 10, colClasses = classes) # 使用&quot;colClasses=&quot;指定列 代码的解读， 1）测试读入数据，即读取3行 2）使用sapply获取每一列的数据类型 3）去除不想要的列的数值类型，即 -&gt;None 4）重新使用read.csv(, colClasses=)，对目的数据进行读取 4）使用sapply()创建一个数据框 df1 &lt;- sapply(20, function(n){ replicate(n, sample(1:100, 1)) }) %&gt;% as.data.frame(.) %&gt;% dplyr::rename(`0` = V1) R tips str()：查看对象每一个子对象的类型 R中有非常多的对象，比如 data.frame中的每一个变量都是一个子对象， 列表中的每一个$，均为一个子对象 str(df) # 'data.frame': 8 obs. of 2 variables: # $ grammer: Factor w/ 6 levels &quot;C&quot;,&quot;GO&quot;,&quot;Java&quot;,..: 5 1 3 2 NA 6 4 5 # $ score : num 1 2 NA 4 5 6 7 10 .：既是占位符，也是通配符 .在一些情况下，可以充当占位符，比如下列代码的含义为过滤掉salary小于10000的行，且查看过滤后的数据的行数， Note：可以类比为Shell中的- df &lt;- read_csv('pandas120.csv') df %&gt;% filter(salary &gt; 10000) %&gt;% dim(.) %&gt;% .[1] .的另一种含义为通配符，但是在此处不做过多赘述。 &quot;[&quot;：索引的另一种方式 直接举例说明， &quot;[&quot;(c(123, 12, 3), 1) # 123 &quot;[&quot;(c(123, 12, 3), 2) # 12 基于glue的格式化输出 1）使用R包glue（集合于tidyverse中），即可以直接理解为C、Pytohn中的格式化输出（e.g. sprintf） 2）paste(, collapse=) -&gt; .join() # library(readr) # df &lt;- read_csv('600000.SH.csv') library(glue) for (i in names(df)){ if(sum(is.na(df[,'日期'])) != 0){ res1 &lt;- which(is.na(df[,i])) res2 &lt;- paste(res1, collapse = ',') print(glue('列名：&quot;{i}&quot;, 第[{res2}]行有缺失值')) } } 参考资料 [1] R语言数据处理120题 ","link":"https://chenyoupu.top/post/R-exercises-1/"},{"title":"To change or not to change？","content":"Part I 专属于我的弯路 其实我一直都保留着，我想要研究做人类方面的研究的想法，一直都有。 但是本科阶段做了很多的傻事，也没有遇到一个好的老师，对我的方向有着怎么样的指引，我现在就正走在属于我的弯路上。 Part II 规划重要吗？ 规划自己的生活重要吗？ 虽然开学才短短不到20天，但是我却已经产生了要转硕士/转课题组的想法， 不为什么， 但是也为了什么。 为了想要完成自己曾经不敢去想的事情，为了将自己生命中宝贵的时间，都投入到自己感兴趣的事情上去。 在上一堂课的时候，林老师说，“规划人生不重要，我当年想读生物信息学，但是没有名额，最终现在在做mRNA……”， 引发了我的疑惑，我下课就去问他， 最终具体说了什么我忘了，大概说了一些，“你永远不知道明天会发生什么，明天也有可能直接被车了”、“路上碰到一个女孩，我想让她当我女朋友，最后没有成功，有什么关系呢？？我尽力去做了”， 所以，并不是规划不重要，而是能否接受现实与理想的不同，并继续努力好好生活。 “有时候，就是头脑一热，就干了” —— 林金钟 Part III 我的反抗、我的自由、我的激情 上面这句话来自加缪。 而我想写一句对应着我自己的话，“我的兴趣、我的坚持、我的义无反顾”。 作为一个小镇错题库，我不知道我是如何成长到现在这样的， 我看了很多的心理学书籍，我看了很多的哲学书籍，我也看了很多我兴趣方面的书籍。 我的父母不理解我想要做什么，大概也已经觉得我走到现在这一步就已经可以了， 但是，我不这样认为。 ","link":"https://chenyoupu.top/post/to-change-or-no-to-change/"},{"title":"「转录组」001｜WGCNA专题：实战原理两不误","content":"Hello，这里是即将开学的陈有朴。 表达矩阵的处理 后续分析所用到的数据，均为FPKM标准化后的表达矩阵。 从流程上对WGCNA进行解读 1）当对芯片数据或者RNA-Seq等数据完成分析之后，我们可以得到一张表达矩阵 一行为一个样本，一列为一个gene。 2）读取表达矩阵之后，对其进行adjacency矩阵的计算 adjacency矩阵，基于gene之间相关性的矩阵。每一个单元代表了2个gene之间的关联性（similarity）。 adjacency矩阵的构建，涉及到软阈值的选择，即构建一个幂函数，选择一个指数（power），强化强相关性，弱化弱相关性。 「adjacency矩阵的拓展」 adjacency矩阵有2种计算方式， 1）unsigned：aij=∣cor(xx,xj)∣βa_{ij} = |cor(x_{x}, x_{j})|^{β}aij​=∣cor(xx​,xj​)∣β 2）signed：aij=(0.5∗(1+cor))βa_{ij}=(0.5*(1+cor))^{β}aij​=(0.5∗(1+cor))β 第一种情况，认为具有高负相关的gene之间，是有联系的。 第二种情况，认为具有高负相关的gene之间，是没有联系的。 举个例子， 假设cor = -1，β=2，unsigned情况下，计算得到的adjacency为1，即gene之间高度关联。signed情况下，计算得到的adjacency为0，即gene之间无关联。 3）TOM矩阵的构建 引入一个指标，即topological overlaps的计算，用于定义该gene是否有多个高关联性的gene（用于gene module的构建） Cor(xi,xj)−&gt;TOM(xi,xj)Cor(x_{i},x_{j}) -&gt; TOM(x_{i}, x_{j}) Cor(xi​,xj​)−&gt;TOM(xi​,xj​) 「TOM矩阵的拓展」 但是针对unsigned类型的adjacency矩阵，可能会出现无法判断几个基因之间的关联关系。 比如现有i，j，k 3个gene，计算得到它们之间的关联关系为(+,+,−)(+,+,-)(+,+,−)， 1）i和j之间为正相关；2）i和k之间为正相关；3）j和k之间为负相关。 这就矛盾了，说明上面的关联关系可能受到了一些噪音的影响等等。 此时，引入signed TOM，其相较于unsigned TOM能够更好的解决gene之间的冲突情况。 Note：需要注意的是，当使用signed adjacency进行分析时，不会出现上述矛盾。 同时，这些矛盾作者都已经考虑到了，但还是有一定区别， TOMsimilarity()中，默认设置为TOMType = &quot;unsigned&quot; TOMsimilarityFromExpr()中，默认设置为TOMType = &quot;signed&quot; 4）TOM dissimilarity矩阵的构建 TOM dissimilarity矩阵代表了某个gene与其他gene之间的距离。 用上述矩阵进行聚类分析，得到gene module。 5）初步构建gene module 以TOM dissimilarity矩阵作为输入，进行聚类分析。 代码如下， # Turn data expression into topological overlap matrix power=sft$powerEstimate # 使用sft$powerEstimate调用预估出的软阈值 TOM = TOMsimilarityFromExpr(datExpr, power = power) dissTOM = 1-TOM # Plot gene tree geneTree = hclust(as.dist(dissTOM), method = &quot;average&quot;); # 用于后续cutreeDynamic()，对gene tree进行裁剪 # pdf(file = &quot;3-gene_cluster.pdf&quot;, width = 12, height = 9); plot(geneTree, xlab=&quot;&quot;, sub=&quot;&quot;, main = &quot;Gene clustering on TOM-based dissimilarity&quot;, labels = FALSE, hang = 0.04); # dev.off() 6）使用cutreeDynamic()进行聚类分析的优化 使用pairwise eigengene，进一步计算得到eigengene dissimilarity，用于聚类分析，筛选指标，最终合并gene module。 Note：eigengene为gene表达模式的指标（PCA降维得到的第一个主成分） cutreeDynamic()代码如下， cutreeDynamic labels2colors plotDendroAndColors # Module identification using dynamic tree cut dynamicMods = cutreeDynamic(dendro = geneTree, distM = dissTOM, deepSplit = 2, pamRespectsDendro = FALSE, minClusterSize = 30); table(dynamicMods) length(table(dynamicMods)) # Convert numeric labels into colors dynamicColors = labels2colors(dynamicMods) table(dynamicColors) # Plot the dendrogram and colors underneath # pdf(file = &quot;4-module_tree.pdf&quot;, width = 8, height = 6); plotDendroAndColors(geneTree, dynamicColors, &quot;Dynamic Tree Cut&quot;,dendroLabels = FALSE, hang = 0.03,addGuide = TRUE, guideHang = 0.05,main = &quot;Gene dendrogram and module colors&quot;) # dev.off() 绘制图如下， gene module合并代码如下 -&gt; 基于cutreeDynamic()的分析结果进行聚类分析的gene module的合并 mergeCloseModules：合并gene module plotDendroAndColors # Merge close modules MEDissThres=0.25 abline(h=MEDissThres, col = &quot;red&quot;) merge = mergeCloseModules(datExpr, dynamicColors, cutHeight = MEDissThres, verbose = 3) mergedColors = merge$colors mergedMEs = merge$newMEs # Plot merged module tree # pdf(file = &quot;5-merged_Module_Tree.pdf&quot;, width = 12, height = 9) plotDendroAndColors(geneTree, cbind(dynamicColors, mergedColors), c(&quot;Dynamic Tree Cut&quot;, &quot;Merged dynamic&quot;), dendroLabels = FALSE, hang = 0.03, addGuide = TRUE, guideHang = 0.05) # dev.off() # merge$oldMEs，为数据框，行为样本，列为对应的gene module，其中的数值代表了它们之间的关联度 write.table(merge$oldMEs,file=&quot;oldMEs.txt&quot;); write.table(merge$newMEs,file=&quot;newMEs.txt&quot;); 绘制图如下，可以看到有一些gene module被合并了， 7）将gene module与表型特征相联系 使用标准化的eigengene进行计算。 Cor(MEs,Traits) Cor(MEs, Traits) Cor(MEs,Traits) 代码如下， moduleEigengenes，计算每个gene module的特征值 moduleTraitCor = cor(MEs, datTraits, use = &quot;p&quot;) moduleTraitPvalue = corPvalueStudent(moduleTraitCor, nSamples) # Define numbers of genes and samples nGenes = ncol(datExpr); nSamples = nrow(datExpr); # Recalculate MEs with color labels MEs0 = moduleEigengenes(datExpr, mergedColors)$eigengenes MEs = orderMEs(MEs0) # r$&gt; MEs[1:5, 1:5] # MElightcyan1 MEdarkolivegreen MEgreenyellow MEcyan MEwhite # LH38_Z1_1 0.3093871 -0.01665288 0.1583588 -0.017809267 -0.01305706 # LH38_Z1_2 0.3318247 -0.01679628 0.1629585 -0.010450719 0.04613664 # LH38_Z1_3 0.3087032 -0.01762908 0.1527085 -0.018456470 -0.03661408 # LH38_Z2_1 0.3050361 -0.01431782 0.1589569 -0.020199995 0.06161998 # LH38_Z2_2 0.3422982 -0.01466196 0.1722949 -0.006663298 0.08192540 # 一些数据处理部分 # Read microbial data as traits bac_traits = read.table(&quot;traits_file/b_order_234.txt&quot;, header = T, sep = &quot;\\t&quot;) rownames(bac_traits) = bac_traits[, 1] bac_traits = bac_traits[, -1] # r$&gt; bac_traits[1:5, 1:5] # Pseudomonadales Enterobacteriales Xanthomonadales Burkholderiales Verrucomicrobiales # LH38_Z1_1 0.02120943 0.006338742 0.07261663 0.05920385 0.02674949 # LH38_Z1_2 0.04192444 0.009089757 0.06880071 0.05583164 0.02156440 # LH38_Z1_3 0.01393256 0.004525862 0.06961207 0.05100152 0.02189402 # LH39_Z1_1 0.11288033 0.013045132 0.07138692 0.04186106 0.01743154 # LH39_Z1_2 0.01214503 0.003410243 0.08236562 0.03733519 0.01953600 rownames(MEs) = paste(substr(rownames(MEs), 1, nchar(rownames(MEs))-1), rep(c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;), 60), sep = &quot;&quot;) # sample names should be consistent in eigen genes and traits !!!! bac_traits = bac_traits[match(rownames(MEs), rownames(bac_traits)), ] table(rownames(MEs) == rownames(bac_traits)) # Calculate pearson correlation coefficients between module eigen-genes and traits moduleTraitCor = cor(MEs, bac_traits, use = &quot;p&quot;); moduleTraitPvalue = corPvalueStudent(moduleTraitCor, nSamples); write.table(moduleTraitCor,file=&quot;moduleTrait_correlation.txt&quot;); write.table(moduleTraitPvalue,file=&quot;moduleTrait_pValue.txt&quot;); 结果图可视化代码如下， sizeGrWindow(10,6) # Will display correlations and their p-values # 合并 textMatrix = paste(signif(moduleTraitCor, 2), &quot;\\n(&quot;, signif(moduleTraitPvalue, 1), &quot;)&quot;, sep = &quot;&quot;); dim(textMatrix) = dim(moduleTraitCor) # pdf(&quot;module-traits-bacteria-order.pdf&quot;, width = 100, height = 30) par(mar = c(15, 12, 5, 5)); # Display the correlation values within a heatmap plot labeledHeatmap(Matrix = moduleTraitCor, xLabels = names(bac_traits), yLabels = names(MEs), ySymbols = names(MEs), colorLabels = FALSE, colors = greenWhiteRed(50), textMatrix = textMatrix, # 矩阵单元格上需要显示的信息 setStdMargins = FALSE, cex.text = 0.5, zlim = c(-1,1), main = paste(&quot;Module-trait relationships&quot;)) # dev.off() 结果图如下， Note：右边的图例，代表相关性程度 8）Hub gene的鉴定 2种方法， 与目标module有强关联性的gene（gene significance） 使用module membership来鉴定关键gene，Cor(i,ME)Cor(i,ME)Cor(i,ME) 一般先绘制出以module membership为x，gene significance为y的散点图。 代码如下， # 以相关性最高的模块为例 which(moduleTraitCor == max(moduleTraitCor, na.rm = TRUE), arr.ind = TRUE) # row col # MEskyblue3 25 30 Nitrosomonadales &lt;- as.data.frame(bac_traits[, 30]) names(Nitrosomonadales) = &quot;Nitrosomonadales&quot; modNames = substring(names(MEs), 3) # 去除ME前缀 # 计算module membership # 使用的是WGCNA自带cor函数，使用皮尔逊计算相关性 geneModuleMembership = as.data.frame(cor(datExpr, MEs, use = &quot;p&quot;)); geneModuleMembership[1:5, 1:5] # MElightcyan1 MEdarkolivegreen MEgreenyellow MEcyan MEwhite # Zm00001d001763 -0.045547074 -0.10334193 -0.20030129 0.22671189 0.1501121833 # Zm00001d001766 0.004752275 -0.01548396 -0.11130522 -0.22346048 0.0178421845 # Zm00001d001770 -0.286230379 -0.23340743 -0.39930773 0.22791746 0.0925346831 # Zm00001d001774 0.029505956 -0.06714112 -0.06323784 -0.34302212 0.1677206174 # Zm00001d001775 -0.029767437 -0.07642270 -0.13732818 -0.07876773 -0.0008678361 MMPvalue = as.data.frame(corPvalueStudent(as.matrix(geneModuleMembership), nSamples)); MMPvalue[1:5, 1:5] names(geneModuleMembership) = paste(&quot;MM&quot;, modNames, sep=&quot;&quot;); names(MMPvalue) = paste(&quot;p.MM&quot;, modNames, sep=&quot;&quot;); # 计算gene significance geneTraitSignificance = as.data.frame(cor(datExpr, Nitrosomonadales, use = &quot;p&quot;)); # head(geneTraitSignificance) # nrow(geneTraitSignificance) GSPvalue = as.data.frame(corPvalueStudent(as.matrix(geneTraitSignificance), nSamples)); names(geneTraitSignificance) = paste(&quot;GS.&quot;, names(Nitrosomonadales), sep=&quot;&quot;); names(GSPvalue) = paste(&quot;p.GS.&quot;, names(Nitrosomonadales), sep=&quot;&quot;); module = &quot;skyblue3&quot; column = match(module, modNames); # match(x, y)，找到y中x的索引位置 moduleGenes = mergedColors==module; # length(mergedColors) # nrow(geneModuleMembership) # nrow(geneTraitSignificance) # table(mergedColors) table(moduleGenes) sizeGrWindow(7, 7); par(mfrow = c(1,1)); verboseScatterplot(abs(geneModuleMembership[moduleGenes, column]), abs(geneTraitSignificance[moduleGenes, 1]), xlab = paste(&quot;Module Membership in&quot;, module, &quot;module&quot;), ylab = &quot;Gene significance for Nitrosomonadales&quot;, main = paste(&quot;Module membership vs. gene significance\\n&quot;), cex.main = 1.2, cex.lab = 1.2, cex.axis = 1.2, col = module) 散点图结果如下，可以看到module membership值越高的gene，其gene significance也越高。 其他 绘制gene module eigengenes与samples之间的热图 library(&quot;pheatmap&quot;) # Heatmap of old module eigen-genes and samples pdf(file=&quot;oldMEs.pdf&quot;,heigh=80,width=20) row.names(merge$oldMEs)=names(data0) # oldMEs，是一个矩阵 pheatmap(merge$oldMEs,cluster_col=T,cluster_row=T,show_rownames=T,show_colnames=T,fontsize=6) dev.off() # Heatmap of new module eigen-genes and samples # pdf(file=&quot;newMEs.pdf&quot;,heigh=60,width=20) row.names(merge$newMEs)=names(data0) pheatmap(merge$newMEs,cluster_col=T,cluster_row=T,show_rownames=T,show_colnames=T,fontsize=6) # dev.off() 参考资料 [1] https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/Rpackages/WGCNA/Tutorials/ [2] https://github.com/PengYuMaize/Yu2021NaturePlants [3] 跟着Nature Plants学数据分析：R语言WGCNA分析完整示例 [4] https://www.youtube.com/watch?v=BzYfg1lO3jw [5] https://www.biostars.org/p/288153/ [6] https://peterlangfelder.com/2018/11/25/signed-or-unsigned-which-network-type-is-preferable/ [7] https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/Rpackages/WGCNA/TechnicalReports/signedTOM.pdf ","link":"https://chenyoupu.top/post/WGCNA/"},{"title":"这可能是我写过最全的GATK笔记","content":"软件安装 官方网站：https://gatk.broadinstitute.org/hc/en-us 点击“Download GATK4”： 选择zip安装包： wget https://github.com/broadinstitute/gatk/releases/download/4.2.5.0/gatk-4.2.5.0.zip unzip gatk-4.2.5.0.zip 其他 安装GATK Best Practice所需要的软件：https://gatk.broadinstitute.org/hc/en-us/articles/360041320571--How-to-Install-all-software-packages-required-to-follow-the-GATK-Best-Practices 其他分析流程（bwa替代流程）：https://gatk.broadinstitute.org/hc/en-us/articles/4407897446939--How-to-Run-germline-single-sample-short-variant-discovery-in-DRAGEN-mode GATK：Jave选项 GATK实际使用的命令为：java -jar program.jar，但是为了GATK的开发者为了方便将其添加到环境变量，对其进行了封装，即使用安装目录下的gatk可执行脚本可直接运行： 下面给出一个封装前后，设置额外参数的例子： 未进行封装前，设置额外参数的方式为java -Xmx4G -jar gatk-package-4.2.2.0-local.jar 进行封装后，设置额外参数的方式为gatk --java-options &quot;-Xmx4G&quot; 官方文档如下： [1] https://gatk.broadinstitute.org/hc/en-us/articles/360035531892-GATK4-command-line-syntax GATK：HallotypeCaller 关于HallotypeCaller（GATK 3.6之后的版本）是否还需要使用重比对？ 答案是不需要。 细节请看： [1] https://github.com/broadinstitute/gatk-docs/blob/master/blog-2012-to-2019/2016-06-21-Changing_workflows_around_calling_SNPs_and_indels.md?id=7847 GATK：GenomicsDBImport 在完成gatk HallotypeCaller分析这一步之后，可以选择GenomicsDBImport将生成的gvcf文件进行整合，便于后续的joint genotyping。 【标注】 “GATK4 Best Practice for SNP and Indel”一般都选择GenomicsDBImport（而不是CombineGVCFs）进行gvcf文件的合并。GenomicsDBImport有一套独立的数据存储系统； GenomicsDBImport与CombineGVCFs功能类似 —— 合并gvcf文件。前者将genomic loci作为划分依据（e.g. chromosome, scaffold, contig），后者使用sample； 可以使用SelectVariants，对GenomicsDBImport产生的数据库内容进行访问； 要求输入数据 GenomicsDBImport所要求的输入数据，为HallotypeCaller添加-ERC GVCF或-ERC BP_RESOLUTION参数生成的结果文件，即gvcf文件。 参数设置（强制要求） 运行GenomicsDBImport的时候，有一些参数是必须的，还有一些参数是额外设置，可以用于增大文件读取速度或者进行个性化分析。 --genomicsdb-workspace-path # 构建GenomicsDatabase的目标文件夹 -V # gvcf文件名 --sample-name-map # 一个包含所有gvcf ID的文本，使用tab分隔符，第一列为sample ID，第二列为gvcf ID -L | --intervals # 选择需要合并的基因组区域（每一行一个染色体编号） 额外参数设置： --batch-size # 代表每批次能够读入多少个样本，默认为0，表示一次性全部读入。当样本数超过100时需要注意 输入文件 GenomicsDBImport特殊的数据存储格式（e.g. ） GenomicsDBImport：示例代码 # 每一个sample gvcf作为输入文件 gatk --java-options &quot;-Xmx4g -Xms4g&quot; GenomicsDBImport \\ -V data/gvcfs/mother.g.vcf.gz \\ -V data/gvcfs/father.g.vcf.gz \\ -V data/gvcfs/son.g.vcf.gz \\ --genomicsdb-workspace-path my_database \\ --tmp-dir=/path/to/large/tmp \\ -L 20 # 将所有sample gvcf名称放入map文件中 # 对应参数：--sample-name-map gatk --java-options &quot;-Xmx4g -Xms4g&quot; \\ GenomicsDBImport \\ --genomicsdb-workspace-path my_database \\ --batch-size 50 \\ -L chr1:1000-10000 \\ --sample-name-map cohort.sample_map \\ --tmp-dir /path/to/large/tmp \\ --reader-threads 5 # 将新sample添加到GenomicsDBImport数据库中 # 对应参数：--genomicsdb-update-workspace-path gatk --java-options &quot;-Xmx4g -Xms4g&quot; GenomicsDBImport \\ -V data/gvcfs/mother.g.vcf.gz \\ -V data/gvcfs/father.g.vcf.gz \\ -V data/gvcfs/son.g.vcf.gz \\ --genomicsdb-update-workspace-path my_database \\ --tmp-dir /path/to/large/tmp \\ 【标注】sample map文件使用tab分隔符，每一行一个gvcf文件名 sample1 sample1.vcf.gz sample2 sample2.vcf.gz sample3 sample3.vcf.gz 还有一些细节就看看官方文档吧~ [1] https://gatk.broadinstitute.org/hc/en-us/articles/360057439331-GenomicsDBImport [2] https://github.com/GenomicsDB/GenomicsDB/wiki GATK：CombineGVCFs CombineGVCFs的主要功能就是将HaplotypeCaller产生的gvcf文件，给合并。 【标注】 有且只能是HaplotypeCaller产生的gvcf可以用作输入文件 VCF文件的合并，使用的是MergeVcfs，是Picard下的一个工具 官方给出的说明是：1000+以上的样本推荐使用GenomicsDBImport CombineGVCFs：示例代码 gatk CombineGVCFs \\ -R reference.fasta \\ --variant sample1.g.vcf.gz \\ --variant sample2.g.vcf.gz \\ -O cohort.g.vcf.gz # 将染色体拆分运行 gatk --java-options &quot;-Xmx4g -Xms4g&quot; CombineGVCFs -L 1 -V DRR083661.g.vcf.gz -O chr1.vcf.gz -R /home/chphl/2022_3_4_WGS/00.ref/arab_ref.fa 官方说明文档如下： [1] https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs GATK：GenotypeGVCFs 这边有一个非常关键词，“joint genotyping”。 genotyping，实际上就是发现给定群体（数据）中的DNA变异，包括SNP、INDEL、non-variation位点等。 要求的输入数据是HaplotypeCaller附加“-ERC GVCF”或“-ERC BP_RESOLUTION”参数，所产生的gvcf文件 输出文件：VCF GenotypeGVCFs：示例代码 gatk --java-options &quot;-Xmx4g&quot; GenotypeGVCFs \\ -R Homo_sapiens_assembly38.fasta \\ -V input.g.vcf.gz \\ -O output.vcf.gz # 当使用GenomicsDBImport作为合并HaplotypeCaller输出数据的工具时， gatk GenotypeGVCFs \\ -R data/ref/ref.fasta \\ -V gendb://my_database \\ -O test_output.vcf 一些常用参数： --TMP_DIR # 使用暂时文件夹对结果文件进行保存 --max-genotype-count # 每一个位置的genotype数量上限，默认为1024 # 其他参数 --sample-ploidy # 在混池测序的时候需要注意，设置为“Number of samples in each pool * Sample Ploidy” 官方说明文档如下： [1] https://gatk.broadinstitute.org/hc/en-us/articles/360037057852-GenotypeGVCFs [2] https://gatk.broadinstitute.org/hc/en-us/articles/360035889971--How-to-Consolidate-GVCFs-for-joint-calling-with-GenotypeGVCFs GATK：SelectVariants SelectVariants用于选择给定VCF文件中的个体 &amp; SNP类型（Select a subset of variants from a VCF file），功能可以概述为： 1、从给定VCF文件中，挑选个体（参数：-sn） 2、从给定VCF文件中，挑选对应区域（参数：--intervals） 3、选择不同类型的SNP（参数：--select-type） e.g. 只挑选“INDELs” 一些常用参数： -R | --reference # 给定参考基因组（官网说明文档给的是NULL，可有有无？） --sn # 选择给定的样本，样本名保存在以.args结尾的文本中（一定要.args结尾！！！） -xl-sn # 反向选择样本 --intervals # 选择给定区域 --select-type-to-include | -select-type # 选择保留给定类型的一种variants（注意，只有一种） # 没有给定的情况下，默认保留所有位点 # ！！！多次设置来选择多种类型的位点 # 包括：INDEL, SNP, MIXED, MNP, SYMBOLIC, NO_VARIATION # INDEL：insertion和deletion # SNP：single nucleotide polymorphism # MIXED：同一个位置上，不仅有SNP，还有IDEL（在多个个体水平） # MNP：由相邻物理位置上组合成的SNP集合 # SYMBOLIC： # NO_VARIATION：非多态性位点 --restrict-alleles-to # 限制variants的类型为“ALL”，“BIALLELIC”，“MULTIALLELIC”其中的一种 --exclude-filtered # 启用该FLAG值时，输出结果中只会包括有“PASS”标记的位点 --set-filtered-gt-to-nocall # 默认情况下不开启，开启之后将“./.”位点全部过滤 其他参数： --selectExpressions # 选择variants的标准（自行定义） 需要注意的问题：IndexFeatureFile 在服务器中移动VCF文件时，转移到一个文件夹的时候忘记把对应VCF文件的索引也转移过来了 或者，本身就没有对该VCF创建索引 可以使用如下命令，对目标VCF文件建立索引 gatk --java-options &quot;-Xmx4G&quot; IndexFeatureFile --input input.vcf 需要注意的问题：CreateSequenceDictionary 在输入参考基因组（fasta格式）进行辅助分析的时候，不仅需要使用samtools对fasta文件进行fai索引的构建，还需要使用属于Picard工具包的CreateSequenceDictionary对其进行dict文件的构建： samtools faidx ref.fasta # 结果文件：ref.fasta.fai gatk CreateSequenceDictionary R=ref.fasta O=ref.dict # 结果文件：ref.dict 官方说明文档： [1] https://gatk.broadinstitute.org/hc/en-us/articles/360036362532-SelectVariants [2] https://gatk.broadinstitute.org/hc/en-us/articles/360035530752-What-types-of-variants-can-GATK-tools-detect-or-handle- [3] https://gatk.broadinstitute.org/hc/en-us/articles/360035890771-Biallelic-vs-Multiallelic-sites [4] https://www.strand-ngs.com/files/manual/reference/snp.html GATK：VariantFiltration VariantFiltration命令主要的功能是设置阈值，对SNP进行硬过滤，在FILTER这一列对位点进行标记。 一般被过滤的位点，被标记为自行给定的FILTER名称 没有被过滤的位点，被标记为PASS VariantFiltration：示例代码 gatk VariantFiltration \\ -R reference.fasta \\ -V input.vcf.gz \\ -O output.vcf.gz \\ --filter-name &quot;my_filter1&quot; \\ --filter-expression &quot;AB &lt; 0.2&quot; \\ --filter-name &quot;my_filter2&quot; \\ --filter-expression &quot;MQ0 &gt; 50&quot; 一些参数的说明 官方推荐将SNP和INDEL单独提取出来之后，再分别进行VariantFiltration。 【分析标注】SelectVariants使用“-select-type”对变异位点进行筛选时，只会根据给定类型，筛选单一子集（e.g. 给定选择SNP，就只会筛选SNP，不会选择SNP和INDEL共存的位点）。若需要选择混合类型的位点，使用参数“MIXED” # 示例hard-filtering gatk VariantFiltration \\ -V snps.vcf.gz \\ -filter &quot;QD &lt; 2.0&quot; --filter-name &quot;QD2&quot; \\ -filter &quot;QUAL &lt; 30.0&quot; --filter-name &quot;QUAL30&quot; \\ -filter &quot;SOR &gt; 3.0&quot; --filter-name &quot;SOR3&quot; \\ -filter &quot;FS &gt; 60.0&quot; --filter-name &quot;FS60&quot; \\ -filter &quot;MQ &lt; 40.0&quot; --filter-name &quot;MQ40&quot; \\ -filter &quot;MQRankSum &lt; -12.5&quot; --filter-name &quot;MQRankSum-12.5&quot; \\ -filter &quot;ReadPosRankSum &lt; -8.0&quot; --filter-name &quot;ReadPosRankSum-8&quot; \\ -O snps_filtered.vcf.gz gatk VariantFiltration \\ -V indels.vcf.gz \\ -filter &quot;QD &lt; 2.0&quot; --filter-name &quot;QD2&quot; \\ -filter &quot;QUAL &lt; 30.0&quot; --filter-name &quot;QUAL30&quot; \\ -filter &quot;FS &gt; 200.0&quot; --filter-name &quot;FS200&quot; \\ -filter &quot;ReadPosRankSum &lt; -20.0&quot; --filter-name &quot;ReadPosRankSum-20&quot; \\ -O indels_filtered.vcf.gz 再给出一篇文章中的过滤参数： 官方说明文档如下： [1] https://gatk.broadinstitute.org/hc/en-us/articles/360037434691-VariantFiltration [2] https://gatk.broadinstitute.org/hc/en-us/articles/360035531112--How-to-Filter-variants-either-with-VQSR-or-by-hard-filtering【主要看“2. Hard filter a cohort callset with VariantFiltration”这部分就行】 其他GATK命令 GATK：碱基质量矫正 碱基质量矫正（Base Quality Score Recalibration/BQSR），这一步分析主要用于检测由于测序仪器造成的系统误差。 BQSR理解 可以从以下2个方面对BQSR进行理解： 1、首先需要明确的一点是，BQSR是对碱基质量值进行矫正，而不是碱基。碱基质量值代表了该碱基的可信度（有百分之多少的概率，我们可以相信这个位点的测序情况是正确的，或者说有百分之多少的概率，认为这个位点是测错的），而base quality又与后续的SNP检测有关，因此对碱基质量值进行矫正非常关键。 【标注】也就是说，我们不能够决定一个低质量的A碱基，其原本是不是一个T，但是我们可以决定相信这个A的程度（基于base quality） 2、BQSR通过机器学习的方法对base quality进行矫正（本质都是回归） 【标注】 作者给出的例子，当AA在序列中连着出现的时候，认为后续出现的碱基都增加了1%被测序的概率，因此对应的base quality也需要下降 上述影响是具有累加性的 BQSR分析过程 1、基于输入数据（bam），建立一个协变模型（这个名词有待商榷） 结果文件为一个recalibration file 2、使用ApplyBQSR对原始输入文件的base quality进行矫正，结果文件为bam文件（新生成），同时在上述步骤要求输入先验SNP位点。具体过程： 统计全局差异（observed/reported quality vs expected/empirical quality） 每一个位点加上窗口计算的差异 + 每一个位点加上循环计算 &amp; dinucleotide差异 【分析标注】 “dinucleotide”【待解决】 推荐二次建立模型，看看recalibration对整体结果的影响变化。 BQSR：示例代码 # 还没学，用不到 官方文档如下： [1] https://gatk.broadinstitute.org/hc/en-us/articles/360035890531-Base-Quality-Score-Recalibration-BQSR- [2] https://gatk.broadinstitute.org/hc/en-us/articles/360036898312-BaseRecalibrator GATK：VariantRecalibrator &amp; ApplyVQSR VariantRecalibrator采用机器学习的方式，计算位点的VQSLOD值（取代该位点原本的QUAL），作为后续分析的指标，一般分为以下2个步骤： 使用高质量的VCF数据集，训练模型 将模型应用到我们自己的数据当中去 从上述的话，其实也能很明显地看出来，VariantRecalibrator &amp; ApplyVQSR这套方法的缺陷 —— 需要先验的高质量VCF数据集。 输入数据的要求： VariantRecalibrator：示例运行 # exome data gatk VariantRecalibrator \\ -R Homo_sapiens_assembly38.fasta \\ -V input.vcf.gz \\ --resource hapmap,known=false,training=true,truth=true,prior=15.0:hapmap_3.3.hg38.sites.vcf.gz \\ --resource omni,known=false,training=true,truth=false,prior=12.0:1000G_omni2.5.hg38.sites.vcf.gz \\ --resource 1000G,known=false,training=true,truth=false,prior=10.0:1000G_phase1.snps.high_confidence.hg38.vcf.gz \\ --resource dbsnp,known=true,training=false,truth=false,prior=2.0:Homo_sapiens_assembly38.dbsnp138.vcf.gz \\ -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR \\ -mode SNP \\ -O output.recal \\ --tranches-file output.tranches \\ --rscript-file output.plots.R # Allele-specific version of the SNP recalibration gatk VariantRecalibrator \\ -R Homo_sapiens_assembly38.fasta \\ -V input.vcf.gz \\ -AS \\ --resource hapmap,known=false,training=true,truth=true,prior=15.0:hapmap_3.3.hg38.sites.vcf.gz \\ --resource omni,known=false,training=true,truth=false,prior=12.0:1000G_omni2.5.hg38.sites.vcf.gz \\ --resource 1000G,known=false,training=true,truth=false,prior=10.0:1000G_phase1.snps.high_confidence.hg38.vcf.gz \\ --resource dbsnp,known=true,training=false,truth=false,prior=2.0:Homo_sapiens_assembly38.dbsnp138.vcf.gz \\ -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR \\ -mode SNP \\ -O output.AS.recal \\ --tranches-file output.AS.tranches \\ --rscript-file output.plots.AS.R 官方说明文档如下： [1] https://gatk.broadinstitute.org/hc/en-us/articles/360036510892-VariantRecalibrator [2] https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels- GATK：MergeVCFs MergeVCFs是包含在Picard内的一个工具，用于合并不同区域的VCF文件 对输入文件的要求如下： 每个VCF文件所包含的sample要一致 Input file headers must be contain compatible declarations for common annotations (INFO, FORMAT fields) and filters，即一些通用信息要包含（e.g. INFO, FORMAT, filters） 每个VCF文件包含的SNP，要求经过排序 MergeVCFs：示例代码 java -jar /path/picard.jar MergeVcfs \\ I=input_variants.01.vcf \\ I=input_variants.02.vcf.gz \\ O=output_variants.vcf.gz 题外话 GATK：什么是read group &amp; 如何添加read group信息 在进行bwa mem比对时，需要向bam/sam文件中，添加read group信息。 如果bam/sam文件中，没有对应信息，则会报错。 （1）GATK需要的read group信息是什么？ ID = Read group identifier # 每一个read group 独有的ID，每一对reads 均有一个独特的ID，可以自定义命名； PL = Platform # 测序平台：ILLUMINA, SOLID, LS454, HELICOS and PACBIO，不区分大小写； SM = sample # reads属于的样品名；SM要设定正确，因为GATK产生的VCF文件也使用这个名字; LB = DNA preparation library identifier # 对一个read group的reads进行重复序列标记时，需要使用LB来区分reads来自那条lane;有时候，同一个库可能在不同的lane上完成测序;为了加以区分， # 同一个或不同库只要是在不同的lane产生的reads都要单独给一个ID. 一般无特殊说明，成对儿read属于同一库，可自定义，比如：library1 （2）如何进行read group信息添加？ 图示： @A00312:194:HFK5KDSX2 # 测序仪器 4 # lane ID 1101 # tail坐标 24469 # tail x坐标 13150 # tail y坐标 # bwa比对时，对read group信息进行添加 bwa -R '@RG\\tID:lane_${lane}\\tPL:illumina\\tSM:${sample}' 参考资料如下： https://www.jianshu.com/p/c41e8f3266b4 写在最后 变异分析流程的软件有很多，金标准只有GATK一个（针对于WGS和WES），但是GATK自己本身就不断在更新。所以说，只学习别人软件的用法，这样还是走不长远的，我觉得还是需要以后自己有能力去做一款软件，靠这个吃饭，我觉得比较ok。 ","link":"https://chenyoupu.top/post/GATK-BESTPRACTICE/"}]}